from PyQt5.QtWidgets import (QWidget, QVBoxLayout, QPushButton, QLabel, QFileDialog,
                           QHBoxLayout, QGroupBox, QGridLayout, QListWidget,
                           QListWidgetItem, QMessageBox, QTableWidget, QTableWidgetItem,
                           QHeaderView, QTabWidget, QTextEdit, QProgressBar, QSplitter,
                           QScrollArea, QSizePolicy, QComboBox)
from PyQt5.QtCore import Qt, pyqtSignal, QThread
from PyQt5.QtGui import QFont
import os
import json
import sys
import time
import torch
import torchvision
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Qt5Agg')
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
import seaborn as sns
try:
    import mplcursors
    MPLCURSORS_AVAILABLE = True
except ImportError:
    MPLCURSORS_AVAILABLE = False
    print("mplcursorsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡é€‰çš„æ‚¬åœåŠŸèƒ½")
from sklearn.metrics import (classification_report, confusion_matrix, 
                           precision_recall_fscore_support, roc_auc_score, 
                           average_precision_score, accuracy_score)
from sklearn.preprocessing import label_binarize
from collections import defaultdict


class ModelEvaluationThread(QThread):
    """æ¨¡å‹è¯„ä¼°çº¿ç¨‹"""
    progress_updated = pyqtSignal(int)
    status_updated = pyqtSignal(str)
    evaluation_finished = pyqtSignal(dict)
    evaluation_error = pyqtSignal(str)
    
    def __init__(self, model_path, model_arch, class_info_path, test_dir, task_type):
        super().__init__()
        self.model_path = model_path
        self.model_arch = model_arch
        self.class_info_path = class_info_path
        self.test_dir = test_dir
        self.task_type = task_type
        
    def run(self):
        """æ‰§è¡Œæ¨¡å‹è¯„ä¼°"""
        try:
            # æ·»åŠ srcç›®å½•åˆ°sys.path
            sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
            from predictor import Predictor
            
            # åˆ›å»ºé¢„æµ‹å™¨
            predictor = Predictor()
            
            # åŠ è½½æ¨¡å‹
            model_info = {
                'model_path': self.model_path,
                'class_info_path': self.class_info_path,
                'model_type': self.task_type,
                'model_arch': self.model_arch
            }
            
            self.status_updated.emit("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            predictor.load_model_with_info(model_info)
            
            # æ‰§è¡Œè¯„ä¼°
            if self.task_type == "åˆ†ç±»æ¨¡å‹":
                result = self._evaluate_classification_model(predictor, self.test_dir)
            else:
                result = self._evaluate_detection_model(predictor, self.test_dir)
                
            self.evaluation_finished.emit(result)
            
        except Exception as e:
            import traceback
            self.evaluation_error.emit(f"è¯„ä¼°å¤±è´¥: {str(e)}\n{traceback.format_exc()}")
    
    def _evaluate_classification_model(self, predictor, test_dir):
        """è¯„ä¼°åˆ†ç±»æ¨¡å‹"""
        self.status_updated.emit("æ­£åœ¨æ”¶é›†æµ‹è¯•æ ·æœ¬...")
        
        # æ”¶é›†æµ‹è¯•æ ·æœ¬
        test_samples = []
        class_dirs = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]
        
        for class_dir in class_dirs:
            class_path = os.path.join(test_dir, class_dir)
            image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            
            for img_file in image_files:
                img_path = os.path.join(class_path, img_file)
                test_samples.append((img_path, class_dir))
        
        if not test_samples:
            raise Exception("æœªæ‰¾åˆ°æµ‹è¯•æ ·æœ¬")
        
        self.status_updated.emit(f"æ‰¾åˆ° {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # æ‰§è¡Œé¢„æµ‹
        y_true = []
        y_pred = []
        y_scores = []  # ç”¨äºè®¡ç®—AUC
        inference_times = []
        
        predictor.model.eval()
        total_samples = len(test_samples)
        
        with torch.no_grad():
            for i, (img_path, true_label) in enumerate(test_samples):
                # æ›´æ–°è¿›åº¦
                progress = int((i + 1) / total_samples * 100)
                self.progress_updated.emit(progress)
                self.status_updated.emit(f"æ­£åœ¨é¢„æµ‹ {i+1}/{total_samples}")
                
                # é¢„æµ‹
                start_time = time.time()
                result = predictor.predict_image(img_path)
                end_time = time.time()
                
                inference_times.append(end_time - start_time)
                
                if result and result['predictions']:
                    predicted_label = result['predictions'][0]['class_name']
                    predicted_prob = result['predictions'][0]['probability'] / 100.0
                    
                    y_true.append(true_label)
                    y_pred.append(predicted_label)
                    
                    # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†æ•°
                    class_scores = {}
                    for pred in result['predictions']:
                        class_scores[pred['class_name']] = pred['probability'] / 100.0
                    
                    # ä¸ºæ¯ä¸ªç±»åˆ«å¡«å……åˆ†æ•°ï¼ˆæœªå‡ºç°çš„ç±»åˆ«è®¾ä¸º0ï¼‰
                    scores_vector = []
                    for class_name in predictor.class_names:
                        scores_vector.append(class_scores.get(class_name, 0.0))
                    y_scores.append(scores_vector)
        
        # è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
        self.status_updated.emit("æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)
        
        # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        class_precision, class_recall, class_f1, class_support = precision_recall_fscore_support(
            y_true, y_pred, average=None, labels=predictor.class_names, zero_division=0
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred, labels=predictor.class_names)
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(y_true, y_pred, target_names=predictor.class_names, output_dict=True, zero_division=0)
        
        # AUCè®¡ç®—ï¼ˆå¤šåˆ†ç±»ï¼‰
        try:
            y_true_binary = label_binarize([predictor.class_names.index(label) for label in y_true], 
                                         classes=range(len(predictor.class_names)))
            y_scores_array = np.array(y_scores)
            
            if len(predictor.class_names) == 2:
                # äºŒåˆ†ç±»
                auc_score = roc_auc_score(y_true_binary[:, 1], y_scores_array[:, 1])
            else:
                # å¤šåˆ†ç±»
                auc_score = roc_auc_score(y_true_binary, y_scores_array, multi_class='ovr', average='weighted')
        except Exception as e:
            print(f"è®¡ç®—AUCæ—¶å‡ºé”™: {e}")
            auc_score = 0.0
        
        # å¹³å‡ç²¾åº¦åˆ†æ•°
        try:
            if len(predictor.class_names) == 2:
                ap_score = average_precision_score(y_true_binary[:, 1], y_scores_array[:, 1])
            else:
                ap_score = average_precision_score(y_true_binary, y_scores_array, average='weighted')
        except Exception as e:
            print(f"è®¡ç®—APæ—¶å‡ºé”™: {e}")
            ap_score = 0.0
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        params_count = sum(p.numel() for p in predictor.model.parameters())
        
        # å¹³å‡æ¨ç†æ—¶é—´
        avg_inference_time = np.mean(inference_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_score': auc_score,
            'ap_score': ap_score,
            'params_count': params_count,
            'avg_inference_time': avg_inference_time,
            'total_samples': len(test_samples),
            'confusion_matrix': cm.tolist(),
            'class_names': predictor.class_names,
            'class_precision': class_precision.tolist(),
            'class_recall': class_recall.tolist(),
            'class_f1': class_f1.tolist(),
            'class_support': class_support.tolist(),
            'classification_report': class_report,
            'y_true': y_true,
            'y_pred': y_pred,
            'y_scores': y_scores
        }
    
    def _evaluate_detection_model(self, predictor, test_dir):
        """è¯„ä¼°æ£€æµ‹æ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # æ£€æµ‹æ¨¡å‹è¯„ä¼°ç›¸å¯¹å¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
        test_img_dir = os.path.join(test_dir, 'images', 'val')
        total_time = 0.0
        total_samples = 0
        
        if os.path.exists(test_img_dir):
            image_files = [f for f in os.listdir(test_img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
            
            for i, img_file in enumerate(image_files[:50]):  # é™åˆ¶æ ·æœ¬æ•°é‡
                self.progress_updated.emit(int((i + 1) / min(50, len(image_files)) * 100))
                self.status_updated.emit(f"æ­£åœ¨è¯„ä¼° {i+1}/{min(50, len(image_files))}")
                
                img_path = os.path.join(test_img_dir, img_file)
                start_time = time.time()
                predictor.predict_image(img_path)
                end_time = time.time()
                
                inference_time = end_time - start_time
                total_time += inference_time
                total_samples += 1
        
        params_count = sum(p.numel() for p in predictor.model.parameters())
        avg_inference_time = (total_time / total_samples) * 1000 if total_samples > 0 else 0
        
        return {
            'params_count': params_count,
            'avg_inference_time': avg_inference_time,
            'total_samples': total_samples
        }


class EnhancedModelEvaluationWidget(QWidget):
    """å¢å¼ºçš„æ¨¡å‹è¯„ä¼°ç»„ä»¶"""
    
    status_updated = pyqtSignal(str)
    
    def __init__(self, parent=None, main_window=None):
        super().__init__(parent)
        self.main_window = main_window
        self.models_dir = ""
        self.test_data_dir = ""
        self.models_list = []
        self.evaluation_results = {}
        
        # æ”¯æŒçš„æ¨¡å‹æ¶æ„åˆ—è¡¨
        self.classification_architectures = [
            "ResNet18", "ResNet34", "ResNet50", "ResNet101", "ResNet152",
            "MobileNetV2", "MobileNetV3Small", "MobileNetV3Large",
            "VGG16", "VGG19", "DenseNet121", "DenseNet169", "DenseNet201",
            "EfficientNetB0", "EfficientNetB1", "EfficientNetB2", "EfficientNetB3",
            "InceptionV3", "Xception"
        ]
        
        # æ”¯æŒçš„æ£€æµ‹æ¨¡å‹æ¶æ„åˆ—è¡¨
        self.detection_architectures = [
            "YOLOv5", "YOLOv7", "YOLOv8", 
            "Faster R-CNN", "Mask R-CNN", "RetinaNet",
            "SSD", "EfficientDet", "DETR"
        ]
        
        self.supported_architectures = self.classification_architectures
        
        self.init_ui()
        
    def init_ui(self):
        """åˆå§‹åŒ–UI"""
        # ä¸»å¸ƒå±€ - ç›´æ¥ä½¿ç”¨ç»„ä»¶çš„å¸ƒå±€ï¼Œä¸éœ€è¦é¢å¤–çš„æ»šåŠ¨åŒºåŸŸ
        layout = QVBoxLayout(self)
        layout.setContentsMargins(10, 10, 10, 10)
        layout.setSpacing(10)
        
        # åˆ›å»ºåˆ†å‰²å™¨
        splitter = QSplitter(Qt.Vertical)
        
        # ä¸ŠåŠéƒ¨åˆ†ï¼šæ¨¡å‹é€‰æ‹©å’ŒåŸºæœ¬ä¿¡æ¯
        top_widget = QWidget()
        top_layout = QVBoxLayout(top_widget)
        
        # æ¨¡å‹ç›®å½•é€‰æ‹©ç»„
        models_group = QGroupBox("æ¨¡å‹ç›®å½•")
        models_layout = QGridLayout()
        
        self.models_path_edit = QLabel()
        self.models_path_edit.setStyleSheet("QLabel { border: 1px solid gray; padding: 5px; }")
        self.models_path_edit.setText("è¯·é€‰æ‹©åŒ…å«æ¨¡å‹çš„ç›®å½•")
        
        models_btn = QPushButton("æµè§ˆ...")
        models_btn.clicked.connect(self.select_models_dir)
        
        refresh_btn = QPushButton("åˆ·æ–°")
        refresh_btn.clicked.connect(self.refresh_model_list)
        
        models_layout.addWidget(QLabel("æ¨¡å‹ç›®å½•:"), 0, 0)
        models_layout.addWidget(self.models_path_edit, 0, 1)
        models_layout.addWidget(models_btn, 0, 2)
        models_layout.addWidget(refresh_btn, 0, 3)
        
        # æµ‹è¯•é›†ç›®å½•é€‰æ‹©
        self.test_data_path_edit = QLabel()
        self.test_data_path_edit.setStyleSheet("QLabel { border: 1px solid gray; padding: 5px; }")
        self.test_data_path_edit.setText("è¯·é€‰æ‹©æµ‹è¯•é›†æ•°æ®ç›®å½•")
        
        test_data_btn = QPushButton("æµè§ˆ...")
        test_data_btn.clicked.connect(self.select_test_data_dir)
        
        models_layout.addWidget(QLabel("æµ‹è¯•é›†ç›®å½•:"), 1, 0)
        models_layout.addWidget(self.test_data_path_edit, 1, 1)
        models_layout.addWidget(test_data_btn, 1, 2)
        
        # æ·»åŠ æ¨¡å‹ç±»å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
        self.model_type_combo = QComboBox()
        self.model_type_combo.addItems(["åˆ†ç±»æ¨¡å‹", "æ£€æµ‹æ¨¡å‹"])
        self.model_type_combo.currentTextChanged.connect(self.on_model_type_changed)
        models_layout.addWidget(QLabel("æ¨¡å‹ç±»å‹:"), 2, 0)
        models_layout.addWidget(self.model_type_combo, 2, 1, 1, 2)
        
        # æ·»åŠ æ¨¡å‹æ¶æ„é€‰æ‹©ä¸‹æ‹‰æ¡†
        self.arch_combo = QComboBox()
        self.arch_combo.addItems(self.supported_architectures)
        models_layout.addWidget(QLabel("æ¨¡å‹æ¶æ„:"), 3, 0)
        models_layout.addWidget(self.arch_combo, 3, 1, 1, 2)
        
        models_group.setLayout(models_layout)
        top_layout.addWidget(models_group)
        
        # æ¨¡å‹åˆ—è¡¨ç»„
        list_group = QGroupBox("å¯ç”¨æ¨¡å‹")
        list_layout = QVBoxLayout()
        
        self.model_list = QListWidget()
        self.model_list.setSelectionMode(QListWidget.MultiSelection)
        self.model_list.setMinimumHeight(120)
        list_layout.addWidget(self.model_list)
        
        # è¯„ä¼°æŒ‰é’®å’Œè¿›åº¦æ¡
        eval_layout = QHBoxLayout()
        self.evaluate_btn = QPushButton("è¯„ä¼°é€‰ä¸­æ¨¡å‹")
        self.evaluate_btn.clicked.connect(self.evaluate_models)
        
        self.compare_btn = QPushButton("å¯¹æ¯”é€‰ä¸­æ¨¡å‹")
        self.compare_btn.clicked.connect(self.compare_models)
        
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        
        eval_layout.addWidget(self.evaluate_btn)
        eval_layout.addWidget(self.compare_btn)
        eval_layout.addWidget(self.progress_bar)
        
        list_layout.addLayout(eval_layout)
        list_group.setLayout(list_layout)
        top_layout.addWidget(list_group)
        
        splitter.addWidget(top_widget)
        
        # ä¸‹åŠéƒ¨åˆ†ï¼šè¯„ä¼°ç»“æœå±•ç¤º
        self.results_tabs = QTabWidget()
        
        # æ€»è§ˆæ ‡ç­¾é¡µ
        self.overview_tab = self.create_overview_tab()
        self.results_tabs.addTab(self.overview_tab, "è¯„ä¼°æ€»è§ˆ")
        
        # è¯¦ç»†æŒ‡æ ‡æ ‡ç­¾é¡µ
        self.metrics_tab = self.create_metrics_tab()
        self.results_tabs.addTab(self.metrics_tab, "è¯¦ç»†æŒ‡æ ‡")
        
        # æ··æ·†çŸ©é˜µæ ‡ç­¾é¡µ
        self.confusion_tab = self.create_confusion_tab()
        self.results_tabs.addTab(self.confusion_tab, "æ··æ·†çŸ©é˜µ")
        
        # åˆ†ç±»æŠ¥å‘Šæ ‡ç­¾é¡µ
        self.report_tab = self.create_report_tab()
        self.results_tabs.addTab(self.report_tab, "åˆ†ç±»æŠ¥å‘Š")
        
        # æ¨¡å‹å¯¹æ¯”æ ‡ç­¾é¡µ
        self.comparison_tab = self.create_comparison_tab()
        self.results_tabs.addTab(self.comparison_tab, "æ¨¡å‹å¯¹æ¯”")
        
        splitter.addWidget(self.results_tabs)
        
        # è®¾ç½®åˆ†å‰²å™¨æ¯”ä¾‹
        splitter.setSizes([300, 500])
        
        layout.addWidget(splitter)
    
    def create_overview_tab(self):
        """åˆ›å»ºæ€»è§ˆæ ‡ç­¾é¡µ"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # åŸºæœ¬æŒ‡æ ‡è¡¨æ ¼
        self.overview_table = QTableWidget(0, 2)
        self.overview_table.setHorizontalHeaderLabels(["æŒ‡æ ‡", "å€¼"])
        self.overview_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        self.overview_table.setMaximumHeight(300)
        
        layout.addWidget(QLabel("æ¨¡å‹è¯„ä¼°æ¦‚è§ˆ"))
        layout.addWidget(self.overview_table)
        
        # çŠ¶æ€æ ‡ç­¾
        self.status_label = QLabel("è¯·é€‰æ‹©æ¨¡å‹è¿›è¡Œè¯„ä¼°")
        self.status_label.setStyleSheet("QLabel { color: blue; font-weight: bold; }")
        layout.addWidget(self.status_label)
        
        layout.addStretch()
        return widget
    
    def create_metrics_tab(self):
        """åˆ›å»ºè¯¦ç»†æŒ‡æ ‡æ ‡ç­¾é¡µ"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        self.metrics_table = QTableWidget(0, 5)
        self.metrics_table.setHorizontalHeaderLabels(["ç±»åˆ«", "ç²¾ç¡®ç‡", "å¬å›ç‡", "F1åˆ†æ•°", "æ”¯æŒæ ·æœ¬æ•°"])
        self.metrics_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        
        layout.addWidget(QLabel("å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡"))
        layout.addWidget(self.metrics_table)
        
        return widget
    
    def create_confusion_tab(self):
        """åˆ›å»ºæ··æ·†çŸ©é˜µæ ‡ç­¾é¡µ"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        layout.setContentsMargins(10, 10, 10, 10)
        layout.setSpacing(10)
        
        # æ‚¬åœæç¤ºæ ‡ç­¾
        self.confusion_hover_label = QLabel("å°†é¼ æ ‡æ‚¬åœåœ¨æ··æ·†çŸ©é˜µä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
        self.confusion_hover_label.setStyleSheet("""
            QLabel {
                background-color: #f0f0f0;
                border: 1px solid #ccc;
                padding: 5px;
                border-radius: 3px;
                font-size: 12px;
                color: #666;
            }
        """)
        self.confusion_hover_label.setMinimumHeight(25)
        layout.addWidget(self.confusion_hover_label)
        
        # åˆ›å»ºmatplotlibå›¾è¡¨
        self.confusion_figure = Figure(figsize=(10, 8))  # å¢å¤§å°ºå¯¸
        self.confusion_canvas = FigureCanvas(self.confusion_figure)
        self.confusion_canvas.setMinimumHeight(500)  # è®¾ç½®æœ€å°é«˜åº¦
        
        layout.addWidget(QLabel("æ··æ·†çŸ©é˜µ"))
        layout.addWidget(self.confusion_canvas)
        
        return widget
    
    def create_report_tab(self):
        """åˆ›å»ºåˆ†ç±»æŠ¥å‘Šæ ‡ç­¾é¡µ"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # æ–‡æœ¬æ˜¾ç¤ºåŒºåŸŸ
        self.report_text = QTextEdit()
        self.report_text.setReadOnly(True)
        self.report_text.setFont(QFont("Courier", 10))
        
        layout.addWidget(QLabel("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š"))
        layout.addWidget(self.report_text)
        
        return widget
    
    def create_comparison_tab(self):
        """åˆ›å»ºæ¨¡å‹å¯¹æ¯”æ ‡ç­¾é¡µ"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        layout.setContentsMargins(10, 10, 10, 10)
        layout.setSpacing(10)
        
        # å¯¹æ¯”è¡¨æ ¼
        self.comparison_table = QTableWidget(0, 0)
        self.comparison_table.horizontalHeader().setSectionResizeMode(QHeaderView.Stretch)
        # ç§»é™¤é«˜åº¦é™åˆ¶ï¼Œè®©è¡¨æ ¼ç›´æ¥å±•å¼€æ˜¾ç¤ºæ‰€æœ‰å†…å®¹
        self.comparison_table.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Minimum)
        self.comparison_table.verticalHeader().setVisible(False)  # éšè—è¡Œå·
        
        layout.addWidget(QLabel("æ¨¡å‹å¯¹æ¯”ç»“æœ"))
        layout.addWidget(self.comparison_table)
        
        # æ‚¬åœæç¤ºæ ‡ç­¾
        self.hover_info_label = QLabel("å°†é¼ æ ‡æ‚¬åœåœ¨å›¾è¡¨ä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
        self.hover_info_label.setStyleSheet("""
            QLabel {
                background-color: #f0f0f0;
                border: 1px solid #ccc;
                padding: 5px;
                border-radius: 3px;
                font-size: 12px;
                color: #666;
            }
        """)
        self.hover_info_label.setMinimumHeight(25)
        layout.addWidget(self.hover_info_label)
        
        # å¯¹æ¯”å›¾è¡¨
        self.comparison_figure = Figure(figsize=(14, 10))  # å¢å¤§å›¾è¡¨å°ºå¯¸
        self.comparison_canvas = FigureCanvas(self.comparison_figure)
        self.comparison_canvas.setMinimumHeight(600)  # è®¾ç½®æœ€å°é«˜åº¦
        
        layout.addWidget(QLabel("æ€§èƒ½å¯¹æ¯”å›¾"))
        layout.addWidget(self.comparison_canvas)
        
        return widget
    
    def select_models_dir(self):
        """é€‰æ‹©æ¨¡å‹ç›®å½•"""
        folder = QFileDialog.getExistingDirectory(self, "é€‰æ‹©æ¨¡å‹ç›®å½•")
        if folder:
            self.models_dir = folder
            self.models_path_edit.setText(folder)
            self.refresh_model_list()
    
    def select_test_data_dir(self):
        """é€‰æ‹©æµ‹è¯•é›†æ•°æ®ç›®å½•"""
        folder = QFileDialog.getExistingDirectory(self, "é€‰æ‹©æµ‹è¯•é›†æ•°æ®ç›®å½•")
        if folder:
            self.test_data_dir = folder
            self.test_data_path_edit.setText(folder)
    
    def refresh_model_list(self):
        """åˆ·æ–°æ¨¡å‹åˆ—è¡¨"""
        if not self.models_dir:
            return
            
        try:
            self.model_list.clear()
            self.models_list = []
            
            # æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
            for file in os.listdir(self.models_dir):
                if file.endswith(('.h5', '.pb', '.tflite', '.pth')):
                    self.models_list.append(file)
                    self.model_list.addItem(file)
            
            if not self.models_list:
                QMessageBox.information(self, "æç¤º", "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
                
        except Exception as e:
            QMessageBox.critical(self, "é”™è¯¯", f"åˆ·æ–°æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
    
    def evaluate_models(self):
        """è¯„ä¼°é€‰ä¸­çš„æ¨¡å‹"""
        selected_items = self.model_list.selectedItems()
        if not selected_items:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·é€‰æ‹©è¦è¯„ä¼°çš„æ¨¡å‹")
            return
        
        if len(selected_items) == 1:
            # å•ä¸ªæ¨¡å‹è¯„ä¼°
            self.evaluate_single_model(selected_items[0].text())
        else:
            # å¤šä¸ªæ¨¡å‹è¯„ä¼°
            self.evaluate_multiple_models([item.text() for item in selected_items])
    
    def compare_models(self):
        """å¯¹æ¯”é€‰ä¸­çš„æ¨¡å‹"""
        selected_items = self.model_list.selectedItems()
        if len(selected_items) < 2:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·è‡³å°‘é€‰æ‹©ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”")
            return
        
        model_names = [item.text() for item in selected_items]
        self.compare_multiple_models(model_names)
    
    def evaluate_single_model(self, model_filename):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        
        # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†æµ‹è¯•é›†ç›®å½•
        if not self.test_data_dir:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·é€‰æ‹©æµ‹è¯•é›†æ•°æ®ç›®å½•")
            return
        
        if not os.path.exists(self.test_data_dir):
            QMessageBox.warning(self, "é”™è¯¯", "æµ‹è¯•é›†ç›®å½•ä¸å­˜åœ¨")
            return
        
        model_path = os.path.join(self.models_dir, model_filename)
        
        # è·å–é…ç½®ä¿¡æ¯
        config_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))), 'config.json')
        
        class_info_path = ""
        
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                class_info_path = config.get('default_class_info_file', '')
        
        # éªŒè¯ç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        if not class_info_path or not os.path.exists(class_info_path):
            json_files = [f for f in os.listdir(self.models_dir) if f.endswith('_classes.json') or f == 'class_info.json']
            if json_files:
                class_info_path = os.path.join(self.models_dir, json_files[0])
            else:
                QMessageBox.warning(self, "é”™è¯¯", "æ‰¾ä¸åˆ°ç±»åˆ«ä¿¡æ¯æ–‡ä»¶")
                return
        
        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ä»»åŠ¡ç±»å‹
        task_type = self.model_type_combo.currentText()
        
        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹æ¶æ„
        model_arch = self.arch_combo.currentText()
        
        # åˆ›å»ºå¹¶å¯åŠ¨è¯„ä¼°çº¿ç¨‹
        self.evaluation_thread = ModelEvaluationThread(
            model_path, model_arch, class_info_path, self.test_data_dir, task_type
        )
        
        # è¿æ¥ä¿¡å·
        self.evaluation_thread.progress_updated.connect(self.progress_bar.setValue)
        self.evaluation_thread.status_updated.connect(self.update_status)
        self.evaluation_thread.evaluation_finished.connect(self.on_evaluation_finished)
        self.evaluation_thread.evaluation_error.connect(self.on_evaluation_error)
        
        # å¯åŠ¨è¯„ä¼°
        self.evaluate_btn.setEnabled(False)
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.evaluation_thread.start()
    
    def evaluate_multiple_models(self, model_names):
        """è¯„ä¼°å¤šä¸ªæ¨¡å‹"""
        self.current_batch_models = model_names
        self.batch_results = {}
        self.current_batch_index = 0
        
        self.update_status(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(model_names)} ä¸ªæ¨¡å‹...")
        self.evaluate_next_model_in_batch()
    
    def evaluate_next_model_in_batch(self):
        """è¯„ä¼°æ‰¹é‡ä¸­çš„ä¸‹ä¸€ä¸ªæ¨¡å‹"""
        if not hasattr(self, 'current_batch_models') or not hasattr(self, 'current_batch_index'):
            return
            
        if self.current_batch_index >= len(self.current_batch_models):
            # æ‰€æœ‰æ¨¡å‹è¯„ä¼°å®Œæ¯•
            self.evaluate_btn.setEnabled(True)
            self.progress_bar.setVisible(False)
            self.on_batch_evaluation_finished()
            return
            
        # è·å–å½“å‰è¦è¯„ä¼°çš„æ¨¡å‹
        model_filename = self.current_batch_models[self.current_batch_index]
        model_path = os.path.join(self.models_dir, model_filename)
        
        # è·å–é…ç½®ä¿¡æ¯
        config_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))), 'config.json')
        
        class_info_path = ""
        
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                class_info_path = config.get('default_class_info_file', '')
        
        # éªŒè¯ç±»åˆ«ä¿¡æ¯æ–‡ä»¶
        if not class_info_path or not os.path.exists(class_info_path):
            json_files = [f for f in os.listdir(self.models_dir) if f.endswith('_classes.json') or f == 'class_info.json']
            if json_files:
                class_info_path = os.path.join(self.models_dir, json_files[0])
            else:
                QMessageBox.warning(self, "é”™è¯¯", "æ‰¾ä¸åˆ°ç±»åˆ«ä¿¡æ¯æ–‡ä»¶")
                return
        
        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ä»»åŠ¡ç±»å‹
        task_type = self.model_type_combo.currentText()
        
        # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹æ¶æ„ï¼Œè€Œä¸æ˜¯çŒœæµ‹
        model_arch = self.arch_combo.currentText()
        
        # åˆ›å»ºå¹¶å¯åŠ¨è¯„ä¼°çº¿ç¨‹
        self.evaluation_thread = ModelEvaluationThread(
            model_path, model_arch, class_info_path, self.test_data_dir, task_type
        )
        
        # è¿æ¥ä¿¡å·
        self.evaluation_thread.progress_updated.connect(self.progress_bar.setValue)
        self.evaluation_thread.status_updated.connect(self.update_status)
        self.evaluation_thread.evaluation_finished.connect(self.on_evaluation_finished)
        self.evaluation_thread.evaluation_error.connect(self.on_evaluation_error)
        
        # å¯åŠ¨è¯„ä¼°
        self.evaluate_btn.setEnabled(False)
        self.progress_bar.setVisible(True)
        self.progress_bar.setValue(0)
        self.update_status(f"æ­£åœ¨è¯„ä¼°æ¨¡å‹ {model_filename} ({self.current_batch_index + 1}/{len(self.current_batch_models)})")
        self.evaluation_thread.start()
    
    def compare_multiple_models(self, model_names):
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯„ä¼°ç»“æœ
        missing_results = []
        for model_name in model_names:
            if model_name not in self.evaluation_results:
                missing_results.append(model_name)
        
        if missing_results:
            reply = QMessageBox.question(
                self, "ç¡®è®¤", 
                f"ä»¥ä¸‹æ¨¡å‹å°šæœªè¯„ä¼°ï¼š\n{', '.join(missing_results)}\n\næ˜¯å¦å…ˆè¯„ä¼°è¿™äº›æ¨¡å‹ï¼Ÿ",
                QMessageBox.Yes | QMessageBox.No
            )
            
            if reply == QMessageBox.Yes:
                # å…ˆè¯„ä¼°ç¼ºå¤±çš„æ¨¡å‹ï¼Œç„¶åè¿›è¡Œå¯¹æ¯”
                self.pending_comparison_models = model_names
                self.evaluate_multiple_models(missing_results)
            return
        
        # æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç»“æœï¼Œç›´æ¥è¿›è¡Œå¯¹æ¯”
        self.display_model_comparison(model_names)
    
    def on_batch_evaluation_finished(self):
        """æ‰¹é‡è¯„ä¼°å®Œæˆå¤„ç†"""
        self.update_status(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(self.current_batch_models)} ä¸ªæ¨¡å‹")
        
        # å¦‚æœæœ‰å¾…å¯¹æ¯”çš„æ¨¡å‹ï¼Œè¿›è¡Œå¯¹æ¯”
        if hasattr(self, 'pending_comparison_models'):
            self.display_model_comparison(self.pending_comparison_models)
            delattr(self, 'pending_comparison_models')
        
        # æ˜¾ç¤ºæ‰¹é‡ç»“æœæ€»è§ˆ
        self.display_batch_results()
    
    def display_batch_results(self):
        """æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°ç»“æœ"""
        if not hasattr(self, 'current_batch_models'):
            return
        
        # åˆ‡æ¢åˆ°å¯¹æ¯”æ ‡ç­¾é¡µ
        self.results_tabs.setCurrentWidget(self.comparison_tab)
        
        # æ˜¾ç¤ºæ‰€æœ‰è¯„ä¼°æ¨¡å‹çš„å¯¹æ¯”
        self.display_model_comparison(self.current_batch_models)
    
    def display_model_comparison(self, model_names):
        """æ˜¾ç¤ºæ¨¡å‹å¯¹æ¯”ç»“æœ"""
        # å‡†å¤‡å¯¹æ¯”æ•°æ®
        comparison_data = []
        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_score', 'params_count', 'avg_inference_time']
        metric_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'AUCåˆ†æ•°', 'å‚æ•°æ•°é‡', 'æ¨ç†æ—¶é—´(ms)']
        
        # å®šä¹‰å‚æ•°è§£é‡Šçš„å·¥å…·æç¤º
        metric_tooltips = {
            'æ¨¡å‹åç§°': 'è¢«è¯„ä¼°çš„æ¨¡å‹æ–‡ä»¶åç§°',
            'å‡†ç¡®ç‡': 'æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: (TP+TN)/(TP+TN+FP+FN)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½',
            'ç²¾ç¡®ç‡': 'é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­å®é™…ä¸ºæ­£ç±»çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: TP/(TP+FP)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½\nåæ˜ æ¨¡å‹é¢„æµ‹æ­£ç±»çš„å‡†ç¡®æ€§',
            'å¬å›ç‡': 'å®é™…æ­£ç±»æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: TP/(TP+FN)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½\nåæ˜ æ¨¡å‹æ‰¾å‡ºæ­£ç±»çš„èƒ½åŠ›',
            'F1åˆ†æ•°': 'ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°\nè®¡ç®—å…¬å¼: 2Ã—(ç²¾ç¡®ç‡Ã—å¬å›ç‡)/(ç²¾ç¡®ç‡+å¬å›ç‡)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½\nç»¼åˆè¯„ä¼°åˆ†ç±»æ€§èƒ½',
            'AUCåˆ†æ•°': 'ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œè¡¡é‡åˆ†ç±»å™¨åŒºåˆ†èƒ½åŠ›\nèŒƒå›´: 0-1ï¼Œ0.5ä¸ºéšæœºåˆ†ç±»å™¨\nè¶Šæ¥è¿‘1è¡¨ç¤ºåˆ†ç±»èƒ½åŠ›è¶Šå¼º',
            'å‚æ•°æ•°é‡': 'æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°é‡\nå½±å“æ¨¡å‹å¤æ‚åº¦ã€å†…å­˜å ç”¨å’Œè®¡ç®—é‡\nå‚æ•°è¶Šå¤šæ¨¡å‹è¶Šå¤æ‚ä½†å¯èƒ½è¿‡æ‹Ÿåˆ',
            'æ¨ç†æ—¶é—´(ms)': 'æ¨¡å‹å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹çš„å¹³å‡è€—æ—¶\nå•ä½: æ¯«ç§’(ms)\nè¶Šå°è¡¨ç¤ºæ¨ç†é€Ÿåº¦è¶Šå¿«ï¼Œå®æ—¶æ€§è¶Šå¥½'
        }
        
        for model_name in model_names:
            if model_name in self.evaluation_results:
                result = self.evaluation_results[model_name]
                row_data = [model_name]
                for metric in metrics:
                    value = result.get(metric, 0)
                    if metric == 'params_count':
                        row_data.append(f"{value:,}")
                    elif metric == 'avg_inference_time':
                        row_data.append(f"{value:.2f}")
                    else:
                        row_data.append(f"{value:.4f}")
                comparison_data.append(row_data)
        
        # æ›´æ–°å¯¹æ¯”è¡¨æ ¼
        self.comparison_table.setRowCount(len(comparison_data))
        self.comparison_table.setColumnCount(len(metrics) + 1)
        self.comparison_table.setHorizontalHeaderLabels(['æ¨¡å‹åç§°'] + metric_names)
        
        # ä¸ºè¡¨å¤´æ·»åŠ å·¥å…·æç¤º
        header = self.comparison_table.horizontalHeader()
        for i, header_text in enumerate(['æ¨¡å‹åç§°'] + metric_names):
            if header_text in metric_tooltips:
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„QTableWidgetItemæ¥è®¾ç½®å·¥å…·æç¤º
                header_item = QTableWidgetItem(header_text)
                header_item.setToolTip(metric_tooltips[header_text])
                self.comparison_table.setHorizontalHeaderItem(i, header_item)
        
        for i, row_data in enumerate(comparison_data):
            for j, value in enumerate(row_data):
                self.comparison_table.setItem(i, j, QTableWidgetItem(str(value)))
        
        # è‡ªåŠ¨è°ƒæ•´è¡¨æ ¼å¤§å°ä»¥é€‚åº”å†…å®¹
        self.comparison_table.resizeRowsToContents()
        self.comparison_table.resizeColumnsToContents()
        
        # è®¾ç½®è¡¨æ ¼é«˜åº¦ä»¥é€‚åº”æ‰€æœ‰è¡Œ
        total_height = self.comparison_table.horizontalHeader().height()
        for i in range(self.comparison_table.rowCount()):
            total_height += self.comparison_table.rowHeight(i)
        total_height += self.comparison_table.frameWidth() * 2
        self.comparison_table.setFixedHeight(total_height)
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
        self.plot_model_comparison(model_names)
    
    def plot_model_comparison(self, model_names):
        """ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”å›¾è¡¨"""
        self.comparison_figure.clear()
        
        # å‡†å¤‡æ•°æ®
        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']
        metric_labels = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
        
        model_data = {}
        for model_name in model_names:
            if model_name in self.evaluation_results:
                result = self.evaluation_results[model_name]
                model_data[model_name] = [result.get(metric, 0) for metric in metrics_to_plot]
        
        if not model_data:
            return
        
        # åˆ›å»ºå­å›¾ï¼Œå¢åŠ å­å›¾é—´è·
        self.comparison_figure.subplots_adjust(hspace=0.4, wspace=0.3, bottom=0.15, top=0.95)
        ax1 = self.comparison_figure.add_subplot(2, 2, 1)
        ax2 = self.comparison_figure.add_subplot(2, 2, 2)
        ax3 = self.comparison_figure.add_subplot(2, 2, 3)
        ax4 = self.comparison_figure.add_subplot(2, 2, 4)
        
        axes = [ax1, ax2, ax3, ax4]
        
        # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”
        x_pos = range(len(model_names))
        colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))
        
        # æ™ºèƒ½å¤„ç†æ¨¡å‹åç§°æ˜¾ç¤º
        def get_display_name(name, max_length=12):
            """è·å–é€‚åˆæ˜¾ç¤ºçš„æ¨¡å‹åç§°"""
            if len(name) <= max_length:
                return name
            # å°è¯•ä¿ç•™å…³é”®ä¿¡æ¯
            if '_' in name:
                parts = name.split('_')
                if len(parts) >= 2:
                    # ä¿ç•™ç¬¬ä¸€éƒ¨åˆ†å’Œæœ€åéƒ¨åˆ†
                    return f"{parts[0][:6]}...{parts[-1][-4:]}"
            return name[:max_length-3] + '...'
        
        display_names = [get_display_name(name) for name in model_names]
        
        # å­˜å‚¨æ‚¬åœä¿¡æ¯
        self.hover_cursors = []
        
        for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):
            ax = axes[i]
            values = [model_data[model_name][i] for model_name in model_names]
            
            bars = ax.bar(x_pos, values, color=colors)
            ax.set_title(label, fontsize=10)
            ax.set_xlabel('æ¨¡å‹', fontsize=9)
            ax.set_ylabel('åˆ†æ•°', fontsize=9)
            ax.set_xticks(x_pos)
            
            # è®¾ç½®æ¨ªåæ ‡æ ‡ç­¾ï¼Œæ ¹æ®æ¨¡å‹æ•°é‡è°ƒæ•´è§’åº¦å’Œå­—ä½“å¤§å°
            if len(model_names) <= 3:
                # æ¨¡å‹å°‘æ—¶ï¼Œä¸æ—‹è½¬ï¼Œå­—ä½“ç¨å¤§
                ax.set_xticklabels(display_names, rotation=0, fontsize=8, ha='center')
            elif len(model_names) <= 5:
                # ä¸­ç­‰æ•°é‡æ¨¡å‹ï¼Œå°è§’åº¦æ—‹è½¬
                ax.set_xticklabels(display_names, rotation=30, fontsize=7, ha='right')
            else:
                # æ¨¡å‹å¤šæ—¶ï¼Œå¤§è§’åº¦æ—‹è½¬ï¼Œå­—ä½“æ›´å°
                ax.set_xticklabels(display_names, rotation=45, fontsize=6, ha='right')
            
            ax.set_ylim(0, 1)
            ax.tick_params(axis='y', labelsize=8)
            
            # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom', fontsize=7)
            
            # æ·»åŠ æ‚¬åœåŠŸèƒ½æ˜¾ç¤ºå®Œæ•´æ¨¡å‹åç§°å’Œè¯¦ç»†ä¿¡æ¯
            try:
                if MPLCURSORS_AVAILABLE:
                    cursor = mplcursors.cursor(bars, hover=True)
                    cursor.connect("add", lambda sel, metric_name=label, model_list=model_names, values_list=values: 
                        self._on_bar_hover(sel, metric_name, model_list, values_list))
                    self.hover_cursors.append(cursor)
                else:
                    # ä½¿ç”¨matplotlibå†…ç½®çš„æ‚¬åœåŠŸèƒ½
                    self._add_matplotlib_hover(ax, bars, label, model_names, values)
            except Exception as e:
                print(f"æ·»åŠ æ‚¬åœåŠŸèƒ½æ—¶å‡ºé”™: {e}")
        
        # ä½¿ç”¨tight_layoutä½†è®¾ç½®æ›´å¤šçš„åº•éƒ¨è¾¹è·ä»¥å®¹çº³æ—‹è½¬çš„æ ‡ç­¾
        self.comparison_figure.tight_layout(pad=2.0, rect=[0, 0.05, 1, 0.98])
        self.comparison_canvas.draw()
    
    def _on_bar_hover(self, sel, metric_name, model_list, values_list):
        """å¤„ç†æŸ±çŠ¶å›¾æ‚¬åœäº‹ä»¶ï¼ˆmplcursorsç‰ˆæœ¬ï¼‰"""
        try:
            index = int(sel.target.index)
            if 0 <= index < len(model_list):
                model_name = model_list[index]
                value = values_list[index]
                
                # åˆ›å»ºè¯¦ç»†çš„æ‚¬åœä¿¡æ¯
                hover_text = f"æ¨¡å‹: {model_name}\n{metric_name}: {value:.4f}"
                
                # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
                if model_name in self.evaluation_results:
                    result = self.evaluation_results[model_name]
                    hover_text += f"\nå‚æ•°æ•°é‡: {result.get('params_count', 0):,}"
                    hover_text += f"\næ¨ç†æ—¶é—´: {result.get('avg_inference_time', 0):.2f}ms"
                
                sel.annotation.set_text(hover_text)
                sel.annotation.get_bbox_patch().set(boxstyle="round,pad=0.5", 
                                                  facecolor="lightyellow", 
                                                  edgecolor="gray", 
                                                  alpha=0.9)
        except Exception as e:
            print(f"å¤„ç†æ‚¬åœäº‹ä»¶æ—¶å‡ºé”™: {e}")
            sel.annotation.set_text("æ‚¬åœä¿¡æ¯è·å–å¤±è´¥")
    
    def _add_matplotlib_hover(self, ax, bars, metric_name, model_names, values):
        """ä½¿ç”¨matplotlibå†…ç½®åŠŸèƒ½æ·»åŠ æ‚¬åœæ•ˆæœ"""
        def on_hover(event):
            if event.inaxes == ax:
                hover_found = False
                for i, bar in enumerate(bars):
                    if bar.contains(event)[0]:
                        # é¼ æ ‡åœ¨æŸ±å­ä¸Š
                        model_name = model_names[i]
                        value = values[i]
                        
                        # åˆ›å»ºè¯¦ç»†çš„æ‚¬åœä¿¡æ¯
                        hover_text = f"ğŸ“Š {metric_name} | ğŸ·ï¸ æ¨¡å‹: {model_name} | ğŸ“ˆ åˆ†æ•°: {value:.4f}"
                        
                        # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
                        if model_name in self.evaluation_results:
                            result = self.evaluation_results[model_name]
                            hover_text += f" | âš™ï¸ å‚æ•°: {result.get('params_count', 0):,}"
                            hover_text += f" | âš¡ æ¨ç†: {result.get('avg_inference_time', 0):.2f}ms"
                        
                        # æ›´æ–°æ‚¬åœä¿¡æ¯æ ‡ç­¾
                        if hasattr(self, 'hover_info_label'):
                            self.hover_info_label.setText(hover_text)
                            self.hover_info_label.setStyleSheet("""
                                QLabel {
                                    background-color: #e8f4fd;
                                    border: 1px solid #4a90e2;
                                    padding: 5px;
                                    border-radius: 3px;
                                    font-size: 12px;
                                    color: #2c3e50;
                                    font-weight: bold;
                                }
                            """)
                        
                        hover_found = True
                        break
                
                # é¼ æ ‡ä¸åœ¨ä»»ä½•æŸ±å­ä¸Šï¼Œæ¢å¤é»˜è®¤ä¿¡æ¯
                if not hover_found and hasattr(self, 'hover_info_label'):
                    self.hover_info_label.setText("å°†é¼ æ ‡æ‚¬åœåœ¨å›¾è¡¨ä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
                    self.hover_info_label.setStyleSheet("""
                        QLabel {
                            background-color: #f0f0f0;
                            border: 1px solid #ccc;
                            padding: 5px;
                            border-radius: 3px;
                            font-size: 12px;
                            color: #666;
                        }
                    """)
        
        # è¿æ¥æ‚¬åœäº‹ä»¶
        self.comparison_canvas.mpl_connect('motion_notify_event', on_hover)
    
    def _guess_model_architecture(self, model_filename):
        """æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ¨¡å‹æ¶æ„"""
        # è¿”å›ç”¨æˆ·é€‰æ‹©çš„æ¶æ„ï¼Œä¸å†æ ¹æ®æ–‡ä»¶åçŒœæµ‹
        return self.arch_combo.currentText()
    
    def _is_classification_dataset(self, data_dir):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†ç±»æ•°æ®é›†"""
        try:
            items = os.listdir(data_dir)
            # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ï¼ˆç±»åˆ«æ–‡ä»¶å¤¹ï¼‰
            subdirs = [item for item in items if os.path.isdir(os.path.join(data_dir, item))]
            
            if len(subdirs) > 0:
                # æ£€æŸ¥å­ç›®å½•ä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡æ–‡ä»¶
                for subdir in subdirs[:3]:  # åªæ£€æŸ¥å‰3ä¸ªå­ç›®å½•
                    subdir_path = os.path.join(data_dir, subdir)
                    files = os.listdir(subdir_path)
                    image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
                    if image_files:
                        return True
            return False
        except:
            return False
    
    def _is_detection_dataset(self, data_dir):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ£€æµ‹æ•°æ®é›†"""
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«imagesæ–‡ä»¶å¤¹
            images_dir = os.path.join(data_dir, 'images')
            if os.path.exists(images_dir):
                # æ£€æŸ¥imagesæ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰valæˆ–testå­æ–‡ä»¶å¤¹
                val_dir = os.path.join(images_dir, 'val')
                test_dir = os.path.join(images_dir, 'test')
                return os.path.exists(val_dir) or os.path.exists(test_dir)
            return False
        except:
            return False
    
    def update_status(self, message):
        """æ›´æ–°çŠ¶æ€"""
        self.status_label.setText(message)
        self.status_updated.emit(message)
    
    def on_evaluation_finished(self, result):
        """è¯„ä¼°å®Œæˆå¤„ç†"""
        self.evaluate_btn.setEnabled(True)
        self.progress_bar.setVisible(False)
        
        # å¤„ç†æ‰¹é‡è¯„ä¼°
        if hasattr(self, 'current_batch_models') and hasattr(self, 'current_batch_index'):
            model_name = self.current_batch_models[self.current_batch_index]
            self.evaluation_results[model_name] = result
            self.batch_results[model_name] = result
            
            self.update_status(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ")
            
            # ç»§ç»­ä¸‹ä¸€ä¸ªæ¨¡å‹
            self.current_batch_index += 1
            self.evaluate_next_model_in_batch()
        else:
            # å•ä¸ªæ¨¡å‹è¯„ä¼°
            selected_items = self.model_list.selectedItems()
            if selected_items:
                model_name = selected_items[0].text()
                self.evaluation_results[model_name] = result
                self.display_results(result)
                self.update_status(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ")
    
    def on_evaluation_error(self, error_msg):
        """è¯„ä¼°é”™è¯¯å¤„ç†"""
        self.evaluate_btn.setEnabled(True)
        self.progress_bar.setVisible(False)
        QMessageBox.critical(self, "è¯„ä¼°é”™è¯¯", error_msg)
        self.update_status("è¯„ä¼°å¤±è´¥")
    
    def display_results(self, result):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        # æ›´æ–°æ€»è§ˆè¡¨æ ¼
        self.update_overview_table(result)
        
        # æ›´æ–°è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
        if 'class_names' in result:
            self.update_metrics_table(result)
            
            # æ›´æ–°æ··æ·†çŸ©é˜µ
            self.update_confusion_matrix(result)
            
            # æ›´æ–°åˆ†ç±»æŠ¥å‘Š
            self.update_classification_report(result)
    
    def update_overview_table(self, result):
        """æ›´æ–°æ€»è§ˆè¡¨æ ¼"""
        # å®šä¹‰æŒ‡æ ‡è§£é‡Šçš„å·¥å…·æç¤º
        overview_tooltips = {
            "å‡†ç¡®ç‡": "æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: (TP+TN)/(TP+TN+FP+FN)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½",
            "ç²¾ç¡®ç‡ (åŠ æƒå¹³å‡)": "å„ç±»åˆ«ç²¾ç¡®ç‡æŒ‰æ ·æœ¬æ•°é‡åŠ æƒçš„å¹³å‡å€¼\nç²¾ç¡®ç‡ = TP/(TP+FP)\nåæ˜ æ¨¡å‹é¢„æµ‹æ­£ç±»çš„å‡†ç¡®æ€§",
            "å¬å›ç‡ (åŠ æƒå¹³å‡)": "å„ç±»åˆ«å¬å›ç‡æŒ‰æ ·æœ¬æ•°é‡åŠ æƒçš„å¹³å‡å€¼\nå¬å›ç‡ = TP/(TP+FN)\nåæ˜ æ¨¡å‹æ‰¾å‡ºæ­£ç±»çš„èƒ½åŠ›",
            "F1åˆ†æ•° (åŠ æƒå¹³å‡)": "å„ç±»åˆ«F1åˆ†æ•°æŒ‰æ ·æœ¬æ•°é‡åŠ æƒçš„å¹³å‡å€¼\nF1 = 2Ã—(ç²¾ç¡®ç‡Ã—å¬å›ç‡)/(ç²¾ç¡®ç‡+å¬å›ç‡)\nç»¼åˆè¯„ä¼°åˆ†ç±»æ€§èƒ½",
            "AUCåˆ†æ•°": "ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œè¡¡é‡åˆ†ç±»å™¨åŒºåˆ†èƒ½åŠ›\nèŒƒå›´: 0-1ï¼Œ0.5ä¸ºéšæœºåˆ†ç±»å™¨\nè¶Šæ¥è¿‘1è¡¨ç¤ºåˆ†ç±»èƒ½åŠ›è¶Šå¼º",
            "å¹³å‡ç²¾åº¦åˆ†æ•°": "ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹çš„é¢ç§¯\nç»¼åˆè€ƒè™‘ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½",
            "æ¨¡å‹å‚æ•°æ•°é‡": "æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°é‡\nå½±å“æ¨¡å‹å¤æ‚åº¦ã€å†…å­˜å ç”¨å’Œè®¡ç®—é‡\nå‚æ•°è¶Šå¤šæ¨¡å‹è¶Šå¤æ‚ä½†å¯èƒ½è¿‡æ‹Ÿåˆ",
            "å¹³å‡æ¨ç†æ—¶é—´": "æ¨¡å‹å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹çš„å¹³å‡è€—æ—¶\nå•ä½: æ¯«ç§’(ms)\nè¶Šå°è¡¨ç¤ºæ¨ç†é€Ÿåº¦è¶Šå¿«ï¼Œå®æ—¶æ€§è¶Šå¥½",
            "æµ‹è¯•æ ·æœ¬æ€»æ•°": "ç”¨äºè¯„ä¼°çš„æµ‹è¯•æ ·æœ¬æ€»æ•°é‡\næ ·æœ¬æ•°é‡è¶Šå¤šè¯„ä¼°ç»“æœè¶Šå¯é "
        }
        
        overview_data = [
            ("å‡†ç¡®ç‡", f"{result.get('accuracy', 0):.4f}"),
            ("ç²¾ç¡®ç‡ (åŠ æƒå¹³å‡)", f"{result.get('precision', 0):.4f}"),
            ("å¬å›ç‡ (åŠ æƒå¹³å‡)", f"{result.get('recall', 0):.4f}"),
            ("F1åˆ†æ•° (åŠ æƒå¹³å‡)", f"{result.get('f1_score', 0):.4f}"),
            ("AUCåˆ†æ•°", f"{result.get('auc_score', 0):.4f}"),
            ("å¹³å‡ç²¾åº¦åˆ†æ•°", f"{result.get('ap_score', 0):.4f}"),
            ("æ¨¡å‹å‚æ•°æ•°é‡", f"{result.get('params_count', 0):,}"),
            ("å¹³å‡æ¨ç†æ—¶é—´", f"{result.get('avg_inference_time', 0):.2f} ms"),
            ("æµ‹è¯•æ ·æœ¬æ€»æ•°", f"{result.get('total_samples', 0):,}")
        ]
        
        self.overview_table.setRowCount(len(overview_data))
        
        for i, (metric, value) in enumerate(overview_data):
            # åˆ›å»ºæŒ‡æ ‡åç§°é¡¹å¹¶è®¾ç½®å·¥å…·æç¤º
            metric_item = QTableWidgetItem(metric)
            if metric in overview_tooltips:
                metric_item.setToolTip(overview_tooltips[metric])
            self.overview_table.setItem(i, 0, metric_item)
            
            # åˆ›å»ºæ•°å€¼é¡¹
            value_item = QTableWidgetItem(value)
            self.overview_table.setItem(i, 1, value_item)
    
    def update_metrics_table(self, result):
        """æ›´æ–°è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼"""
        # å®šä¹‰è¯¦ç»†æŒ‡æ ‡è¡¨å¤´çš„å·¥å…·æç¤º
        metrics_header_tooltips = {
            "ç±»åˆ«": "æ•°æ®é›†ä¸­çš„åˆ†ç±»ç±»åˆ«åç§°",
            "ç²¾ç¡®ç‡": "è¯¥ç±»åˆ«é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°å é¢„æµ‹ä¸ºè¯¥ç±»åˆ«æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: TP/(TP+FP)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½",
            "å¬å›ç‡": "è¯¥ç±»åˆ«é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°å å®é™…è¯¥ç±»åˆ«æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹\nè®¡ç®—å…¬å¼: TP/(TP+FN)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½",
            "F1åˆ†æ•°": "è¯¥ç±»åˆ«ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°\nè®¡ç®—å…¬å¼: 2Ã—(ç²¾ç¡®ç‡Ã—å¬å›ç‡)/(ç²¾ç¡®ç‡+å¬å›ç‡)\nèŒƒå›´: 0-1ï¼Œè¶Šé«˜è¶Šå¥½",
            "æ”¯æŒæ ·æœ¬æ•°": "æµ‹è¯•é›†ä¸­è¯¥ç±»åˆ«çš„å®é™…æ ·æœ¬æ•°é‡\nç”¨äºè®¡ç®—åŠ æƒå¹³å‡æ—¶çš„æƒé‡ä¾æ®"
        }
        
        class_names = result['class_names']
        class_precision = result['class_precision']
        class_recall = result['class_recall']
        class_f1 = result['class_f1']
        class_support = result['class_support']
        
        self.metrics_table.setRowCount(len(class_names))
        
        # ä¸ºè¡¨å¤´è®¾ç½®å·¥å…·æç¤º
        header_labels = ["ç±»åˆ«", "ç²¾ç¡®ç‡", "å¬å›ç‡", "F1åˆ†æ•°", "æ”¯æŒæ ·æœ¬æ•°"]
        for i, header_text in enumerate(header_labels):
            if header_text in metrics_header_tooltips:
                header_item = QTableWidgetItem(header_text)
                header_item.setToolTip(metrics_header_tooltips[header_text])
                self.metrics_table.setHorizontalHeaderItem(i, header_item)
        
        for i, class_name in enumerate(class_names):
            self.metrics_table.setItem(i, 0, QTableWidgetItem(class_name))
            self.metrics_table.setItem(i, 1, QTableWidgetItem(f"{class_precision[i]:.4f}"))
            self.metrics_table.setItem(i, 2, QTableWidgetItem(f"{class_recall[i]:.4f}"))
            self.metrics_table.setItem(i, 3, QTableWidgetItem(f"{class_f1[i]:.4f}"))
            self.metrics_table.setItem(i, 4, QTableWidgetItem(f"{int(class_support[i])}"))
    
    def update_confusion_matrix(self, result):
        """æ›´æ–°æ··æ·†çŸ©é˜µå›¾è¡¨"""
        self.confusion_figure.clear()
        ax = self.confusion_figure.add_subplot(111)
        
        cm = np.array(result['confusion_matrix'])
        class_names = result['class_names']
        
        # æ™ºèƒ½å¤„ç†ç±»åˆ«åç§°æ˜¾ç¤º
        def get_display_class_name(name, max_length=8):
            """è·å–é€‚åˆæ˜¾ç¤ºçš„ç±»åˆ«åç§°"""
            if len(name) <= max_length:
                return name
            return name[:max_length-2] + '..'
        
        display_class_names = [get_display_class_name(name) for name in class_names]
        
        # ä½¿ç”¨seabornç»˜åˆ¶çƒ­åŠ›å›¾
        heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                             xticklabels=display_class_names, yticklabels=display_class_names, ax=ax)
        
        ax.set_title('æ··æ·†çŸ©é˜µ', fontsize=12)
        ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=10)
        ax.set_ylabel('çœŸå®ç±»åˆ«', fontsize=10)
        
        # æ ¹æ®ç±»åˆ«æ•°é‡è°ƒæ•´æ ‡ç­¾æ˜¾ç¤º
        if len(class_names) <= 5:
            # ç±»åˆ«å°‘æ—¶ï¼Œä¸æ—‹è½¬
            ax.set_xticklabels(display_class_names, rotation=0, fontsize=9)
            ax.set_yticklabels(display_class_names, rotation=0, fontsize=9)
        elif len(class_names) <= 10:
            # ä¸­ç­‰æ•°é‡ç±»åˆ«ï¼Œå°è§’åº¦æ—‹è½¬
            ax.set_xticklabels(display_class_names, rotation=30, fontsize=8, ha='right')
            ax.set_yticklabels(display_class_names, rotation=0, fontsize=8)
        else:
            # ç±»åˆ«å¤šæ—¶ï¼Œå¤§è§’åº¦æ—‹è½¬ï¼Œå­—ä½“æ›´å°
            ax.set_xticklabels(display_class_names, rotation=45, fontsize=7, ha='right')
            ax.set_yticklabels(display_class_names, rotation=0, fontsize=7)
        
        # æ·»åŠ æ‚¬åœåŠŸèƒ½æ˜¾ç¤ºå®Œæ•´ç±»åˆ«åç§°å’Œè¯¦ç»†ä¿¡æ¯
        try:
            # å­˜å‚¨æ··æ·†çŸ©é˜µæ•°æ®ä»¥ä¾›æ‚¬åœåŠŸèƒ½ä½¿ç”¨
            self.current_cm = cm
            self.current_class_names = class_names
            
            # ä¸ºæ··æ·†çŸ©é˜µæ·»åŠ è‡ªå®šä¹‰æ‚¬åœåŠŸèƒ½
            def on_hover(event):
                if event.inaxes == ax and event.xdata is not None and event.ydata is not None:
                    # seaborn heatmapçš„åæ ‡ç³»ç»Ÿï¼š
                    # - xè½´ä»å·¦åˆ°å³å¯¹åº”é¢„æµ‹ç±»åˆ«ï¼ˆåˆ—ç´¢å¼•ï¼‰
                    # - yè½´ä»ä¸Šåˆ°ä¸‹å¯¹åº”çœŸå®ç±»åˆ«ï¼ˆè¡Œç´¢å¼•ï¼‰
                    # - æ¯ä¸ªæ ¼å­çš„ä¸­å¿ƒåœ¨æ•´æ•°åæ ‡ä¸Š (0, 0), (1, 0), (0, 1) ç­‰
                    
                    # ä½¿ç”¨å››èˆäº”å…¥æ¥è·å–æœ€è¿‘çš„æ ¼å­ç´¢å¼•
                    x_idx = int(round(event.xdata))
                    y_idx = int(round(event.ydata))
                    
                    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if 0 <= x_idx < len(class_names) and 0 <= y_idx < len(class_names):
                        predicted_class = class_names[x_idx]
                        true_class = class_names[y_idx]
                        count = cm[y_idx, x_idx]  # æ··æ·†çŸ©é˜µä¸­ [çœŸå®ç±»åˆ«è¡Œ, é¢„æµ‹ç±»åˆ«åˆ—]
                        
                        # è®¡ç®—åœ¨è¯¥çœŸå®ç±»åˆ«ä¸­çš„å æ¯”
                        total_true = np.sum(cm[y_idx, :])
                        percentage = (count / total_true * 100) if total_true > 0 else 0
                        
                        # åˆ¤æ–­æ˜¯å¦ä¸ºæ­£ç¡®åˆ†ç±»ï¼ˆå¯¹è§’çº¿å…ƒç´ ï¼‰
                        if x_idx == y_idx:
                            classification_type = "âœ… æ­£ç¡®åˆ†ç±»"
                            style_color = "#d4edda"  # ç»¿è‰²èƒŒæ™¯
                            border_color = "#28a745"
                        else:
                            classification_type = "âŒ é”™è¯¯åˆ†ç±»"
                            style_color = "#f8d7da"  # çº¢è‰²èƒŒæ™¯
                            border_color = "#dc3545"
                        
                        # åˆ›å»ºè¯¦ç»†çš„æ‚¬åœæç¤º
                        hover_text = f"ğŸ¯ çœŸå®: {true_class} | ğŸ”® é¢„æµ‹: {predicted_class} | ğŸ“Š æ ·æœ¬: {count} | ğŸ“ˆ å æ¯”: {percentage:.1f}% | {classification_type}"
                        
                        # æ›´æ–°æ‚¬åœä¿¡æ¯æ ‡ç­¾
                        if hasattr(self, 'confusion_hover_label'):
                            self.confusion_hover_label.setText(hover_text)
                            self.confusion_hover_label.setStyleSheet(f"""
                                QLabel {{
                                    background-color: {style_color};
                                    border: 2px solid {border_color};
                                    padding: 8px;
                                    border-radius: 5px;
                                    font-size: 12px;
                                    color: #2c3e50;
                                    font-weight: bold;
                                }}
                            """)
                    else:
                        # é¼ æ ‡ç§»å‡ºæœ‰æ•ˆåŒºåŸŸæ—¶æ¢å¤é»˜è®¤ä¿¡æ¯
                        if hasattr(self, 'confusion_hover_label'):
                            self.confusion_hover_label.setText("å°†é¼ æ ‡æ‚¬åœåœ¨æ··æ·†çŸ©é˜µä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
                            self.confusion_hover_label.setStyleSheet("""
                                QLabel {
                                    background-color: #f0f0f0;
                                    border: 1px solid #ccc;
                                    padding: 5px;
                                    border-radius: 3px;
                                    font-size: 12px;
                                    color: #666;
                                }
                            """)
            
            # è¿æ¥æ‚¬åœäº‹ä»¶
            self.confusion_canvas.mpl_connect('motion_notify_event', on_hover)
            
        except Exception as e:
            print(f"æ·»åŠ æ··æ·†çŸ©é˜µæ‚¬åœåŠŸèƒ½æ—¶å‡ºé”™: {e}")
        
        # è®¾ç½®æ›´å¤šçš„è¾¹è·ä»¥å®¹çº³æ—‹è½¬çš„æ ‡ç­¾
        self.confusion_figure.tight_layout(pad=2.0, rect=[0.05, 0.1, 0.95, 0.95])
        self.confusion_canvas.draw()
    
    def update_classification_report(self, result):
        """æ›´æ–°åˆ†ç±»æŠ¥å‘Š"""
        if 'classification_report' in result:
            report_dict = result['classification_report']
            
            # æ ¼å¼åŒ–åˆ†ç±»æŠ¥å‘Š
            report_text = "åˆ†ç±»æŠ¥å‘Š\n" + "="*50 + "\n\n"
            
            # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†ä¿¡æ¯
            for class_name in result['class_names']:
                if class_name in report_dict:
                    class_data = report_dict[class_name]
                    report_text += f"ç±»åˆ«: {class_name}\n"
                    report_text += f"  ç²¾ç¡®ç‡: {class_data['precision']:.4f}\n"
                    report_text += f"  å¬å›ç‡: {class_data['recall']:.4f}\n"
                    report_text += f"  F1åˆ†æ•°: {class_data['f1-score']:.4f}\n"
                    report_text += f"  æ”¯æŒæ ·æœ¬æ•°: {int(class_data['support'])}\n\n"
            
            # æ·»åŠ æ€»ä½“ç»Ÿè®¡
            if 'accuracy' in report_dict:
                report_text += f"æ€»ä½“å‡†ç¡®ç‡: {report_dict['accuracy']:.4f}\n\n"
            
            if 'macro avg' in report_dict:
                macro_avg = report_dict['macro avg']
                report_text += "å®å¹³å‡:\n"
                report_text += f"  ç²¾ç¡®ç‡: {macro_avg['precision']:.4f}\n"
                report_text += f"  å¬å›ç‡: {macro_avg['recall']:.4f}\n"
                report_text += f"  F1åˆ†æ•°: {macro_avg['f1-score']:.4f}\n\n"
            
            if 'weighted avg' in report_dict:
                weighted_avg = report_dict['weighted avg']
                report_text += "åŠ æƒå¹³å‡:\n"
                report_text += f"  ç²¾ç¡®ç‡: {weighted_avg['precision']:.4f}\n"
                report_text += f"  å¬å›ç‡: {weighted_avg['recall']:.4f}\n"
                report_text += f"  F1åˆ†æ•°: {weighted_avg['f1-score']:.4f}\n"
            
            self.report_text.setPlainText(report_text)
    
    def apply_config(self, config):
        """åº”ç”¨é…ç½®"""
        if not config:
            return
            
        # è®¾ç½®æ¨¡å‹è¯„ä¼°ç›®å½•
        if 'default_model_eval_dir' in config:
            model_dir = config['default_model_eval_dir']
            if os.path.exists(model_dir):
                self.models_path_edit.setText(model_dir)
                self.models_dir = model_dir
                self.refresh_model_list()
        
        # è®¾ç½®é»˜è®¤æµ‹è¯•é›†ç›®å½•
        if 'default_test_data_dir' in config:
            test_dir = config['default_test_data_dir']
            if os.path.exists(test_dir):
                self.test_data_path_edit.setText(test_dir)
                self.test_data_dir = test_dir
        elif 'default_output_folder' in config:
            # å°è¯•ä»è¾“å‡ºæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æµ‹è¯•é›†
            output_folder = config['default_output_folder']
            possible_test_dirs = [
                os.path.join(output_folder, 'dataset', 'val'),
                os.path.join(output_folder, 'dataset', 'test'),
                os.path.join(output_folder, 'detection_data'),
                os.path.join(output_folder, 'test_data')
            ]
            
            for test_dir in possible_test_dirs:
                if os.path.exists(test_dir):
                    self.test_data_path_edit.setText(test_dir)
                    self.test_data_dir = test_dir
                    break 
    
    def on_model_type_changed(self, model_type):
        """å½“æ¨¡å‹ç±»å‹æ”¹å˜æ—¶æ›´æ–°æ¶æ„åˆ—è¡¨"""
        self.arch_combo.clear()
        if model_type == "åˆ†ç±»æ¨¡å‹":
            self.supported_architectures = self.classification_architectures
        else:  # æ£€æµ‹æ¨¡å‹
            self.supported_architectures = self.detection_architectures
        
        self.arch_combo.addItems(self.supported_architectures) 