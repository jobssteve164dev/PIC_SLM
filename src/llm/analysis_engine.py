"""
Training Analysis Engine

This module provides intelligent analysis capabilities for training metrics,
including performance analysis, optimization suggestions, and problem diagnosis.
"""

import json
import time
import os
import glob
from typing import Dict, List, Optional, Any, Tuple
from .model_adapters import LLMAdapter, MockLLMAdapter
from .prompt_templates import PromptTemplates, PromptBuilder


class TrainingAnalysisEngine:
    """è®­ç»ƒåˆ†æå¼•æ“"""
    
    def __init__(self, llm_adapter: LLMAdapter = None):
        self.llm = llm_adapter or MockLLMAdapter()
        self.prompt_templates = PromptTemplates()
        self.prompt_builder = PromptBuilder()
        self.analysis_history = []
        self.metrics_buffer = []
        
        # å®æ—¶æŒ‡æ ‡é‡‡é›†å™¨ - å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        self.metrics_collector = None
        
        # è®­ç»ƒé…ç½®æ–‡ä»¶ç¼“å­˜
        self.training_config_cache = {}
        self.config_cache_timestamp = 0
        
    def _find_latest_training_config(self) -> Optional[Dict[str, Any]]:
        """æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒé…ç½®æ–‡ä»¶"""
        try:
            # æŸ¥æ‰¾è®­ç»ƒé…ç½®æ–‡ä»¶ç›®å½•
            config_dirs = [
                "models/params/classification",
                "models/params/detection", 
                "train_config",
                "models/saved_models"
            ]
            
            latest_config = None
            latest_timestamp = 0
            
            for config_dir in config_dirs:
                if not os.path.exists(config_dir):
                    continue
                    
                # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
                patterns = [
                    os.path.join(config_dir, "*_config.json"),
                    os.path.join(config_dir, "*.json")
                ]
                
                for pattern in patterns:
                    config_files = glob.glob(pattern)
                    for config_file in config_files:
                        try:
                            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                            file_timestamp = os.path.getmtime(config_file)
                            
                            # è¯»å–é…ç½®æ–‡ä»¶
                            with open(config_file, 'r', encoding='utf-8') as f:
                                config_data = json.load(f)
                                
                            # æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒé…ç½®æ–‡ä»¶ï¼ˆåŒ…å«å…³é”®è®­ç»ƒå‚æ•°ï¼‰
                            if self._is_training_config(config_data):
                                if file_timestamp > latest_timestamp:
                                    latest_timestamp = file_timestamp
                                    latest_config = {
                                        'file_path': config_file,
                                        'timestamp': file_timestamp,
                                        'config': config_data
                                    }
                                    
                        except Exception as e:
                            print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_file}: {str(e)}")
                            continue
            
            return latest_config
            
        except Exception as e:
            print(f"æŸ¥æ‰¾è®­ç»ƒé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _is_training_config(self, config_data: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè®­ç»ƒé…ç½®æ–‡ä»¶"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è®­ç»ƒå‚æ•°
        training_keys = [
            'model_name', 'num_epochs', 'batch_size', 'learning_rate',
            'task_type', 'optimizer', 'data_dir'
        ]
        
        return any(key in config_data for key in training_keys)
    
    def _get_training_config_context(self) -> str:
        """è·å–è®­ç»ƒé…ç½®ä¸Šä¸‹æ–‡"""
        try:
            # æŸ¥æ‰¾æœ€æ–°é…ç½®
            latest_config = self._find_latest_training_config()
            
            if not latest_config:
                return "âš ï¸ **è®­ç»ƒé…ç½®**: æœªæ‰¾åˆ°è®­ç»ƒé…ç½®æ–‡ä»¶"
            
            config_data = latest_config['config']
            file_path = latest_config['file_path']
            
            # æ„å»ºé…ç½®ä¸Šä¸‹æ–‡
            context = f"""
## ğŸ“‹ è®­ç»ƒé…ç½®ä¿¡æ¯
**é…ç½®æ–‡ä»¶**: {os.path.basename(file_path)}
**é…ç½®æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(latest_config['timestamp']))}

### ğŸ—ï¸ æ¨¡å‹é…ç½®
- **æ¨¡å‹æ¶æ„**: {config_data.get('model_name', 'N/A')}
- **ä»»åŠ¡ç±»å‹**: {config_data.get('task_type', 'N/A')}
- **æ¿€æ´»å‡½æ•°**: {config_data.get('activation_function', 'N/A')}
- **Dropoutç‡**: {config_data.get('dropout_rate', 'N/A')}

### âš™ï¸ è®­ç»ƒå‚æ•°
- **è®­ç»ƒè½®æ•°**: {config_data.get('num_epochs', 'N/A')}
- **æ‰¹æ¬¡å¤§å°**: {config_data.get('batch_size', 'N/A')}
- **å­¦ä¹ ç‡**: {config_data.get('learning_rate', 'N/A')}
- **ä¼˜åŒ–å™¨**: {config_data.get('optimizer', 'N/A')}
- **æƒé‡è¡°å‡**: {config_data.get('weight_decay', 'N/A')}
- **å­¦ä¹ ç‡è°ƒåº¦**: {config_data.get('lr_scheduler', 'N/A')}

### ğŸ”§ é«˜çº§é…ç½®
- **æ•°æ®å¢å¼º**: {'å¯ç”¨' if config_data.get('use_augmentation') else 'ç¦ç”¨'}
- **æ—©åœæœºåˆ¶**: {'å¯ç”¨' if config_data.get('early_stopping') else 'ç¦ç”¨'}
- **æ—©åœè€å¿ƒå€¼**: {config_data.get('early_stopping_patience', 'N/A')}
- **æ¢¯åº¦è£å‰ª**: {'å¯ç”¨' if config_data.get('gradient_clipping') else 'ç¦ç”¨'}
- **æ··åˆç²¾åº¦**: {'å¯ç”¨' if config_data.get('mixed_precision') else 'ç¦ç”¨'}

### âš–ï¸ ç±»åˆ«æƒé‡é…ç½®
- **ä½¿ç”¨ç±»åˆ«æƒé‡**: {'æ˜¯' if config_data.get('use_class_weights') else 'å¦'}
- **æƒé‡ç­–ç•¥**: {config_data.get('weight_strategy', 'N/A')}
"""
            
            # æ·»åŠ ç±»åˆ«æƒé‡è¯¦æƒ…
            if config_data.get('class_weights'):
                context += "\n**ç±»åˆ«æƒé‡è¯¦æƒ…**:\n"
                for class_name, weight in config_data['class_weights'].items():
                    context += f"- {class_name}: {weight}\n"
            
            # æ·»åŠ é«˜çº§è¶…å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            advanced_params = []
            if config_data.get('warmup_enabled'):
                advanced_params.append(f"é¢„çƒ­æ­¥æ•°: {config_data.get('warmup_steps', 'N/A')}")
            if config_data.get('min_lr_enabled'):
                advanced_params.append(f"æœ€å°å­¦ä¹ ç‡: {config_data.get('min_lr', 'N/A')}")
            if config_data.get('label_smoothing_enabled'):
                advanced_params.append(f"æ ‡ç­¾å¹³æ»‘: {config_data.get('label_smoothing', 'N/A')}")
            if config_data.get('model_ema'):
                advanced_params.append(f"æ¨¡å‹EMA: {config_data.get('model_ema_decay', 'N/A')}")
            
            if advanced_params:
                context += "\n**é«˜çº§è¶…å‚æ•°**:\n"
                for param in advanced_params:
                    context += f"- {param}\n"
            
            # æ·»åŠ ç›®æ ‡æ£€æµ‹ç‰¹æœ‰å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if config_data.get('task_type') == 'detection':
                context += f"""
### ğŸ¯ ç›®æ ‡æ£€æµ‹ç‰¹æœ‰å‚æ•°
- **IOUé˜ˆå€¼**: {config_data.get('iou_threshold', 'N/A')}
- **ç½®ä¿¡åº¦é˜ˆå€¼**: {config_data.get('conf_threshold', 'N/A')}
- **åˆ†è¾¨ç‡**: {config_data.get('resolution', 'N/A')}
- **é©¬èµ›å…‹å¢å¼º**: {'å¯ç”¨' if config_data.get('use_mosaic') else 'ç¦ç”¨'}
- **å¤šå°ºåº¦è®­ç»ƒ**: {'å¯ç”¨' if config_data.get('use_multiscale') else 'ç¦ç”¨'}
- **EMA**: {'å¯ç”¨' if config_data.get('use_ema') else 'ç¦ç”¨'}
"""
            
            return context.strip()
            
        except Exception as e:
            return f"âš ï¸ **è®­ç»ƒé…ç½®**: è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
    
    def _build_enhanced_analysis_prompt(self, metrics_data: Dict, trends: Dict, real_data: Dict) -> str:
        """æ„å»ºå¢å¼ºçš„åˆ†ææç¤ºè¯ï¼ˆåŒ…å«è®­ç»ƒé…ç½®ï¼‰"""
        # è·å–è®­ç»ƒé…ç½®ä¸Šä¸‹æ–‡
        config_context = self._get_training_config_context()
        
        return f"""
è¯·åŸºäºä»¥ä¸‹å®Œæ•´ä¿¡æ¯è¿›è¡Œä¸“ä¸šçš„æ·±åº¦å­¦ä¹ è®­ç»ƒåˆ†æï¼š

{config_context}

## ğŸ“Š å®æ—¶è®­ç»ƒæ•°æ®
- è®­ç»ƒä¼šè¯: {real_data.get('session_id', 'Unknown')}
- æ•°æ®é‡‡é›†æ—¶é•¿: {real_data.get('collection_duration', 0):.1f}ç§’
- æ•°æ®ç‚¹æ•°é‡: {real_data.get('total_data_points', 0)}ä¸ª
- è®­ç»ƒçŠ¶æ€: {real_data.get('training_status', 'unknown')}

## ğŸ“ˆ å½“å‰è®­ç»ƒæŒ‡æ ‡
- å½“å‰Epoch: {metrics_data.get('epoch', 'N/A')}
- è®­ç»ƒæŸå¤±: {metrics_data.get('train_loss', 'N/A')}
- éªŒè¯æŸå¤±: {metrics_data.get('val_loss', 'N/A')}
- è®­ç»ƒå‡†ç¡®ç‡: {metrics_data.get('train_accuracy', 'N/A')}
- éªŒè¯å‡†ç¡®ç‡: {metrics_data.get('val_accuracy', 'N/A')}

## ğŸ“‰ è®­ç»ƒè¶‹åŠ¿åˆ†æ
- è®­ç»ƒæŸå¤±è¶‹åŠ¿: {trends.get('train_losses', [])}
- éªŒè¯æŸå¤±è¶‹åŠ¿: {trends.get('val_losses', [])}
- è®­ç»ƒå‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('train_accuracies', [])}
- éªŒè¯å‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('val_accuracies', [])}

## ğŸ¯ åˆ†æè¦æ±‚

è¯·åŸºäºä»¥ä¸Šè®­ç»ƒé…ç½®å’Œå®æ—¶æ•°æ®ï¼Œæä¾›ä»¥ä¸‹åˆ†æï¼š

### 1. å‚æ•°é…ç½®è¯„ä¼°
- å½“å‰å‚æ•°é…ç½®æ˜¯å¦åˆç†ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„å‚æ•°å†²çªæˆ–ä¸å½“è®¾ç½®ï¼Ÿ
- é’ˆå¯¹å½“å‰æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå‚æ•°æ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ

### 2. è®­ç»ƒçŠ¶æ€è¯Šæ–­
- å½“å‰è®­ç»ƒçŠ¶æ€ï¼ˆæ”¶æ•›æƒ…å†µã€è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆé£é™©ï¼‰
- åŸºäºè¶‹åŠ¿æ•°æ®çš„è®­ç»ƒè¿›å±•è¯„ä¼°
- ä¸é…ç½®å‚æ•°çš„å…³è”åˆ†æ

### 3. é’ˆå¯¹æ€§ä¼˜åŒ–å»ºè®®
- åŸºäºå…·ä½“å‚æ•°é…ç½®çš„ä¼˜åŒ–å»ºè®®
- å‚æ•°è°ƒæ•´çš„ä¼˜å…ˆçº§å’Œé¢„æœŸæ•ˆæœ
- é’ˆå¯¹å½“å‰é…ç½®çš„å…·ä½“æ”¹è¿›æ–¹æ¡ˆ

### 4. æ½œåœ¨é—®é¢˜é¢„è­¦
- éœ€è¦å…³æ³¨çš„æ½œåœ¨é—®é¢˜
- é…ç½®å‚æ•°å¯èƒ½å¸¦æ¥çš„é£é™©
- é¢„é˜²æªæ–½å’Œå»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå®ç”¨æ€§ï¼Œé‡ç‚¹å…³æ³¨å‚æ•°é…ç½®ä¸è®­ç»ƒè¡¨ç°çš„å…³è”æ€§ã€‚
"""

    def analyze_real_training_progress(self) -> Dict[str, Any]:
        """åˆ†æçœŸå®çš„è®­ç»ƒè¿›åº¦ï¼ˆä½¿ç”¨å®æ—¶é‡‡é›†çš„æ•°æ®ï¼‰"""
        try:
            # å»¶è¿Ÿå¯¼å…¥å¹¶è·å–æŒ‡æ ‡é‡‡é›†å™¨
            if self.metrics_collector is None:
                try:
                    import sys
                    import os
                    sys.path.append(os.path.dirname(os.path.dirname(__file__)))
                    from training_components.real_time_metrics_collector import get_global_metrics_collector
                    self.metrics_collector = get_global_metrics_collector()
                except ImportError:
                    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œç›´æ¥è¯»å–æ–‡ä»¶
                    return self._analyze_from_file()
            
            # è·å–çœŸå®çš„è®­ç»ƒæ•°æ®
            real_data = self.metrics_collector.get_current_training_data_for_ai()
            
            if "error" in real_data:
                return {
                    'error': f"æ— æ³•è·å–çœŸå®è®­ç»ƒæ•°æ®: {real_data['error']}",
                    'timestamp': time.time(),
                    'data_source': 'real_time_collector'
                }
            
            # ä»çœŸå®æ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡
            current_metrics = real_data.get('current_metrics', {})
            trends = real_data.get('training_trends', {})
            
            # æ„å»ºåˆ†æç”¨çš„æŒ‡æ ‡æ•°æ®
            if current_metrics:
                metrics_data = {
                    'epoch': current_metrics.get('epoch', 0),
                    'train_loss': trends.get('train_losses', [])[-1] if trends.get('train_losses') else current_metrics.get('loss', 0),
                    'val_loss': trends.get('val_losses', [])[-1] if trends.get('val_losses') else current_metrics.get('loss', 0),
                    'train_accuracy': trends.get('train_accuracies', [])[-1] if trends.get('train_accuracies') else current_metrics.get('accuracy', 0),
                    'val_accuracy': trends.get('val_accuracies', [])[-1] if trends.get('val_accuracies') else current_metrics.get('accuracy', 0),
                    'training_duration': real_data.get('collection_duration', 0),
                    'data_points': real_data.get('total_data_points', 0),
                    'training_status': real_data.get('training_status', 'unknown')
                }
            else:
                return {
                    'error': 'å½“å‰æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæŒ‡æ ‡æ•°æ®',
                    'timestamp': time.time(),
                    'data_source': 'real_time_collector'
                }
            
            # æ„å»ºå¢å¼ºçš„åˆ†ææç¤ºè¯ï¼ŒåŒ…å«è®­ç»ƒé…ç½®å’ŒçœŸå®æ•°æ®ä¸Šä¸‹æ–‡
            prompt = self._build_enhanced_analysis_prompt(metrics_data, trends, real_data)
            
            # è·å–LLMåˆ†æï¼ˆä½¿ç”¨å¢å¼ºçš„æç¤ºè¯ï¼‰
            llm_analysis = self.llm.analyze_metrics(metrics_data, prompt)
            
            # ç»“åˆè§„åˆ™åˆ†æ
            rule_analysis = self._rule_based_analysis(metrics_data)
            
            # ç”Ÿæˆç»¼åˆåˆ†æç»“æœ
            analysis_result = {
                'timestamp': time.time(),
                'data_source': 'real_time_collector',
                'session_id': real_data.get('session_id', 'unknown'),
                'metrics': metrics_data,
                'trends': trends,
                'raw_data_summary': {
                    'total_data_points': real_data.get('total_data_points', 0),
                    'collection_duration': real_data.get('collection_duration', 0),
                    'training_status': real_data.get('training_status', 'unknown')
                },
                'llm_analysis': llm_analysis,
                'rule_analysis': rule_analysis,
                'combined_insights': self._combine_real_data_analyses(llm_analysis, rule_analysis, real_data),
                'recommendations': self._generate_recommendations(metrics_data, llm_analysis, rule_analysis),
                'alerts': self._check_alerts(metrics_data)
            }
            
            # æ·»åŠ åˆ°åˆ†æå†å²
            self.analysis_history.append(analysis_result)
            if len(self.analysis_history) > 50:
                self.analysis_history.pop(0)
            
            return analysis_result
            
        except Exception as e:
            return {
                'error': f"çœŸå®æ•°æ®åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                'timestamp': time.time(),
                'data_source': 'real_time_collector'
            }
        
    def analyze_training_progress(self, metrics_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè®­ç»ƒè¿›åº¦"""
        try:
            # æ·»åŠ åˆ°æŒ‡æ ‡ç¼“å†²åŒº
            self.metrics_buffer.append({
                'timestamp': time.time(),
                'metrics': metrics_data.copy()
            })
            
            # ä¿æŒæœ€è¿‘100ä¸ªæŒ‡æ ‡
            if len(self.metrics_buffer) > 100:
                self.metrics_buffer.pop(0)
            
            # æ„å»ºå¢å¼ºçš„åˆ†ææç¤ºè¯ï¼ŒåŒ…å«è®­ç»ƒé…ç½®
            config_context = self._get_training_config_context()
            prompt = f"""
è¯·åŸºäºä»¥ä¸‹å®Œæ•´ä¿¡æ¯è¿›è¡Œä¸“ä¸šçš„æ·±åº¦å­¦ä¹ è®­ç»ƒåˆ†æï¼š

{config_context}

## ğŸ“Š è®­ç»ƒæŒ‡æ ‡æ•°æ®
{json.dumps(metrics_data, ensure_ascii=False, indent=2)}

## ğŸ¯ åˆ†æè¦æ±‚

è¯·åŸºäºä»¥ä¸Šè®­ç»ƒé…ç½®å’ŒæŒ‡æ ‡æ•°æ®ï¼Œæä¾›ä»¥ä¸‹åˆ†æï¼š

### 1. å‚æ•°é…ç½®è¯„ä¼°
- å½“å‰å‚æ•°é…ç½®æ˜¯å¦åˆç†ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„å‚æ•°å†²çªæˆ–ä¸å½“è®¾ç½®ï¼Ÿ
- é’ˆå¯¹å½“å‰æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå‚æ•°æ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ

### 2. è®­ç»ƒçŠ¶æ€è¯Šæ–­
- å½“å‰è®­ç»ƒçŠ¶æ€ï¼ˆæ”¶æ•›æƒ…å†µã€è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆé£é™©ï¼‰
- åŸºäºæŒ‡æ ‡æ•°æ®çš„è®­ç»ƒè¿›å±•è¯„ä¼°
- ä¸é…ç½®å‚æ•°çš„å…³è”åˆ†æ

### 3. é’ˆå¯¹æ€§ä¼˜åŒ–å»ºè®®
- åŸºäºå…·ä½“å‚æ•°é…ç½®çš„ä¼˜åŒ–å»ºè®®
- å‚æ•°è°ƒæ•´çš„ä¼˜å…ˆçº§å’Œé¢„æœŸæ•ˆæœ
- é’ˆå¯¹å½“å‰é…ç½®çš„å…·ä½“æ”¹è¿›æ–¹æ¡ˆ

### 4. æ½œåœ¨é—®é¢˜é¢„è­¦
- éœ€è¦å…³æ³¨çš„æ½œåœ¨é—®é¢˜
- é…ç½®å‚æ•°å¯èƒ½å¸¦æ¥çš„é£é™©
- é¢„é˜²æªæ–½å’Œå»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå®ç”¨æ€§ï¼Œé‡ç‚¹å…³æ³¨å‚æ•°é…ç½®ä¸è®­ç»ƒè¡¨ç°çš„å…³è”æ€§ã€‚
"""
            
            # è·å–LLMåˆ†æï¼ˆä½¿ç”¨å¢å¼ºçš„æç¤ºè¯ï¼‰
            llm_analysis = self.llm.analyze_metrics(metrics_data, prompt)
            
            # ç»“åˆè§„åˆ™åˆ†æ
            rule_analysis = self._rule_based_analysis(metrics_data)
            
            # ç”Ÿæˆç»¼åˆåˆ†æç»“æœ
            analysis_result = {
                'timestamp': time.time(),
                'metrics': metrics_data,
                'llm_analysis': llm_analysis,
                'rule_analysis': rule_analysis,
                'combined_insights': self._combine_analyses(llm_analysis, rule_analysis),
                'recommendations': self._generate_recommendations(metrics_data, llm_analysis, rule_analysis),
                'alerts': self._check_alerts(metrics_data)
            }
            
            # æ·»åŠ åˆ°åˆ†æå†å²
            self.analysis_history.append(analysis_result)
            if len(self.analysis_history) > 50:
                self.analysis_history.pop(0)
            
            return analysis_result
            
        except Exception as e:
            return {
                'error': f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                'timestamp': time.time(),
                'metrics': metrics_data
            }
    
    def suggest_hyperparameter_tuning(self, current_metrics: Dict, 
                                    current_params: Dict = None) -> Dict[str, Any]:
        """å»ºè®®è¶…å‚æ•°è°ƒä¼˜"""
        try:
            # è·å–å†å²æŒ‡æ ‡
            history = [item['metrics'] for item in self.metrics_buffer[-10:]]
            
            # æ„å»ºè°ƒä¼˜æç¤ºè¯
            prompt = self.prompt_templates.build_hyperparameter_tuning_prompt(
                current_metrics, history, current_params
            )
            
            # è·å–LLMå»ºè®®
            llm_suggestions = self.llm.generate_response(
                prompt, 
                context={'type': 'hyperparameter_tuning', 'metrics': current_metrics}
            )
            
            # ç”Ÿæˆè§„åˆ™å»ºè®®
            rule_suggestions = self._rule_based_hyperparameter_suggestions(
                current_metrics, history, current_params
            )
            
            return {
                'timestamp': time.time(),
                'current_metrics': current_metrics,
                'current_params': current_params,
                'llm_suggestions': llm_suggestions,
                'rule_suggestions': rule_suggestions,
                'priority_actions': self._prioritize_suggestions(rule_suggestions),
                'expected_improvements': self._estimate_improvements(rule_suggestions)
            }
            
        except Exception as e:
            return {
                'error': f"è¶…å‚æ•°è°ƒä¼˜å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}",
                'timestamp': time.time()
            }
    
    def diagnose_training_issues(self, metrics_data: Dict, error_info: str = None) -> Dict[str, Any]:
        """è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        try:
            # æ£€æµ‹å¼‚å¸¸æ¨¡å¼
            anomalies = self._detect_anomalies(metrics_data)
            
            # æ„å»ºè¯Šæ–­æç¤ºè¯
            prompt = self.prompt_templates.build_problem_diagnosis_prompt(metrics_data, error_info)
            
            # è·å–LLMè¯Šæ–­
            llm_diagnosis = self.llm.generate_response(
                prompt,
                context={'type': 'problem_diagnosis', 'anomalies': anomalies}
            )
            
            # è§„åˆ™è¯Šæ–­
            rule_diagnosis = self._rule_based_diagnosis(metrics_data, anomalies)
            
            return {
                'timestamp': time.time(),
                'metrics': metrics_data,
                'detected_anomalies': anomalies,
                'llm_diagnosis': llm_diagnosis,
                'rule_diagnosis': rule_diagnosis,
                'severity_assessment': self._assess_severity(anomalies),
                'recommended_actions': self._recommend_actions(anomalies, rule_diagnosis),
                'prevention_tips': self._generate_prevention_tips(anomalies)
            }
            
        except Exception as e:
            return {
                'error': f"é—®é¢˜è¯Šæ–­å¤±è´¥: {str(e)}",
                'timestamp': time.time()
            }
    
    def compare_models(self, model_results: List[Dict]) -> Dict[str, Any]:
        """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
        try:
            # æ„å»ºå¯¹æ¯”æç¤ºè¯
            prompt = self.prompt_templates.build_model_comparison_prompt(model_results)
            
            # è·å–LLMå¯¹æ¯”åˆ†æ
            llm_comparison = self.llm.generate_response(
                prompt,
                context={'type': 'model_comparison', 'models_count': len(model_results)}
            )
            
            # è§„åˆ™å¯¹æ¯”
            rule_comparison = self._rule_based_model_comparison(model_results)
            
            return {
                'timestamp': time.time(),
                'models': model_results,
                'llm_comparison': llm_comparison,
                'rule_comparison': rule_comparison,
                'best_model': self._select_best_model(model_results),
                'performance_ranking': self._rank_models(model_results)
            }
            
        except Exception as e:
            return {
                'error': f"æ¨¡å‹å¯¹æ¯”å¤±è´¥: {str(e)}",
                'timestamp': time.time()
            }
    
    def chat_with_context(self, user_question: str) -> Dict[str, Any]:
        """åŸºäºè®­ç»ƒä¸Šä¸‹æ–‡çš„å¯¹è¯"""
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_chat_context()
            
            # æ„å»ºé—®é¢˜æç¤ºè¯
            prompt = self.prompt_templates.build_custom_question_prompt(user_question, context)
            
            # è·å–LLMå›ç­”
            response = self.llm.generate_response(prompt, context)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.prompt_builder.add_context({
                'type': 'user_question',
                'question': user_question,
                'response': response,
                'timestamp': time.time()
            })
            
            return {
                'timestamp': time.time(),
                'question': user_question,
                'response': response,
                'context_used': context,
                'llm_stats': self.llm.get_stats()
            }
            
        except Exception as e:
            return {
                'error': f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}",
                'timestamp': time.time(),
                'question': user_question
            }
    
    def _rule_based_analysis(self, metrics: Dict) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„åˆ†æ"""
        analysis = {
            'training_state': 'unknown',
            'convergence_status': 'unknown',
            'overfitting_risk': 'low',
            'performance_assessment': 'unknown'
        }
        
        train_loss = metrics.get('train_loss', 0)
        val_loss = metrics.get('val_loss', 0)
        train_acc = metrics.get('train_accuracy', 0)
        val_acc = metrics.get('val_accuracy', 0)
        epoch = metrics.get('epoch', 0)
        
        # åˆ¤æ–­è®­ç»ƒçŠ¶æ€
        if train_loss > 0 and val_loss > 0:
            if val_loss > train_loss * 1.5:
                analysis['training_state'] = 'overfitting'
                analysis['overfitting_risk'] = 'high'
            elif val_loss < train_loss * 0.8:
                analysis['training_state'] = 'underfitting'
            else:
                analysis['training_state'] = 'normal'
        
        # åˆ¤æ–­æ”¶æ•›çŠ¶æ€
        if len(self.metrics_buffer) >= 5:
            recent_losses = [item['metrics'].get('train_loss', 0) 
                           for item in self.metrics_buffer[-5:]]
            if len(recent_losses) >= 2:
                loss_change = abs(recent_losses[-1] - recent_losses[0])
                if loss_change < 0.001:
                    analysis['convergence_status'] = 'converged'
                elif recent_losses[-1] < recent_losses[0]:
                    analysis['convergence_status'] = 'converging'
                else:
                    analysis['convergence_status'] = 'diverging'
        
        # æ€§èƒ½è¯„ä¼°
        if val_acc > 0.9:
            analysis['performance_assessment'] = 'excellent'
        elif val_acc > 0.8:
            analysis['performance_assessment'] = 'good'
        elif val_acc > 0.7:
            analysis['performance_assessment'] = 'fair'
        else:
            analysis['performance_assessment'] = 'poor'
        
        return analysis
    
    def _rule_based_hyperparameter_suggestions(self, current_metrics: Dict, 
                                             history: List[Dict], 
                                             current_params: Dict = None) -> List[Dict]:
        """åŸºäºè§„åˆ™çš„è¶…å‚æ•°å»ºè®®"""
        suggestions = []
        
        train_loss = current_metrics.get('train_loss', 0)
        val_loss = current_metrics.get('val_loss', 0)
        learning_rate = current_metrics.get('learning_rate', 0.001)
        
        # å­¦ä¹ ç‡å»ºè®®
        if len(history) >= 3:
            recent_losses = [h.get('train_loss', 0) for h in history[-3:]]
            if all(l1 <= l2 for l1, l2 in zip(recent_losses[:-1], recent_losses[1:])):
                suggestions.append({
                    'parameter': 'learning_rate',
                    'current_value': learning_rate,
                    'suggested_value': learning_rate * 0.1,
                    'reason': 'è®­ç»ƒæŸå¤±åœæ­¢ä¸‹é™ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡',
                    'priority': 'high'
                })
        
        # è¿‡æ‹Ÿåˆå»ºè®®
        if val_loss > train_loss * 1.3:
            suggestions.append({
                'parameter': 'regularization',
                'current_value': 'unknown',
                'suggested_value': 'increase_dropout',
                'reason': 'æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–',
                'priority': 'high'
            })
        
        # æ‰¹é‡å¤§å°å»ºè®®
        gpu_memory = current_metrics.get('gpu_memory_used', 0)
        if gpu_memory < 4.0:  # GPUå†…å­˜ä½¿ç”¨è¾ƒä½
            suggestions.append({
                'parameter': 'batch_size',
                'current_value': current_params.get('batch_size', 32) if current_params else 32,
                'suggested_value': 64,
                'reason': 'GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥å¢åŠ æ‰¹é‡å¤§å°',
                'priority': 'medium'
            })
        
        return suggestions
    
    def _detect_anomalies(self, metrics: Dict) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸æ¨¡å¼"""
        anomalies = []
        
        train_loss = metrics.get('train_loss', 0)
        val_loss = metrics.get('val_loss', 0)
        learning_rate = metrics.get('learning_rate', 0)
        
        # æ£€æµ‹NaNæˆ–æ— ç©·å¤§
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                if value != value:  # NaNæ£€æµ‹
                    anomalies.append({
                        'type': 'nan_value',
                        'parameter': key,
                        'severity': 'critical',
                        'description': f'{key}å‡ºç°NaNå€¼'
                    })
                elif abs(value) == float('inf'):
                    anomalies.append({
                        'type': 'infinite_value',
                        'parameter': key,
                        'severity': 'critical',
                        'description': f'{key}å‡ºç°æ— ç©·å¤§å€¼'
                    })
        
        # æ£€æµ‹æŸå¤±çˆ†ç‚¸
        if train_loss > 100:
            anomalies.append({
                'type': 'loss_explosion',
                'parameter': 'train_loss',
                'severity': 'critical',
                'description': 'è®­ç»ƒæŸå¤±è¿‡å¤§ï¼Œå¯èƒ½å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸'
            })
        
        # æ£€æµ‹å­¦ä¹ ç‡è¿‡å¤§
        if learning_rate > 1.0:
            anomalies.append({
                'type': 'high_learning_rate',
                'parameter': 'learning_rate',
                'severity': 'high',
                'description': 'å­¦ä¹ ç‡è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š'
            })
        
        return anomalies
    
    def _combine_analyses(self, llm_analysis: str, rule_analysis: Dict) -> str:
        """ç»“åˆLLMå’Œè§„åˆ™åˆ†æ"""
        combined = f"""
## ç»¼åˆåˆ†æç»“æœ

### è§„åˆ™åˆ†ææ‘˜è¦
- è®­ç»ƒçŠ¶æ€: {rule_analysis.get('training_state', 'æœªçŸ¥')}
- æ”¶æ•›çŠ¶æ€: {rule_analysis.get('convergence_status', 'æœªçŸ¥')}
- è¿‡æ‹Ÿåˆé£é™©: {rule_analysis.get('overfitting_risk', 'æœªçŸ¥')}
- æ€§èƒ½è¯„ä¼°: {rule_analysis.get('performance_assessment', 'æœªçŸ¥')}

### AIåˆ†ææ´å¯Ÿ
{llm_analysis}

### ç»“è®º
åŸºäºè§„åˆ™åˆ†æå’ŒAIæ´å¯Ÿçš„ç»¼åˆåˆ¤æ–­ï¼Œå½“å‰è®­ç»ƒçŠ¶æ€ä¸º{rule_analysis.get('training_state', 'æœªçŸ¥')}ï¼Œ
å»ºè®®å…³æ³¨{rule_analysis.get('convergence_status', 'æ”¶æ•›')}æƒ…å†µã€‚
"""
        return combined
    
    def _build_real_data_analysis_prompt(self, metrics_data: Dict, trends: Dict, real_data: Dict) -> str:
        """æ„å»ºåŸºäºçœŸå®æ•°æ®çš„åˆ†ææç¤ºè¯"""
        return f"""
è¯·åŸºäºä»¥ä¸‹çœŸå®è®­ç»ƒæ•°æ®è¿›è¡Œä¸“ä¸šåˆ†æï¼š

## è®­ç»ƒä¼šè¯ä¿¡æ¯
- ä¼šè¯ID: {real_data.get('session_id', 'Unknown')}
- è®­ç»ƒæ—¶é•¿: {real_data.get('collection_duration', 0):.1f}ç§’
- æ•°æ®ç‚¹æ•°é‡: {real_data.get('total_data_points', 0)}ä¸ª
- è®­ç»ƒçŠ¶æ€: {real_data.get('training_status', 'unknown')}

## å½“å‰è®­ç»ƒæŒ‡æ ‡
- å½“å‰Epoch: {metrics_data.get('epoch', 'N/A')}
- è®­ç»ƒæŸå¤±: {metrics_data.get('train_loss', 'N/A')}
- éªŒè¯æŸå¤±: {metrics_data.get('val_loss', 'N/A')}
- è®­ç»ƒå‡†ç¡®ç‡: {metrics_data.get('train_accuracy', 'N/A')}
- éªŒè¯å‡†ç¡®ç‡: {metrics_data.get('val_accuracy', 'N/A')}

## è®­ç»ƒè¶‹åŠ¿åˆ†æ
- è®­ç»ƒæŸå¤±è¶‹åŠ¿: {trends.get('train_losses', [])}
- éªŒè¯æŸå¤±è¶‹åŠ¿: {trends.get('val_losses', [])}
- è®­ç»ƒå‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('train_accuracies', [])}
- éªŒè¯å‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('val_accuracies', [])}

è¯·åˆ†æï¼š
1. å½“å‰è®­ç»ƒçŠ¶æ€ï¼ˆæ”¶æ•›æƒ…å†µã€è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆé£é™©ï¼‰
2. åŸºäºè¶‹åŠ¿æ•°æ®çš„è®­ç»ƒè¿›å±•è¯„ä¼°
3. å…·ä½“çš„ä¼˜åŒ–å»ºè®®
4. éœ€è¦å…³æ³¨çš„æ½œåœ¨é—®é¢˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå®ç”¨æ€§ã€‚
"""

    def _combine_real_data_analyses(self, llm_analysis: str, rule_analysis: Dict, real_data: Dict) -> str:
        """ç»“åˆLLMå’Œè§„åˆ™åˆ†æï¼ˆé’ˆå¯¹çœŸå®æ•°æ®ï¼‰"""
        # è·å–è®­ç»ƒé…ç½®ä¿¡æ¯
        config_context = self._get_training_config_context()
        
        combined = f"""
## ç»¼åˆåˆ†æç»“æœï¼ˆåŸºäºçœŸå®è®­ç»ƒæ•°æ®å’Œè®­ç»ƒé…ç½®ï¼‰

{config_context}

### ğŸ“Š æ•°æ®æ¥æºä¿¡æ¯
- è®­ç»ƒä¼šè¯: {real_data.get('session_id', 'Unknown')}
- æ•°æ®é‡‡é›†æ—¶é•¿: {real_data.get('collection_duration', 0):.1f}ç§’
- æ€»æ•°æ®ç‚¹: {real_data.get('total_data_points', 0)}ä¸ª
- è®­ç»ƒçŠ¶æ€: {real_data.get('training_status', 'unknown')}

### ğŸ“ˆ è§„åˆ™åˆ†ææ‘˜è¦
- è®­ç»ƒçŠ¶æ€: {rule_analysis.get('training_state', 'æœªçŸ¥')}
- æ”¶æ•›çŠ¶æ€: {rule_analysis.get('convergence_status', 'æœªçŸ¥')}
- è¿‡æ‹Ÿåˆé£é™©: {rule_analysis.get('overfitting_risk', 'æœªçŸ¥')}
- æ€§èƒ½è¯„ä¼°: {rule_analysis.get('performance_assessment', 'æœªçŸ¥')}

### ğŸ¤– AIåˆ†ææ´å¯Ÿ
{llm_analysis}

### ğŸ¯ ç»“è®º
åŸºäºçœŸå®è®­ç»ƒæ•°æ®å’Œè®­ç»ƒé…ç½®çš„ç»¼åˆåˆ¤æ–­ï¼Œå½“å‰è®­ç»ƒçŠ¶æ€ä¸º{rule_analysis.get('training_state', 'æœªçŸ¥')}ï¼Œ
å»ºè®®å…³æ³¨{rule_analysis.get('convergence_status', 'æ”¶æ•›')}æƒ…å†µã€‚

**æ³¨æ„**: æ­¤åˆ†æåŸºäºå®æ—¶é‡‡é›†çš„çœŸå®è®­ç»ƒæ•°æ®å’Œè®­ç»ƒé…ç½®æ–‡ä»¶ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œé’ˆå¯¹æ€§ã€‚
"""
        return combined
    
    def _analyze_from_file(self) -> Dict[str, Any]:
        """ç›´æ¥ä»æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œåˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            import glob
            import os
            
            # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ•°æ®æ–‡ä»¶
            data_dir = "logs/real_time_metrics"
            if not os.path.exists(data_dir):
                return {'error': 'è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨'}
                
            pattern = os.path.join(data_dir, "*_metrics.json")
            files = glob.glob(pattern)
            
            if not files:
                return {'error': 'æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶'}
                
            # è·å–æœ€æ–°æ–‡ä»¶
            latest_file = max(files, key=os.path.getmtime)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æå–æœ€æ–°çš„å…³é”®æŒ‡æ ‡
            metrics_history = data.get("metrics_history", [])
            if not metrics_history:
                return {"error": "è®­ç»ƒæ•°æ®ä¸ºç©º"}
                
            # è·å–æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡
            current_metrics = data.get('current_metrics', {})
            
            # è®¡ç®—è®­ç»ƒè¶‹åŠ¿
            train_losses = []
            val_losses = []
            train_accs = []
            val_accs = []
            epochs = []
            
            for metric in metrics_history[-10:]:  # æœ€è¿‘10ä¸ªæ•°æ®ç‚¹
                if metric.get("phase") == "train":
                    if "loss" in metric:
                        train_losses.append(metric["loss"])
                    if "accuracy" in metric:
                        train_accs.append(metric["accuracy"])
                elif metric.get("phase") == "val":
                    if "loss" in metric:
                        val_losses.append(metric["loss"])
                    if "accuracy" in metric:
                        val_accs.append(metric["accuracy"])
                        
                if "epoch" in metric:
                    epochs.append(metric["epoch"])
            
            # æ„å»ºAIåˆ†æç”¨çš„æ•°æ®ç»“æ„
            real_data = {
                "session_id": data.get('session_id', 'unknown'),
                "current_metrics": current_metrics,
                "training_trends": {
                    "train_losses": train_losses,
                    "val_losses": val_losses,
                    "train_accuracies": train_accs,
                    "val_accuracies": val_accs,
                    "epochs": list(set(epochs))[-10:] if epochs else []
                },
                "training_status": data.get("training_status", "unknown"),
                "total_data_points": len(metrics_history),
                "collection_duration": time.time() - (data.get("start_time") or time.time())
            }
            
            # æ„å»ºåˆ†æç”¨çš„æŒ‡æ ‡æ•°æ®
            if current_metrics:
                metrics_data = {
                    'epoch': current_metrics.get('epoch', 0),
                    'train_loss': train_losses[-1] if train_losses else current_metrics.get('loss', 0),
                    'val_loss': val_losses[-1] if val_losses else current_metrics.get('loss', 0),
                    'train_accuracy': train_accs[-1] if train_accs else current_metrics.get('accuracy', 0),
                    'val_accuracy': val_accs[-1] if val_accs else current_metrics.get('accuracy', 0),
                    'training_duration': real_data.get('collection_duration', 0),
                    'data_points': real_data.get('total_data_points', 0),
                    'training_status': real_data.get('training_status', 'unknown')
                }
            else:
                return {'error': 'å½“å‰æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæŒ‡æ ‡æ•°æ®'}
            
            # è·å–LLMåˆ†æ
            llm_analysis = self.llm.analyze_metrics(metrics_data)
            
            # ç»“åˆè§„åˆ™åˆ†æ
            rule_analysis = self._rule_based_analysis(metrics_data)
            
            # ç”Ÿæˆç»¼åˆåˆ†æç»“æœ
            analysis_result = {
                'timestamp': time.time(),
                'data_source': 'file_direct_read',
                'session_id': real_data.get('session_id', 'unknown'),
                'metrics': metrics_data,
                'trends': real_data.get('training_trends', {}),
                'raw_data_summary': {
                    'total_data_points': real_data.get('total_data_points', 0),
                    'collection_duration': real_data.get('collection_duration', 0),
                    'training_status': real_data.get('training_status', 'unknown'),
                    'data_file': latest_file
                },
                'llm_analysis': llm_analysis,
                'rule_analysis': rule_analysis,
                'combined_insights': self._combine_real_data_analyses(llm_analysis, rule_analysis, real_data),
                'recommendations': self._generate_recommendations(metrics_data, llm_analysis, rule_analysis),
                'alerts': self._check_alerts(metrics_data)
            }
            
            return analysis_result
            
        except Exception as e:
            return {
                'error': f"ç›´æ¥æ–‡ä»¶è¯»å–åˆ†æå¤±è´¥: {str(e)}",
                'timestamp': time.time(),
                'data_source': 'file_direct_read'
            }
    
    def _generate_recommendations(self, metrics: Dict, llm_analysis: str, rule_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºè§„åˆ™åˆ†æçš„å»ºè®®
        if rule_analysis.get('training_state') == 'overfitting':
            recommendations.append("å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦")
        elif rule_analysis.get('training_state') == 'underfitting':
            recommendations.append("å¢åŠ æ¨¡å‹å®¹é‡æˆ–å‡å°‘æ­£åˆ™åŒ–")
        
        if rule_analysis.get('convergence_status') == 'diverging':
            recommendations.append("é™ä½å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ¢¯åº¦è£å‰ªè®¾ç½®")
        
        # ä»LLMåˆ†æä¸­æå–å»ºè®®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if "å­¦ä¹ ç‡" in llm_analysis and "é™ä½" in llm_analysis:
            recommendations.append("è€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡")
        
        return recommendations
    
    def _check_alerts(self, metrics: Dict) -> List[Dict]:
        """æ£€æŸ¥è­¦æŠ¥"""
        alerts = []
        
        # GPUå†…å­˜è­¦æŠ¥
        gpu_memory = metrics.get('gpu_memory_used', 0)
        gpu_total = metrics.get('gpu_memory_total', 8)
        if gpu_memory / gpu_total > 0.95:
            alerts.append({
                'type': 'gpu_memory_high',
                'severity': 'warning',
                'message': 'GPUå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡95%ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º'
            })
        
        # è®­ç»ƒé€Ÿåº¦è­¦æŠ¥
        training_speed = metrics.get('training_speed', 0)
        if training_speed < 0.1:
            alerts.append({
                'type': 'slow_training',
                'severity': 'info',
                'message': 'è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–'
            })
        
        return alerts
    
    def _build_chat_context(self) -> Dict:
        """æ„å»ºèŠå¤©ä¸Šä¸‹æ–‡"""
        context = {
            'current_metrics': self.metrics_buffer[-1]['metrics'] if self.metrics_buffer else {},
            'recent_analysis': self.analysis_history[-1] if self.analysis_history else {},
            'model_info': {
                'llm_type': type(self.llm).__name__,
                'requests_made': self.llm.request_count,
                'total_tokens': getattr(self.llm, 'total_tokens', 0)
            }
        }
        return context
    
    def get_engine_stats(self) -> Dict:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'analyses_performed': len(self.analysis_history),
            'metrics_processed': len(self.metrics_buffer),
            'llm_stats': self.llm.get_stats(),
            'last_analysis_time': self.analysis_history[-1]['timestamp'] if self.analysis_history else None
        }
    
    def _prioritize_suggestions(self, suggestions: List[Dict]) -> List[Dict]:
        """å¯¹å»ºè®®è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        priority_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        return sorted(suggestions, key=lambda x: priority_order.get(x.get('priority', 'low'), 3))
    
    def _estimate_improvements(self, suggestions: List[Dict]) -> Dict[str, str]:
        """ä¼°ç®—æ”¹è¿›æ•ˆæœ"""
        improvements = {}
        for suggestion in suggestions:
            param = suggestion.get('parameter', '')
            if param == 'learning_rate':
                improvements[param] = "é¢„æœŸè®­ç»ƒæ”¶æ•›é€Ÿåº¦æå‡20-30%"
            elif param == 'regularization':
                improvements[param] = "é¢„æœŸè¿‡æ‹Ÿåˆé£é™©é™ä½ï¼ŒéªŒè¯å‡†ç¡®ç‡æå‡5-10%"
            elif param == 'batch_size':
                improvements[param] = "é¢„æœŸè®­ç»ƒç¨³å®šæ€§æå‡ï¼ŒGPUåˆ©ç”¨ç‡æé«˜"
            else:
                improvements[param] = "é¢„æœŸè®­ç»ƒæ•ˆæœæœ‰æ‰€æ”¹å–„"
        return improvements
    
    def _rule_based_diagnosis(self, metrics: Dict, anomalies: List[Dict]) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„é—®é¢˜è¯Šæ–­"""
        diagnosis = {
            'primary_issues': [],
            'secondary_issues': [],
            'root_causes': [],
            'confidence_level': 'medium'
        }
        
        # åˆ†æå¼‚å¸¸ä¸¥é‡ç¨‹åº¦
        critical_anomalies = [a for a in anomalies if a.get('severity') == 'critical']
        high_anomalies = [a for a in anomalies if a.get('severity') == 'high']
        
        if critical_anomalies:
            diagnosis['primary_issues'].extend([a['description'] for a in critical_anomalies])
            diagnosis['confidence_level'] = 'high'
        
        if high_anomalies:
            diagnosis['secondary_issues'].extend([a['description'] for a in high_anomalies])
        
        # åˆ†ææ ¹æœ¬åŸå› 
        train_loss = metrics.get('train_loss', 0)
        val_loss = metrics.get('val_loss', 0)
        learning_rate = metrics.get('learning_rate', 0)
        
        if train_loss > 10:
            diagnosis['root_causes'].append("å­¦ä¹ ç‡å¯èƒ½è¿‡é«˜ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
        
        if val_loss > train_loss * 2:
            diagnosis['root_causes'].append("æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œéœ€è¦å¢åŠ æ­£åˆ™åŒ–")
        
        return diagnosis
    
    def _assess_severity(self, anomalies: List[Dict]) -> str:
        """è¯„ä¼°é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
        if any(a.get('severity') == 'critical' for a in anomalies):
            return 'critical'
        elif any(a.get('severity') == 'high' for a in anomalies):
            return 'high'
        elif any(a.get('severity') == 'medium' for a in anomalies):
            return 'medium'
        else:
            return 'low'
    
    def _recommend_actions(self, anomalies: List[Dict], diagnosis: Dict) -> List[Dict]:
        """æ¨èå…·ä½“è¡ŒåŠ¨"""
        actions = []
        
        for anomaly in anomalies:
            if anomaly.get('type') == 'loss_explosion':
                actions.append({
                    'action': 'reduce_learning_rate',
                    'description': 'ç«‹å³å°†å­¦ä¹ ç‡é™ä½åˆ°å½“å‰å€¼çš„1/10',
                    'priority': 'immediate',
                    'expected_time': '1åˆ†é’Ÿ'
                })
            elif anomaly.get('type') == 'nan_value':
                actions.append({
                    'action': 'restart_training',
                    'description': 'é‡å¯è®­ç»ƒå¹¶æ£€æŸ¥æ•°æ®é¢„å¤„ç†',
                    'priority': 'immediate',
                    'expected_time': '5åˆ†é’Ÿ'
                })
            elif anomaly.get('type') == 'high_learning_rate':
                actions.append({
                    'action': 'adjust_learning_rate',
                    'description': 'è°ƒæ•´å­¦ä¹ ç‡åˆ°0.001-0.01èŒƒå›´',
                    'priority': 'high',
                    'expected_time': '2åˆ†é’Ÿ'
                })
        
        return actions
    
    def _generate_prevention_tips(self, anomalies: List[Dict]) -> List[str]:
        """ç”Ÿæˆé¢„é˜²å»ºè®®"""
        tips = []
        
        anomaly_types = {a.get('type') for a in anomalies}
        
        if 'loss_explosion' in anomaly_types:
            tips.append("ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
            tips.append("é‡‡ç”¨å­¦ä¹ ç‡é¢„çƒ­ç­–ç•¥")
        
        if 'nan_value' in anomaly_types:
            tips.append("åœ¨è®­ç»ƒå‰éªŒè¯æ•°æ®é›†å®Œæ•´æ€§")
            tips.append("ä½¿ç”¨æ•°å€¼ç¨³å®šçš„æŸå¤±å‡½æ•°")
        
        if 'high_learning_rate' in anomaly_types:
            tips.append("ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨è‡ªåŠ¨è°ƒæ•´")
            tips.append("è¿›è¡Œå­¦ä¹ ç‡èŒƒå›´æµ‹è¯•æ‰¾åˆ°æœ€ä¼˜å€¼")
        
        return tips
    
    def _rule_based_model_comparison(self, model_results: List[Dict]) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„æ¨¡å‹å¯¹æ¯”"""
        if not model_results:
            return {'error': 'æ²¡æœ‰æ¨¡å‹ç»“æœå¯ä¾›å¯¹æ¯”'}
        
        comparison = {
            'best_accuracy': None,
            'best_loss': None,
            'most_stable': None,
            'fastest_convergence': None,
            'summary': {}
        }
        
        # æ‰¾åˆ°æœ€ä½³å‡†ç¡®ç‡
        best_acc_model = max(model_results, key=lambda x: x.get('val_accuracy', 0))
        comparison['best_accuracy'] = best_acc_model
        
        # æ‰¾åˆ°æœ€ä½æŸå¤±
        best_loss_model = min(model_results, key=lambda x: x.get('val_loss', float('inf')))
        comparison['best_loss'] = best_loss_model
        
        # ç”Ÿæˆæ‘˜è¦
        comparison['summary'] = {
            'total_models': len(model_results),
            'accuracy_range': [
                min(m.get('val_accuracy', 0) for m in model_results),
                max(m.get('val_accuracy', 0) for m in model_results)
            ],
            'loss_range': [
                min(m.get('val_loss', float('inf')) for m in model_results),
                max(m.get('val_loss', 0) for m in model_results)
            ]
        }
        
        return comparison
    
    def _select_best_model(self, model_results: List[Dict]) -> Dict:
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        if not model_results:
            return {}
        
        # ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡æƒé‡0.7ï¼ŒæŸå¤±æƒé‡0.3
        def calculate_score(model):
            acc = model.get('val_accuracy', 0)
            loss = model.get('val_loss', float('inf'))
            # æŸå¤±è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥å–å€’æ•°
            loss_score = 1 / (1 + loss) if loss != float('inf') else 0
            return acc * 0.7 + loss_score * 0.3
        
        return max(model_results, key=calculate_score)
    
    def _rank_models(self, model_results: List[Dict]) -> List[Dict]:
        """å¯¹æ¨¡å‹è¿›è¡Œæ’å"""
        if not model_results:
            return []
        
        def calculate_score(model):
            acc = model.get('val_accuracy', 0)
            loss = model.get('val_loss', float('inf'))
            loss_score = 1 / (1 + loss) if loss != float('inf') else 0
            return acc * 0.7 + loss_score * 0.3
        
        ranked = sorted(model_results, key=calculate_score, reverse=True)
        
        # æ·»åŠ æ’åä¿¡æ¯
        for i, model in enumerate(ranked, 1):
            model['rank'] = i
            model['score'] = calculate_score(model)
        
        return ranked
    
    def clear_history(self):
        """æ¸…ç©ºå†å²è®°å½•"""
        self.analysis_history.clear()
        self.metrics_buffer.clear()
        self.prompt_builder.clear_context()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåˆ†æå¼•æ“
    engine = TrainingAnalysisEngine()
    
    # æµ‹è¯•æŒ‡æ ‡åˆ†æ
    test_metrics = {
        "epoch": 15,
        "train_loss": 0.234,
        "val_loss": 0.287,
        "train_accuracy": 0.894,
        "val_accuracy": 0.856,
        "learning_rate": 0.001,
        "gpu_memory_used": 6.2,
        "gpu_memory_total": 8.0,
        "training_speed": 1.23
    }
    
    print("=== è®­ç»ƒè¿›åº¦åˆ†ææµ‹è¯• ===")
    analysis_result = engine.analyze_training_progress(test_metrics)
    print(f"åˆ†æå®Œæˆï¼ŒçŠ¶æ€: {analysis_result.get('rule_analysis', {}).get('training_state', 'æœªçŸ¥')}")
    
    print("\n=== è¶…å‚æ•°è°ƒä¼˜å»ºè®®æµ‹è¯• ===")
    tuning_result = engine.suggest_hyperparameter_tuning(
        test_metrics, 
        {"batch_size": 32, "learning_rate": 0.001}
    )
    print(f"ç”Ÿæˆäº† {len(tuning_result.get('rule_suggestions', []))} æ¡è§„åˆ™å»ºè®®")
    
    print("\n=== å¯¹è¯æµ‹è¯• ===")
    chat_result = engine.chat_with_context("å½“å‰è®­ç»ƒæ•ˆæœå¦‚ä½•ï¼Ÿ")
    print(f"å¯¹è¯å›å¤é•¿åº¦: {len(chat_result.get('response', ''))}")
    
    print(f"\n=== å¼•æ“ç»Ÿè®¡ ===")
    stats = engine.get_engine_stats()
    print(json.dumps(stats, ensure_ascii=False, indent=2)) 