"""
å®æ—¶è®­ç»ƒæŒ‡æ ‡é‡‡é›†å™¨ - éä¾µå…¥å¼æ•°æ®é‡‡é›†

ä¸»è¦åŠŸèƒ½ï¼š
- ç›‘å¬TensorBoardæ•°æ®æµï¼Œå¤åˆ¶ä¸€ä»½åˆ°æœ¬åœ°æ–‡ä»¶
- ä¸ºAIåˆ†ææä¾›å®æ—¶è®­ç»ƒæ•°æ®
- æ”¯æŒå¤šç§è®­ç»ƒæ¡†æ¶ï¼ˆåˆ†ç±»ã€æ£€æµ‹ï¼‰
- è‡ªåŠ¨æ¸…ç†å†å²æ•°æ®ï¼Œä¿æŒæ–‡ä»¶å¤§å°åˆç†
"""

import os
import json
import time
import threading
from typing import Dict, Any, List, Optional
from collections import deque
from PyQt5.QtCore import QObject, pyqtSignal
import torch
from torch.utils.tensorboard.writer import SummaryWriter


class RealTimeMetricsCollector(QObject):
    """å®æ—¶è®­ç»ƒæŒ‡æ ‡é‡‡é›†å™¨"""
    
    # ä¿¡å·å®šä¹‰
    metrics_updated = pyqtSignal(dict)  # æ–°æŒ‡æ ‡æ•°æ®å¯ç”¨
    analysis_data_ready = pyqtSignal(str)  # åˆ†ææ•°æ®æ–‡ä»¶å°±ç»ª
    
    def __init__(self, max_history=100):
        super().__init__()
        self.max_history = max_history
        self.metrics_buffer = deque(maxlen=max_history)
        self.current_training_session = None
        self.data_file_path = None
        self.lock = threading.Lock()
        
        # åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
        self.data_dir = "logs/real_time_metrics"
        os.makedirs(self.data_dir, exist_ok=True)
        
    def start_collection(self, training_session_id: str = None):
        """å¼€å§‹æ•°æ®é‡‡é›†"""
        if training_session_id is None:
            training_session_id = f"training_{int(time.time())}"
            
        self.current_training_session = training_session_id
        self.data_file_path = os.path.join(
            self.data_dir, 
            f"{training_session_id}_metrics.json"
        )
        
        # åˆå§‹åŒ–æ•°æ®æ–‡ä»¶
        initial_data = {
            "session_id": training_session_id,
            "start_time": time.time(),
            "metrics_history": [],
            "current_metrics": {},
            "training_status": "started"
        }
        
        with open(self.data_file_path, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… å¼€å§‹å®æ—¶æŒ‡æ ‡é‡‡é›†ï¼Œä¼šè¯ID: {training_session_id}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {self.data_file_path}")
        
    def collect_tensorboard_metrics(self, epoch: int, phase: str, metrics: Dict[str, Any]):
        """é‡‡é›†TensorBoardæŒ‡æ ‡æ•°æ®ï¼ˆéä¾µå…¥å¼ï¼‰"""
        if not self.current_training_session:
            return
            
        timestamp = time.time()
        
        # æ„å»ºæ ‡å‡†åŒ–æŒ‡æ ‡æ•°æ®
        standardized_metrics = {
            "timestamp": timestamp,
            "epoch": epoch,
            "phase": phase,
            "session_id": self.current_training_session,
            **metrics
        }
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        with self.lock:
            self.metrics_buffer.append(standardized_metrics)
            
        # æ›´æ–°æ•°æ®æ–‡ä»¶
        self._update_data_file(standardized_metrics)
        
        # å‘é€ä¿¡å·
        self.metrics_updated.emit(standardized_metrics)
        
    def collect_scalar_metric(self, tag: str, value: float, step: int):
        """é‡‡é›†å•ä¸ªæ ‡é‡æŒ‡æ ‡ï¼ˆæ¨¡æ‹ŸTensorBoardçš„add_scalarï¼‰"""
        if not self.current_training_session:
            return
            
        timestamp = time.time()
        
        # è§£ætagè·å–ç›¸å…³ä¿¡æ¯
        phase = "train" if "train" in tag.lower() else "val" if "val" in tag.lower() else "unknown"
        metric_type = tag.split('/')[-1] if '/' in tag else tag
        
        metric_data = {
            "timestamp": timestamp,
            "epoch": step,
            "phase": phase,
            "tag": tag,
            "metric_type": metric_type,
            "value": float(value),
            "session_id": self.current_training_session
        }
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        with self.lock:
            self.metrics_buffer.append(metric_data)
            
        # æ›´æ–°æ•°æ®æ–‡ä»¶
        self._update_data_file(metric_data)
        
        # å‘é€ä¿¡å·
        self.metrics_updated.emit(metric_data)
        
    def get_current_training_data_for_ai(self) -> Dict[str, Any]:
        """è·å–å½“å‰è®­ç»ƒæ•°æ®ä¾›AIåˆ†æä½¿ç”¨"""
        # å¦‚æœæ²¡æœ‰æ´»åŠ¨ä¼šè¯ï¼Œå°è¯•æ‰¾åˆ°æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        if not self.data_file_path or not os.path.exists(self.data_file_path):
            latest_file = self._find_latest_metrics_file()
            if latest_file:
                self.data_file_path = latest_file
                print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„è®­ç»ƒæ•°æ®æ–‡ä»¶: {latest_file}")
            else:
                return {"error": "æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®"}
            
        try:
            with open(self.data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æå–æœ€æ–°çš„å…³é”®æŒ‡æ ‡
            metrics_history = data.get("metrics_history", [])
            if not metrics_history:
                return {"error": "è®­ç»ƒæ•°æ®ä¸ºç©º"}
                
            # è·å–æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡
            latest_metrics = metrics_history[-1] if metrics_history else {}
            
            # è®¡ç®—è®­ç»ƒè¶‹åŠ¿
            train_losses = []
            val_losses = []
            train_accs = []
            val_accs = []
            epochs = []
            
            for metric in metrics_history[-10:]:  # æœ€è¿‘10ä¸ªæ•°æ®ç‚¹
                if metric.get("phase") == "train":
                    if "loss" in metric:
                        train_losses.append(metric["loss"])
                    if "accuracy" in metric:
                        train_accs.append(metric["accuracy"])
                elif metric.get("phase") == "val":
                    if "loss" in metric:
                        val_losses.append(metric["loss"])
                    if "accuracy" in metric:
                        val_accs.append(metric["accuracy"])
                        
                if "epoch" in metric:
                    epochs.append(metric["epoch"])
            
            # æ„å»ºAIåˆ†æç”¨çš„æ•°æ®ç»“æ„
            ai_data = {
                "session_id": self.current_training_session,
                "current_metrics": latest_metrics,
                "training_trends": {
                    "train_losses": train_losses,
                    "val_losses": val_losses,
                    "train_accuracies": train_accs,
                    "val_accuracies": val_accs,
                    "epochs": list(set(epochs))[-10:] if epochs else []
                },
                "training_status": data.get("training_status", "unknown"),
                "total_data_points": len(metrics_history),
                "collection_duration": time.time() - (data.get("start_time") or time.time())
            }
            
            return ai_data
            
        except Exception as e:
            return {"error": f"è¯»å–è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"}
            
    def get_formatted_metrics_for_analysis(self) -> str:
        """è·å–æ ¼å¼åŒ–çš„æŒ‡æ ‡æ•°æ®ä¾›AIåˆ†æ"""
        ai_data = self.get_current_training_data_for_ai()
        
        if "error" in ai_data:
            return f"æ•°æ®è·å–å¤±è´¥: {ai_data['error']}"
            
        current = ai_data.get("current_metrics", {})
        trends = ai_data.get("training_trends", {})
        
        # æ„å»ºåˆ†ææ–‡æœ¬
        analysis_text = f"""
## å½“å‰è®­ç»ƒçŠ¶æ€æ•°æ®

### åŸºæœ¬ä¿¡æ¯
- è®­ç»ƒä¼šè¯: {ai_data.get('session_id', 'Unknown')}
- æ•°æ®é‡‡é›†æ—¶é•¿: {ai_data.get('collection_duration', 0):.1f}ç§’
- æ€»æ•°æ®ç‚¹: {ai_data.get('total_data_points', 0)}ä¸ª

### æœ€æ–°æŒ‡æ ‡
- å½“å‰Epoch: {current.get('epoch', 'N/A')}
- è®­ç»ƒé˜¶æ®µ: {current.get('phase', 'N/A')}
- æŸå¤±å€¼: {current.get('loss', 'N/A')}
- å‡†ç¡®ç‡: {current.get('accuracy', 'N/A')}
- æ—¶é—´æˆ³: {current.get('timestamp', 'N/A')}

### è®­ç»ƒè¶‹åŠ¿ï¼ˆæœ€è¿‘10ä¸ªæ•°æ®ç‚¹ï¼‰
- è®­ç»ƒæŸå¤±è¶‹åŠ¿: {trends.get('train_losses', [])}
- éªŒè¯æŸå¤±è¶‹åŠ¿: {trends.get('val_losses', [])}
- è®­ç»ƒå‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('train_accuracies', [])}
- éªŒè¯å‡†ç¡®ç‡è¶‹åŠ¿: {trends.get('val_accuracies', [])}
- Epochåºåˆ—: {trends.get('epochs', [])}

### è®­ç»ƒçŠ¶æ€
- çŠ¶æ€: {ai_data.get('training_status', 'unknown')}
"""
        
        return analysis_text.strip()
        
    def stop_collection(self):
        """åœæ­¢æ•°æ®é‡‡é›†"""
        if self.current_training_session and self.data_file_path:
            # æ›´æ–°è®­ç»ƒçŠ¶æ€ä¸ºå®Œæˆ
            self._update_training_status("completed")
            print(f"âœ… è®­ç»ƒæŒ‡æ ‡é‡‡é›†å·²åœæ­¢ï¼Œä¼šè¯: {self.current_training_session}")
            
        self.current_training_session = None
        self.data_file_path = None
        
    def _update_data_file(self, new_metric: Dict[str, Any]):
        """æ›´æ–°æ•°æ®æ–‡ä»¶"""
        if not self.data_file_path:
            return
            
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(self.data_file_path):
                with open(self.data_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {
                    "session_id": self.current_training_session,
                    "start_time": time.time(),
                    "metrics_history": [],
                    "current_metrics": {},
                    "training_status": "running"
                }
            
            # æ·»åŠ æ–°æŒ‡æ ‡
            data["metrics_history"].append(new_metric)
            data["current_metrics"] = new_metric
            data["last_update"] = time.time()
            
            # ä¿æŒå†å²æ•°æ®åœ¨åˆç†èŒƒå›´å†…
            if len(data["metrics_history"]) > self.max_history:
                data["metrics_history"] = data["metrics_history"][-self.max_history:]
            
            # å†™å›æ–‡ä»¶
            with open(self.data_file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
            # å‘é€æ•°æ®æ–‡ä»¶å°±ç»ªä¿¡å·
            self.analysis_data_ready.emit(self.data_file_path)
            
        except Exception as e:
            print(f"âŒ æ›´æ–°æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            
    def _update_training_status(self, status: str):
        """æ›´æ–°è®­ç»ƒçŠ¶æ€"""
        if not self.data_file_path or not os.path.exists(self.data_file_path):
            return
            
        try:
            with open(self.data_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            data["training_status"] = status
            data["end_time"] = time.time()
            
            with open(self.data_file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âŒ æ›´æ–°è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}")
            
    def cleanup_old_data(self, days_to_keep: int = 7):
        """æ¸…ç†æ—§çš„æ•°æ®æ–‡ä»¶"""
        try:
            current_time = time.time()
            cutoff_time = current_time - (days_to_keep * 24 * 3600)
            
            for filename in os.listdir(self.data_dir):
                if filename.endswith('_metrics.json'):
                    file_path = os.path.join(self.data_dir, filename)
                    if os.path.getmtime(file_path) < cutoff_time:
                        os.remove(file_path)
                        print(f"ğŸ—‘ï¸ å·²æ¸…ç†æ—§æ•°æ®æ–‡ä»¶: {filename}")
                        
        except Exception as e:
            print(f"âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥: {str(e)}")
    
    def _find_latest_metrics_file(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶"""
        try:
            import glob
            pattern = os.path.join(self.data_dir, "*_metrics.json")
            files = glob.glob(pattern)
            
            if not files:
                return None
                
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(files, key=os.path.getmtime)
            return latest_file
            
        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾æœ€æ–°æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None


# å…¨å±€å®ä¾‹
_global_collector = None

def get_global_metrics_collector() -> RealTimeMetricsCollector:
    """è·å–å…¨å±€æŒ‡æ ‡é‡‡é›†å™¨å®ä¾‹"""
    global _global_collector
    if _global_collector is None:
        _global_collector = RealTimeMetricsCollector()
    return _global_collector


class TensorBoardInterceptor:
    """TensorBoardæ•°æ®æ‹¦æˆªå™¨ - éä¾µå…¥å¼åœ°æ•è·æ•°æ®"""
    
    def __init__(self, original_writer: SummaryWriter):
        self.original_writer = original_writer
        self.collector = get_global_metrics_collector()
        
    def add_scalar(self, tag: str, scalar_value: float, global_step: int = None, walltime: float = None):
        """æ‹¦æˆªadd_scalarè°ƒç”¨ï¼Œå¤åˆ¶æ•°æ®åˆ°é‡‡é›†å™¨"""
        # è°ƒç”¨åŸå§‹æ–¹æ³•
        result = self.original_writer.add_scalar(tag, scalar_value, global_step, walltime)
        
        # å¤åˆ¶æ•°æ®åˆ°é‡‡é›†å™¨
        if self.collector.current_training_session:
            self.collector.collect_scalar_metric(tag, scalar_value, global_step or 0)
            
        return result
        
    def __getattr__(self, name):
        """ä»£ç†å…¶ä»–æ–¹æ³•åˆ°åŸå§‹writer"""
        return getattr(self.original_writer, name)


def install_tensorboard_interceptor(writer: SummaryWriter) -> TensorBoardInterceptor:
    """å®‰è£…TensorBoardæ‹¦æˆªå™¨"""
    return TensorBoardInterceptor(writer)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé‡‡é›†å™¨
    collector = RealTimeMetricsCollector()
    
    # å¼€å§‹é‡‡é›†
    collector.start_collection("test_session")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    for epoch in range(5):
        for phase in ['train', 'val']:
            metrics = {
                'loss': 0.5 - epoch * 0.1 + (0.1 if phase == 'val' else 0),
                'accuracy': 0.7 + epoch * 0.05 - (0.05 if phase == 'val' else 0)
            }
            collector.collect_tensorboard_metrics(epoch, phase, metrics)
            time.sleep(0.1)
    
    # è·å–AIåˆ†ææ•°æ®
    ai_data = collector.get_current_training_data_for_ai()
    print("AIåˆ†ææ•°æ®:")
    print(json.dumps(ai_data, indent=2, ensure_ascii=False))
    
    # è·å–æ ¼å¼åŒ–åˆ†ææ–‡æœ¬
    analysis_text = collector.get_formatted_metrics_for_analysis()
    print("\næ ¼å¼åŒ–åˆ†ææ–‡æœ¬:")
    print(analysis_text)
    
    # åœæ­¢é‡‡é›†
    collector.stop_collection() 