"""
é›†æˆäº†çœŸæ­£èµ„æºé™åˆ¶çš„è®­ç»ƒç»„ä»¶
ç¡®ä¿è®­ç»ƒè¿‡ç¨‹éµå®ˆèµ„æºé™åˆ¶ï¼Œå¹¶æä¾›ä¼˜é›…çš„ä¸­æ–­æœºåˆ¶
"""

import os
import sys
import time
import torch
import numpy as np
from typing import Dict, Any, Optional, Callable
from ..utils.resource_limiter import (
    get_resource_limiter, ResourceLimitException, resource_limited_operation
)


class ResourceLimitedTrainer:
    """é›†æˆäº†èµ„æºé™åˆ¶çš„è®­ç»ƒå™¨"""
    
    def __init__(self, original_trainer):
        """
        åˆå§‹åŒ–èµ„æºé™åˆ¶è®­ç»ƒå™¨
        
        Args:
            original_trainer: åŸå§‹è®­ç»ƒå™¨å®ä¾‹
        """
        self.original_trainer = original_trainer
        self.resource_limiter = get_resource_limiter()
        self.interrupted = False
        self.last_checkpoint_epoch = 0
        
    @resource_limited_operation("æ¨¡å‹è®­ç»ƒ")
    def train_epoch(self, epoch: int, train_loader, model, optimizer, criterion, device):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œé›†æˆèµ„æºæ£€æŸ¥"""
        if self.resource_limiter and self.resource_limiter.is_stop_requested():
            print(f"ğŸ›‘ è®­ç»ƒåœ¨ç¬¬ {epoch} ä¸ªepochè¢«èµ„æºé™åˆ¶å™¨ä¸­æ–­")
            self.interrupted = True
            return None
            
        try:
            # åœ¨æ¯ä¸ªepochå¼€å§‹å‰æ£€æŸ¥èµ„æº
            if self.resource_limiter:
                self.resource_limiter.check_resource_before_operation(f"è®­ç»ƒepoch {epoch}")
                
            # æ‰§è¡ŒåŸå§‹çš„è®­ç»ƒé€»è¾‘
            if hasattr(self.original_trainer, 'train_epoch'):
                return self.original_trainer.train_epoch(epoch, train_loader, model, optimizer, criterion, device)
            else:
                # é»˜è®¤è®­ç»ƒé€»è¾‘
                return self._default_train_epoch(epoch, train_loader, model, optimizer, criterion, device)
                
        except ResourceLimitException as e:
            print(f"âŒ è®­ç»ƒå› èµ„æºé™åˆ¶ä¸­æ–­: {e}")
            self.interrupted = True
            self._emergency_save_checkpoint(epoch, model, optimizer)
            raise
        except Exception as e:
            print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
            raise
    
    def _default_train_epoch(self, epoch: int, train_loader, model, optimizer, criterion, device):
        """é»˜è®¤çš„è®­ç»ƒepochå®ç°"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # åœ¨æ¯ä¸ªbatchå‰æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if self.resource_limiter and self.resource_limiter.is_stop_requested():
                print(f"ğŸ›‘ è®­ç»ƒåœ¨ç¬¬ {epoch} epochçš„ç¬¬ {batch_idx} batchè¢«ä¸­æ–­")
                self.interrupted = True
                break
                
            # æ¯éš”ä¸€å®šé—´éš”æ£€æŸ¥èµ„æº
            if batch_idx % 50 == 0 and self.resource_limiter:
                try:
                    self.resource_limiter.check_resource_before_operation(f"batch {batch_idx}")
                except ResourceLimitException as e:
                    print(f"âš ï¸ Batch {batch_idx} èµ„æºæ£€æŸ¥å¤±è´¥: {e}")
                    # ä¸ç›´æ¥ä¸­æ–­ï¼Œè€Œæ˜¯å°è¯•ç»§ç»­
                    if self.resource_limiter:
                        self.resource_limiter.emergency_cleanup()
            
            try:
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                # å®šæœŸæ³¨å†Œä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if batch_idx % 100 == 0 and self.resource_limiter:
                    self._register_temp_files()
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"ğŸ’¾ CUDAå†…å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ¸…ç†...")
                    torch.cuda.empty_cache()
                    if self.resource_limiter:
                        self.resource_limiter.emergency_cleanup()
                    continue
                else:
                    raise
                    
        if self.interrupted:
            return None
            
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'correct': correct,
            'total': total
        }
    
    @resource_limited_operation("æ¨¡å‹éªŒè¯")
    def validate_model(self, val_loader, model, criterion, device):
        """éªŒè¯æ¨¡å‹ï¼Œé›†æˆèµ„æºæ£€æŸ¥"""
        if self.resource_limiter and self.resource_limiter.is_stop_requested():
            print("ğŸ›‘ éªŒè¯è¢«èµ„æºé™åˆ¶å™¨ä¸­æ–­")
            return None
            
        try:
            if hasattr(self.original_trainer, 'validate_model'):
                return self.original_trainer.validate_model(val_loader, model, criterion, device)
            else:
                return self._default_validate_model(val_loader, model, criterion, device)
                
        except ResourceLimitException as e:
            print(f"âŒ éªŒè¯å› èµ„æºé™åˆ¶ä¸­æ–­: {e}")
            return None
            
    def _default_validate_model(self, val_loader, model, criterion, device):
        """é»˜è®¤çš„æ¨¡å‹éªŒè¯å®ç°"""
        model.eval()
        val_loss = 0
        correct = 0
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                # æ£€æŸ¥ä¸­æ–­
                if self.resource_limiter and self.resource_limiter.is_stop_requested():
                    break
                    
                try:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    val_loss += criterion(output, target).item()
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise
        
        val_loss /= len(val_loader.dataset)
        accuracy = 100. * correct / len(val_loader.dataset)
        
        return {
            'val_loss': val_loss,
            'val_accuracy': accuracy,
            'correct': correct,
            'total': len(val_loader.dataset)
        }
    
    def _register_temp_files(self):
        """æ³¨å†Œè®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶"""
        if not self.resource_limiter:
            return
            
        try:
            # æ³¨å†Œå¸¸è§çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_patterns = [
                "/tmp/torch_*",
                "/tmp/python_*", 
                "*.tmp",
                "checkpoint_temp.pth",
                "model_temp.pth"
            ]
            
            import glob
            for pattern in temp_patterns:
                for file_path in glob.glob(pattern):
                    if os.path.isfile(file_path):
                        self.resource_limiter.register_temp_file(file_path)
                        
        except Exception as e:
            print(f"âš ï¸ æ³¨å†Œä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def _emergency_save_checkpoint(self, epoch: int, model, optimizer):
        """ç´§æ€¥ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_path = f"emergency_checkpoint_epoch_{epoch}.pth"
            
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'interrupted': True,
                'timestamp': time.time()
            }
            
            torch.save(checkpoint, checkpoint_path)
            print(f"ğŸ’¾ ç´§æ€¥æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
            
            # æ³¨å†Œæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸ºä¸´æ—¶æ–‡ä»¶
            if self.resource_limiter:
                self.resource_limiter.register_temp_file(checkpoint_path)
                
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´§æ€¥æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def train_with_resource_limits(self, epochs: int, train_loader, val_loader, 
                                 model, optimizer, criterion, device, 
                                 save_callback: Optional[Callable] = None):
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œé›†æˆèµ„æºé™åˆ¶
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            criterion: æŸå¤±å‡½æ•°
            device: è®¾å¤‡
            save_callback: ä¿å­˜å›è°ƒå‡½æ•°
        """
        print(f"ğŸš€ å¼€å§‹èµ„æºé™åˆ¶è®­ç»ƒï¼Œå…± {epochs} ä¸ªepoch")
        
        if self.resource_limiter:
            print(f"ğŸ“Š èµ„æºé™åˆ¶çŠ¶æ€: {self.resource_limiter.get_resource_status()}")
        
        for epoch in range(1, epochs + 1):
            epoch_start_time = time.time()
            
            try:
                # è®­ç»ƒ
                train_result = self.train_epoch(epoch, train_loader, model, optimizer, criterion, device)
                
                if self.interrupted or train_result is None:
                    print(f"â¹ï¸ è®­ç»ƒåœ¨ç¬¬ {epoch} ä¸ªepochè¢«ä¸­æ–­")
                    break
                
                # éªŒè¯
                val_result = self.validate_model(val_loader, model, criterion, device)
                
                if self.interrupted or val_result is None:
                    print(f"â¹ï¸ éªŒè¯åœ¨ç¬¬ {epoch} ä¸ªepochè¢«ä¸­æ–­")
                    break
                
                # æ‰“å°ç»“æœ
                epoch_time = time.time() - epoch_start_time
                print(f"Epoch {epoch}/{epochs} - "
                      f"Loss: {train_result['loss']:.4f}, "
                      f"Acc: {train_result['accuracy']:.2f}%, "
                      f"Val Loss: {val_result['val_loss']:.4f}, "
                      f"Val Acc: {val_result['val_accuracy']:.2f}%, "
                      f"Time: {epoch_time:.2f}s")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if save_callback:
                    save_callback(epoch, model, optimizer, train_result, val_result)
                
                self.last_checkpoint_epoch = epoch
                
                # æ¯5ä¸ªepochæ£€æŸ¥ä¸€æ¬¡èµ„æºçŠ¶æ€
                if epoch % 5 == 0 and self.resource_limiter:
                    status = self.resource_limiter.get_resource_status()
                    print(f"ğŸ“Š èµ„æºçŠ¶æ€: å†…å­˜ {status.get('memory_percent', 0):.1f}%, "
                          f"CPU {status.get('cpu_percent', 0):.1f}%")
                
            except ResourceLimitException as e:
                print(f"âŒ ç¬¬ {epoch} epochè®­ç»ƒå› èµ„æºé™åˆ¶ä¸­æ–­: {e}")
                break
            except KeyboardInterrupt:
                print(f"â¹ï¸ ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­è®­ç»ƒ (Epoch {epoch})")
                self._emergency_save_checkpoint(epoch, model, optimizer)
                break
            except Exception as e:
                print(f"âŒ ç¬¬ {epoch} epochè®­ç»ƒå‡ºé”™: {e}")
                self._emergency_save_checkpoint(epoch, model, optimizer)
                raise
        
        # è®­ç»ƒç»“æŸåçš„æ¸…ç†
        if self.resource_limiter:
            self.resource_limiter.emergency_cleanup()
            
        if self.interrupted:
            print(f"âš ï¸ è®­ç»ƒå› èµ„æºé™åˆ¶åœ¨ç¬¬ {self.last_checkpoint_epoch} epochåä¸­æ–­")
        else:
            print(f"âœ… è®­ç»ƒæˆåŠŸå®Œæˆ {epochs} ä¸ªepoch")
    
    def create_resource_limited_thread(self, target, *args, **kwargs):
        """åˆ›å»ºå—èµ„æºé™åˆ¶çš„çº¿ç¨‹"""
        if self.resource_limiter:
            return self.resource_limiter.create_limited_thread(target, args, kwargs)
        else:
            import threading
            return threading.Thread(target=target, args=args, kwargs=kwargs)


def wrap_trainer_with_resource_limits(trainer_class):
    """è£…é¥°å™¨ï¼šä¸ºè®­ç»ƒå™¨ç±»æ·»åŠ èµ„æºé™åˆ¶æ”¯æŒ"""
    class ResourceLimitedTrainerWrapper(trainer_class):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.resource_limited_trainer = ResourceLimitedTrainer(self)
            
        def train_with_limits(self, *args, **kwargs):
            return self.resource_limited_trainer.train_with_resource_limits(*args, **kwargs)
            
    return ResourceLimitedTrainerWrapper


# ä¾¿æ·å‡½æ•°
def enable_resource_limited_training(trainer_instance):
    """ä¸ºç°æœ‰è®­ç»ƒå™¨å®ä¾‹å¯ç”¨èµ„æºé™åˆ¶"""
    return ResourceLimitedTrainer(trainer_instance) 