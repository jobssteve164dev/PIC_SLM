"""
è®­ç»ƒçº¿ç¨‹ - åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œæ¨¡å‹è®­ç»ƒè¿‡ç¨‹

ä¸»è¦åŠŸèƒ½ï¼š
- åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè®­ç»ƒï¼Œé¿å…é˜»å¡UI
- å¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§çŠ¶æ€æ›´æ–°
- æ”¯æŒè®­ç»ƒè¿‡ç¨‹çš„åœæ­¢æ§åˆ¶
- é›†æˆå„ç§è®­ç»ƒç»„ä»¶ï¼ˆæ¨¡å‹å·¥å‚ã€æƒé‡è®¡ç®—å™¨ç­‰ï¼‰
"""

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import json
from PyQt5.QtCore import QThread, pyqtSignal

from .model_factory import ModelFactory
from .weight_calculator import WeightCalculator
from .model_configurator import ModelConfigurator
from .tensorboard_logger import TensorBoardLogger
from .training_validator import TrainingValidator
from .resource_limited_trainer import ResourceLimitedTrainer, enable_resource_limited_training
from ..utils.resource_limiter import (
    initialize_resource_limiter, ResourceLimits, ResourceLimitException, get_resource_limiter
)


class TrainingThread(QThread):
    """è´Ÿè´£åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹çš„ç±»"""
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int)
    status_updated = pyqtSignal(str)
    training_finished = pyqtSignal()
    training_error = pyqtSignal(str)
    epoch_finished = pyqtSignal(dict)
    model_download_failed = pyqtSignal(str, str)  # æ¨¡å‹åç§°ï¼Œä¸‹è½½é“¾æ¥
    training_stopped = pyqtSignal()
    
    def __init__(self, config, parent=None):
        super().__init__(parent)
        self.config = config
        self.stop_training = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.training_info = {}
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.model_factory = ModelFactory()
        self.weight_calculator = WeightCalculator()
        self.model_configurator = ModelConfigurator()
        self.tensorboard_logger = TensorBoardLogger()
        self.validator = TrainingValidator()
        
        # åˆå§‹åŒ–èµ„æºé™åˆ¶å™¨
        self.resource_limiter = None
        self.resource_limited_trainer = None
        self._setup_resource_limiter()
        
        # è¿æ¥ç»„ä»¶ä¿¡å·
        self._connect_component_signals()
    
    def _setup_resource_limiter(self):
        """è®¾ç½®èµ„æºé™åˆ¶å™¨"""
        try:
            # ä»é…ç½®ä¸­è·å–èµ„æºé™åˆ¶è®¾ç½®
            resource_limits_config = self.config.get('resource_limits', {})
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¼ºåˆ¶èµ„æºé™åˆ¶ï¼ˆä»è®­ç»ƒç•Œé¢æˆ–è®¾ç½®ç•Œé¢ï¼‰
            enable_from_ui = self.config.get('enable_resource_limits', False)  # ä»è®­ç»ƒç•Œé¢
            enable_from_settings = resource_limits_config.get('enforce_limits_enabled', False)  # ä»è®¾ç½®ç•Œé¢
            
            if enable_from_ui or enable_from_settings:
                # åˆ›å»ºèµ„æºé™åˆ¶é…ç½®
                limits = ResourceLimits(
                    max_memory_gb=resource_limits_config.get('memory_absolute_limit_gb', 8.0),
                    max_cpu_percent=resource_limits_config.get('cpu_percent_limit', 80.0),
                    max_disk_usage_gb=resource_limits_config.get('temp_files_limit_gb', 10.0),
                    max_processes=4,
                    max_threads=resource_limits_config.get('cpu_cores_limit', 8),
                    check_interval=resource_limits_config.get('check_interval', 2.0),
                    enforce_limits=True,
                    auto_cleanup=resource_limits_config.get('auto_cleanup_enabled', True)
                )
                
                # åˆå§‹åŒ–å…¨å±€èµ„æºé™åˆ¶å™¨
                self.resource_limiter = initialize_resource_limiter(limits)
                
                # æ·»åŠ å›è°ƒå¤„ç†èµ„æºè¶…é™
                self.resource_limiter.add_callback('memory_limit', self._on_resource_limit_exceeded)
                self.resource_limiter.add_callback('cpu_limit', self._on_resource_limit_exceeded)
                self.resource_limiter.add_callback('disk_limit', self._on_resource_limit_exceeded)
                self.resource_limiter.add_callback('process_limit', self._on_resource_limit_exceeded)
                
                source = "è®­ç»ƒç•Œé¢" if enable_from_ui else "è®¾ç½®ç•Œé¢"
                print(f"âœ… è®­ç»ƒè¿›ç¨‹å¯ç”¨å¼ºåˆ¶èµ„æºé™åˆ¶(æ¥æº: {source}): å†…å­˜{limits.max_memory_gb}GB, CPU{limits.max_cpu_percent}%")
            else:
                print("â„¹ï¸ è®­ç»ƒè¿›ç¨‹æœªå¯ç”¨å¼ºåˆ¶èµ„æºé™åˆ¶ï¼Œä»…ä½¿ç”¨ç›‘æ§æ¨¡å¼")
                
        except Exception as e:
            print(f"âš ï¸ è®¾ç½®èµ„æºé™åˆ¶å™¨å¤±è´¥: {e}")
            self.resource_limiter = None
    
    def _on_resource_limit_exceeded(self, event_type: str, current_value: float, limit_value: float):
        """å¤„ç†èµ„æºé™åˆ¶è¶…é™"""
        resource_name = {"memory_limit": "å†…å­˜", "cpu_limit": "CPU", 
                        "disk_limit": "ç£ç›˜", "process_limit": "è¿›ç¨‹"}
        resource_name = resource_name.get(event_type, event_type)
        
        error_msg = f"ğŸš¨ è®­ç»ƒè¿‡ç¨‹{resource_name}èµ„æºè¶…é™ï¼å½“å‰: {current_value:.2f}, é™åˆ¶: {limit_value:.2f}"
        print(error_msg)
        self.status_updated.emit(error_msg)
        
        # åœæ­¢è®­ç»ƒ
        self.stop_training = True
        self.training_error.emit(f"è®­ç»ƒå› {resource_name}èµ„æºè¶…é™è€Œä¸­æ–­")
    
    def _connect_component_signals(self):
        """è¿æ¥å„ä¸ªç»„ä»¶çš„ä¿¡å·"""
        self.model_factory.status_updated.connect(self.status_updated)
        self.model_factory.model_download_failed.connect(self.model_download_failed)
        
        self.weight_calculator.status_updated.connect(self.status_updated)
        
        self.model_configurator.status_updated.connect(self.status_updated)
        
        self.tensorboard_logger.status_updated.connect(self.status_updated)
        
        self.validator.status_updated.connect(self.status_updated)
        self.validator.validation_error.connect(self.training_error)
    
    def run(self):
        """çº¿ç¨‹è¿è¡Œå…¥å£ï¼Œæ‰§è¡Œæ¨¡å‹è®­ç»ƒ"""
        try:
            # é‡ç½®åœæ­¢æ ‡å¿—
            self.stop_training = False
            
            # ğŸ” å®Œæ•´çš„å‚æ•°æ¥æ”¶éªŒè¯
            print("=" * 60)
            print("ğŸ” è®­ç»ƒçº¿ç¨‹å‚æ•°æ¥æ”¶éªŒè¯")
            print("=" * 60)
            
            # åŸºç¡€è®­ç»ƒå‚æ•°
            print("ğŸ“‹ åŸºç¡€è®­ç»ƒå‚æ•°:")
            basic_params = ['data_dir', 'model_name', 'num_epochs', 'batch_size', 'learning_rate', 
                          'model_save_dir', 'task_type', 'use_tensorboard']
            for param in basic_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                print(f"   {param}: {value}")
            
            # é«˜çº§è®­ç»ƒå‚æ•°
            print("\nğŸ”§ é«˜çº§è®­ç»ƒå‚æ•°:")
            advanced_params = ['optimizer', 'weight_decay', 'lr_scheduler', 'use_augmentation',
                             'early_stopping', 'early_stopping_patience', 'gradient_clipping',
                             'gradient_clipping_value', 'mixed_precision', 'dropout_rate',
                             'activation_function']
            for param in advanced_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                print(f"   {param}: {value}")
            
            # é¢„è®­ç»ƒæ¨¡å‹å‚æ•°
            print("\nğŸ—ï¸ é¢„è®­ç»ƒæ¨¡å‹å‚æ•°:")
            pretrained_params = ['use_pretrained', 'pretrained_path', 'use_local_pretrained', 'pretrained_model']
            for param in pretrained_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                print(f"   {param}: {value}")
            
            # ç±»åˆ«æƒé‡å‚æ•°
            print("\nâš–ï¸ ç±»åˆ«æƒé‡å‚æ•°:")
            weight_params = ['use_class_weights', 'weight_strategy', 'class_weights', 'custom_class_weights']
            for param in weight_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                print(f"   {param}: {value}")
            
            # ç›®æ ‡æ£€æµ‹ç‰¹æœ‰å‚æ•°ï¼ˆå¦‚æœæ˜¯æ£€æµ‹ä»»åŠ¡ï¼‰
            if self.config.get('task_type') == 'detection':
                print("\nğŸ¯ ç›®æ ‡æ£€æµ‹ç‰¹æœ‰å‚æ•°:")
                detection_params = ['iou_threshold', 'conf_threshold', 'resolution', 'use_mosaic',
                                  'use_multiscale', 'use_ema', 'nms_threshold', 'use_fpn']
                for param in detection_params:
                    value = self.config.get(param, 'æœªè®¾ç½®')
                    print(f"   {param}: {value}")
            
            # èµ„æºé™åˆ¶å‚æ•°
            print("\nğŸ’¾ èµ„æºä¸æ§åˆ¶å‚æ•°:")
            resource_params = ['enable_resource_limits', 'metrics', 'model_note', 'layer_config']
            for param in resource_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                if param == 'layer_config' and isinstance(value, dict):
                    print(f"   {param}: å·²é…ç½®å±‚å‚æ•° (å…±{len(value)}é¡¹)")
                else:
                    print(f"   {param}: {value}")
            
            # ç›®å½•é…ç½®å‚æ•°
            print("\nğŸ“ ç›®å½•é…ç½®å‚æ•°:")
            dir_params = ['default_param_save_dir', 'tensorboard_log_dir']
            for param in dir_params:
                value = self.config.get(param, 'æœªè®¾ç½®')
                print(f"   {param}: {value}")
            
            print("=" * 60)
            print(f"âœ… å‚æ•°æ¥æ”¶éªŒè¯å®Œæˆï¼Œå…±æ¥æ”¶ {len(self.config)} ä¸ªå‚æ•°")
            print("=" * 60)
            
            # å¯åŠ¨èµ„æºé™åˆ¶å™¨ç›‘æ§
            if self.resource_limiter:
                self.resource_limiter.start_monitoring()
                self.status_updated.emit("âœ… å¼ºåˆ¶èµ„æºé™åˆ¶å·²å¯åŠ¨")
            
            # éªŒè¯é…ç½®
            if not self.validator.validate_config(self.config):
                return
            
            # æå–åŸºæœ¬å‚æ•°
            data_dir = self.config.get('data_dir', '')
            model_name = self.config.get('model_name', 'ResNet50')
            num_epochs = self.config.get('num_epochs', 20)
            batch_size = self.config.get('batch_size', 32)
            learning_rate = self.config.get('learning_rate', 0.001)
            model_save_dir = self.config.get('model_save_dir', 'models/saved_models')
            task_type = self.config.get('task_type', 'classification')
            use_tensorboard = self.config.get('use_tensorboard', True)
            
            print(f"ğŸš€ å¼€å§‹æ‰§è¡Œè®­ç»ƒæµç¨‹...")
            
            # è°ƒç”¨è®­ç»ƒæµç¨‹
            self.train_model(
                data_dir=data_dir,
                model_name=model_name,
                num_epochs=num_epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                model_save_dir=model_save_dir,
                task_type=task_type,
                use_tensorboard=use_tensorboard
            )
            
            # è®­ç»ƒå®Œæˆ
            self.training_finished.emit()
            
        except Exception as e:
            self.training_error.emit(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            # ç¡®ä¿åœæ­¢èµ„æºé™åˆ¶å™¨
            if self.resource_limiter:
                self.resource_limiter.stop_monitoring()
                self.status_updated.emit("ğŸ”š èµ„æºé™åˆ¶å™¨å·²åœæ­¢")
    
    def stop(self):
        """åœæ­¢è®­ç»ƒè¿‡ç¨‹"""
        self.stop_training = True
        
        # åœæ­¢èµ„æºé™åˆ¶å™¨
        if self.resource_limiter:
            self.resource_limiter.request_stop()
            self.status_updated.emit("ğŸ›‘ å·²è¯·æ±‚èµ„æºé™åˆ¶å™¨åœæ­¢æ‰€æœ‰æ“ä½œ")
        
        self.status_updated.emit("è®­ç»ƒçº¿ç¨‹æ­£åœ¨åœæ­¢...")
    
    def train_model(self, data_dir, model_name, num_epochs, batch_size, learning_rate, 
                   model_save_dir, task_type='classification', use_tensorboard=True):
        """æ‰§è¡Œæ¨¡å‹è®­ç»ƒ"""
        try:
            # æ ‡å‡†åŒ–è·¯å¾„æ ¼å¼
            data_dir = os.path.normpath(data_dir).replace('\\', '/')
            model_save_dir = os.path.normpath(model_save_dir).replace('\\', '/')
            
            # å‡†å¤‡æ•°æ®
            dataloaders, dataset_sizes, class_names, num_classes = self._prepare_data(
                data_dir, batch_size, task_type
            )
            
            if self.stop_training:
                return
            
            # åˆ›å»ºå’Œé…ç½®æ¨¡å‹
            self.model = self._create_and_configure_model(model_name, num_classes, task_type)
            
            if self.stop_training:
                return
            
            # è®¡ç®—ç±»åˆ«æƒé‡å’Œè®¾ç½®æŸå¤±å‡½æ•°
            criterion = self._setup_loss_function(dataloaders['train'], class_names)
            
            if self.stop_training:
                return
            
            # è®¾ç½®ä¼˜åŒ–å™¨
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            
            # åˆå§‹åŒ–TensorBoard
            tensorboard_log_dir = None
            if use_tensorboard:
                tensorboard_log_dir = self.tensorboard_logger.initialize(self.config, model_name)
                self.training_info['tensorboard_log_dir'] = tensorboard_log_dir
                
                # è®°å½•æ¨¡å‹å›¾å’Œç±»åˆ«ä¿¡æ¯
                self.tensorboard_logger.log_model_graph(self.model, dataloaders['train'], self.device)
                
                class_weights = getattr(self.weight_calculator, 'class_weights', None)
                class_distribution = self.weight_calculator.get_class_distribution()
                if class_weights is not None and class_distribution:
                    self.tensorboard_logger.log_class_info(
                        class_names, class_distribution, class_weights, epoch=0
                    )
            
            if self.stop_training:
                return
            
            # æ‰§è¡Œè®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨èµ„æºé™åˆ¶çš„è®­ç»ƒå™¨å¦‚æœå¯ç”¨ï¼‰
            if self.resource_limiter:
                # ä½¿ç”¨èµ„æºé™åˆ¶çš„è®­ç»ƒå™¨
                self.resource_limited_trainer = enable_resource_limited_training(self)
                best_acc = self._resource_limited_training_loop(
                    dataloaders, dataset_sizes, class_names, num_epochs, 
                    criterion, optimizer, model_name, model_save_dir
                )
            else:
                # ä½¿ç”¨æ ‡å‡†è®­ç»ƒå¾ªç¯
                best_acc = self._training_loop(
                    dataloaders, dataset_sizes, class_names, num_epochs, 
                    criterion, optimizer, model_name, model_save_dir
                )
            
            if self.stop_training:
                return
            
            # ä¿å­˜è®­ç»ƒä¿¡æ¯
            self._save_training_info(model_name, num_epochs, batch_size, learning_rate, 
                                   best_acc, class_names, model_save_dir)
            
            # å…³é—­TensorBoard
            self.tensorboard_logger.close()
            
            self.status_updated.emit(f'è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}')
            
        except Exception as e:
            self.training_error.emit(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def _prepare_data(self, data_dir, batch_size, task_type):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        if task_type != 'classification':
            raise ValueError(f"å½“å‰ä»…æ”¯æŒåˆ†ç±»ä»»åŠ¡ï¼Œä¸æ”¯æŒ: {task_type}")
        
        self.status_updated.emit("åŠ è½½åˆ†ç±»æ•°æ®é›†...")
        
        # æ•°æ®è½¬æ¢
        data_transforms = {
            'train': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }
        
        # åŠ è½½æ•°æ®é›†
        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                                data_transforms[x])
                        for x in ['train', 'val']}
        
        dataloaders = {x: DataLoader(image_datasets[x],
                                   batch_size=batch_size,
                                   shuffle=True,
                                   num_workers=4)
                      for x in ['train', 'val']}
        
        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
        class_names = image_datasets['train'].classes
        num_classes = len(class_names)
        
        return dataloaders, dataset_sizes, class_names, num_classes
    
    def _create_and_configure_model(self, model_name, num_classes, task_type):
        """åˆ›å»ºå’Œé…ç½®æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = self.model_factory.create_model(model_name, num_classes, task_type)
        
        # é…ç½®æ¨¡å‹
        model = self.model_configurator.configure_model(model, self.config)
        
        # ç§»åˆ°è®¾å¤‡
        model = model.to(self.device)
        
        return model
    
    def _setup_loss_function(self, train_dataset, class_names):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        use_class_weights = self.config.get('use_class_weights', True)
        weight_strategy = self.config.get('weight_strategy', 'balanced')
        
        if use_class_weights:
            class_weights = self.weight_calculator.calculate_class_weights(
                train_dataset.dataset, class_names, self.config, self.device
            )
            criterion = nn.CrossEntropyLoss(weight=class_weights)
            self.status_updated.emit(f"ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ï¼Œæƒé‡ç­–ç•¥: {weight_strategy}")
        else:
            criterion = nn.CrossEntropyLoss()
            self.status_updated.emit("ä½¿ç”¨æ ‡å‡†æŸå¤±å‡½æ•°ï¼ˆæ— ç±»åˆ«æƒé‡ï¼‰")
        
        return criterion
    
    def _training_loop(self, dataloaders, dataset_sizes, class_names, num_epochs, 
                      criterion, optimizer, model_name, model_save_dir):
        """æ‰§è¡Œè®­ç»ƒå¾ªç¯"""
        best_acc = 0.0
        
        for epoch in range(num_epochs):
            if self.stop_training:
                self.status_updated.emit("è®­ç»ƒå·²åœæ­¢")
                break
            
            self.status_updated.emit(f'Epoch {epoch+1}/{num_epochs}')
            
            # è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
            for phase in ['train', 'val']:
                if self.stop_training:
                    break
                
                epoch_loss, epoch_acc, all_preds, all_labels = self._train_epoch(
                    phase, dataloaders, dataset_sizes, criterion, optimizer, epoch, num_epochs
                )
                
                if self.stop_training:
                    break
                
                # è®°å½•åˆ°TensorBoard
                self.tensorboard_logger.log_epoch_metrics(epoch, phase, epoch_loss, epoch_acc)
                
                # è®°å½•æ ·æœ¬å›¾åƒï¼ˆæ¯5ä¸ªepochä¸€æ¬¡ï¼‰
                if phase == 'val' and epoch % 5 == 0:
                    self.tensorboard_logger.log_sample_images(dataloaders[phase], epoch)
                
                # è®°å½•æ··æ·†çŸ©é˜µï¼ˆéªŒè¯é˜¶æ®µï¼‰
                if phase == 'val':
                    self.tensorboard_logger.log_confusion_matrix(
                        all_labels, all_preds, class_names, epoch
                    )
                
                # åˆ·æ–°TensorBoardæ•°æ®
                self.tensorboard_logger.flush()
            
            if self.stop_training:
                break
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                self._save_best_model(model_name, model_save_dir, epoch, best_acc)
        
        return best_acc
    
    def _resource_limited_training_loop(self, dataloaders, dataset_sizes, class_names, num_epochs, 
                                       criterion, optimizer, model_name, model_save_dir):
        """æ‰§è¡Œå¸¦èµ„æºé™åˆ¶çš„è®­ç»ƒå¾ªç¯"""
        try:
            best_acc = 0.0
            
            def save_callback(epoch, model, optimizer, train_result, val_result):
                """ä¿å­˜å›è°ƒå‡½æ•°"""
                nonlocal best_acc
                val_acc = val_result.get('val_accuracy', 0) / 100.0  # è½¬æ¢ä¸ºå°æ•°
                
                if val_acc > best_acc:
                    best_acc = val_acc
                    self._save_best_model(model_name, model_save_dir, epoch, best_acc)
                
                # è®°å½•åˆ°TensorBoard
                if hasattr(self, 'tensorboard_logger'):
                    self.tensorboard_logger.log_epoch_metrics(epoch-1, 'train', 
                                                            train_result['loss'], train_result['accuracy']/100.0)
                    self.tensorboard_logger.log_epoch_metrics(epoch-1, 'val', 
                                                            val_result['val_loss'], val_result['val_accuracy']/100.0)
                    self.tensorboard_logger.flush()
                
                # å‘é€epochç»“æœ
                epoch_data = {
                    'epoch': epoch,
                    'phase': 'val',
                    'loss': val_result['val_loss'],
                    'accuracy': val_result['val_accuracy'],
                    'batch': len(dataloaders['val']),
                    'total_batches': len(dataloaders['val'])
                }
                self.epoch_finished.emit(epoch_data)
                
                # æ›´æ–°è¿›åº¦
                progress = int((epoch / num_epochs) * 100)
                self.progress_updated.emit(progress)
            
            # ä½¿ç”¨èµ„æºé™åˆ¶çš„è®­ç»ƒå™¨
            self.resource_limited_trainer.train_with_resource_limits(
                epochs=num_epochs,
                train_loader=dataloaders['train'],
                val_loader=dataloaders['val'],
                model=self.model,
                optimizer=optimizer,
                criterion=criterion,
                device=self.device,
                save_callback=save_callback
            )
            
            return best_acc
            
        except ResourceLimitException as e:
            error_msg = f"è®­ç»ƒå› èµ„æºé™åˆ¶ä¸­æ–­: {e}"
            self.status_updated.emit(error_msg)
            self.training_error.emit(error_msg)
            return 0.0
        except Exception as e:
            error_msg = f"èµ„æºé™åˆ¶è®­ç»ƒå¾ªç¯å‡ºé”™: {e}"
            self.status_updated.emit(error_msg)
            self.training_error.emit(error_msg)
            return 0.0
    
    def _train_epoch(self, phase, dataloaders, dataset_sizes, criterion, optimizer, epoch, num_epochs):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        if phase == 'train':
            self.model.train()
        else:
            self.model.eval()
        
        running_loss = 0.0
        running_corrects = 0
        all_preds = []
        all_labels = []
        
        # éå†æ•°æ®
        for i, (inputs, labels) in enumerate(dataloaders[phase]):
            if self.stop_training:
                break
            
            inputs = inputs.to(self.device)
            labels = labels.to(self.device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            with torch.set_grad_enabled(phase == 'train'):
                outputs = self.model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)
                
                # åå‘ä¼ æ’­
                if phase == 'train':
                    loss.backward()
                    optimizer.step()
            
            if self.stop_training:
                break
            
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
            
            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # æ›´æ–°è¿›åº¦
            progress = int(((epoch * len(dataloaders[phase]) + i + 1) /
                          (num_epochs * len(dataloaders[phase]))) * 100)
            self.progress_updated.emit(progress)
            
            # å‘é€è®­ç»ƒçŠ¶æ€æ›´æ–°
            if i % 10 == 0:
                current_loss = running_loss / ((i + 1) * inputs.size(0))
                current_acc = running_corrects.double() / ((i + 1) * inputs.size(0))
                epoch_data = {
                    'epoch': epoch + 1,
                    'phase': phase,
                    'loss': float(current_loss),
                    'accuracy': float(current_acc.item()),
                    'batch': i + 1,
                    'total_batches': len(dataloaders[phase])
                }
                self.epoch_finished.emit(epoch_data)
        
        if self.stop_training:
            return 0, 0, [], []
        
        epoch_loss = running_loss / dataset_sizes[phase]
        epoch_acc = running_corrects.double() / dataset_sizes[phase]
        
        # å‘é€epochç»“æœ
        epoch_data = {
            'epoch': epoch + 1,
            'phase': phase,
            'loss': float(epoch_loss),
            'accuracy': float(epoch_acc.item()) if isinstance(epoch_acc, torch.Tensor) else float(epoch_acc),
            'batch': len(dataloaders[phase]),
            'total_batches': len(dataloaders[phase])
        }
        self.epoch_finished.emit(epoch_data)
        
        return epoch_loss, epoch_acc, all_preds, all_labels
    
    def _save_best_model(self, model_name, model_save_dir, epoch, best_acc):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        model_note = self.config.get('model_note', '')
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        
        # ä¿å­˜PyTorchæ¨¡å‹
        model_save_path = os.path.join(model_save_dir, f'{model_name}_{timestamp}_{model_note}_best.pth')
        torch.save(self.model.state_dict(), model_save_path)
        self.status_updated.emit(f'ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒEpoch {epoch+1}, Acc: {best_acc:.4f}')
        
        # å¯¼å‡ºONNXæ¨¡å‹
        try:
            onnx_save_path = os.path.join(model_save_dir, f'{model_name}_{timestamp}_{model_note}_best.onnx')
            sample_input = torch.randn(1, 3, 224, 224).to(self.device)
            torch.onnx.export(
                self.model, 
                sample_input, 
                onnx_save_path,
                export_params=True,
                opset_version=11,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
            )
            self.status_updated.emit(f'å¯¼å‡ºONNXæ¨¡å‹: {onnx_save_path}')
        except Exception as e:
            self.status_updated.emit(f'å¯¼å‡ºONNXæ¨¡å‹æ—¶å‡ºé”™: {str(e)}')
    
    def _save_training_info(self, model_name, num_epochs, batch_size, learning_rate, 
                           best_acc, class_names, model_save_dir):
        """ä¿å­˜è®­ç»ƒä¿¡æ¯"""
        model_note = self.config.get('model_note', '')
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_save_dir, f'{model_name}_{timestamp}_{model_note}_final.pth')
        torch.save(self.model.state_dict(), final_model_path)
        self.status_updated.emit(f'ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_model_path}')
        
        # ä¿å­˜ç±»åˆ«ä¿¡æ¯
        class_info = {
            'class_names': class_names,
            'class_to_idx': {name: idx for idx, name in enumerate(class_names)}
        }
        
        os.makedirs(model_save_dir, exist_ok=True)
        with open(os.path.join(model_save_dir, 'class_info.json'), 'w') as f:
            json.dump(class_info, f)
        
        # è®°å½•è®­ç»ƒä¿¡æ¯
        training_info = {
            'model_name': model_name,
            'num_epochs': num_epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'best_accuracy': best_acc.item() if isinstance(best_acc, torch.Tensor) else best_acc,
            'class_names': class_names,
            'model_path': final_model_path,
            'timestamp': timestamp
        }
        
        with open(os.path.join(model_save_dir, 'training_info.json'), 'w') as f:
            json.dump(training_info, f, indent=4) 