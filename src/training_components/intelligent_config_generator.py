"""
æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨

åŸºäºè®­ç»ƒé…ç½®å’Œå®æ—¶æ•°æ®ï¼Œä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒé…ç½®
ä¸»è¦åŠŸèƒ½ï¼š
- æ¥æ”¶å½“å‰è®­ç»ƒé…ç½®å’Œå®æ—¶è®­ç»ƒæ•°æ®
- ä½¿ç”¨LLMåˆ†æç”Ÿæˆä¼˜åŒ–é…ç½®
- è°ƒç”¨ç°æœ‰é…ç½®åº”ç”¨æœºåˆ¶
- è®°å½•é…ç½®è°ƒæ•´å†å²
"""

import os
import json
import time
import copy
import threading
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from PyQt5.QtCore import QObject, pyqtSignal, QThread
from PyQt5.QtWidgets import QMessageBox

from .real_time_metrics_collector import get_global_metrics_collector
from ..llm.llm_framework import LLMFramework
from ..llm.analysis_engine import TrainingAnalysisEngine
from .parameter_tuning_report_generator import ParameterTuningReportGenerator


@dataclass
class ConfigAdjustment:
    """é…ç½®è°ƒæ•´è®°å½•"""
    adjustment_id: str
    timestamp: float
    original_config: Dict[str, Any]
    adjusted_config: Dict[str, Any]
    changes: Dict[str, Any]  # å…·ä½“å˜æ›´çš„å‚æ•°
    reason: str  # è°ƒæ•´åŸå› 
    training_metrics: Dict[str, Any]  # è§¦å‘è°ƒæ•´çš„è®­ç»ƒæŒ‡æ ‡
    llm_analysis: Dict[str, Any]  # LLMåˆ†æç»“æœ
    status: str  # 'pending', 'applied', 'failed'


@dataclass
class TrainingSession:
    """è®­ç»ƒä¼šè¯è®°å½•"""
    session_id: str
    start_time: float
    original_config: Dict[str, Any]
    adjustments: List[ConfigAdjustment]
    final_config: Optional[Dict[str, Any]]
    status: str  # 'running', 'completed', 'failed'


class IntelligentConfigGenerator(QObject):
    """æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨"""
    
    # ä¿¡å·å®šä¹‰
    config_generated = pyqtSignal(dict)  # é…ç½®ç”Ÿæˆå®Œæˆä¿¡å·
    config_applied = pyqtSignal(dict)    # é…ç½®åº”ç”¨å®Œæˆä¿¡å·
    adjustment_recorded = pyqtSignal(dict)  # è°ƒæ•´è®°å½•ä¿¡å·
    status_updated = pyqtSignal(str)     # çŠ¶æ€æ›´æ–°ä¿¡å·
    error_occurred = pyqtSignal(str)     # é”™è¯¯ä¿¡å·
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self.llm_framework = None
        self.analysis_engine = None
        self.metrics_collector = None
        self.report_generator = None
        
        # ä¼šè¯ç®¡ç†
        self.current_session: Optional[TrainingSession] = None
        self.adjustment_history: List[ConfigAdjustment] = []
        
        # é…ç½®çº¦æŸ
        self.parameter_constraints = self._load_parameter_constraints()
        
        # åˆ†æç»“æœç¼“å­˜ - ç¡®ä¿å¾®è°ƒæŠ¥å‘Šå’Œè®­ç»ƒå‚æ•°çš„ä¸€è‡´æ€§
        self._cached_analysis_result = None
        self._cached_analysis_key = None

        # å»é‡ä¸å¹¶å‘ä¿æŠ¤
        self._adjustment_lock = threading.Lock()
        self._last_adjustment_signature = None
        self._last_adjustment_time = 0.0
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def update_config(self, config_dict: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        try:
            # æ›´æ–°LLMé…ç½®
            if 'llm_config' in config_dict:
                llm_config = config_dict['llm_config']
                if self.llm_framework:
                    self.llm_framework.update_config(llm_config)
            
            # æ›´æ–°æŠ¥å‘Šç”Ÿæˆå™¨é…ç½®
            if self.report_generator:
                self.report_generator.update_config(config_dict)
            
            # æ›´æ–°å¹²é¢„é˜ˆå€¼é…ç½®
            intervention_keys = [
                'overfitting_threshold', 'underfitting_threshold', 'stagnation_epochs',
                'divergence_threshold', 'min_training_epochs', 'tuning_strategy',
                'enable_auto_intervention', 'intervention_cooldown', 'max_interventions_per_session'
            ]
            
            for key in intervention_keys:
                if key in config_dict:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é…ç½®æ›´æ–°é€»è¾‘
                    print(f"[DEBUG] é…ç½®ç”Ÿæˆå™¨æ›´æ–°é…ç½® {key}: {config_dict[key]}")
            
            print(f"[INFO] æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨é…ç½®å·²æ›´æ–°")
            
        except Exception as e:
            print(f"[ERROR] æ›´æ–°é…ç½®ç”Ÿæˆå™¨é…ç½®å¤±è´¥: {str(e)}")
            raise
        
    def _initialize_components(self):
        """åˆå§‹åŒ–ç›¸å…³ç»„ä»¶"""
        try:
            # è·å–LLMé…ç½®
            llm_config = self._load_llm_config()
            adapter_type = llm_config.get('adapter_type', 'openai')
            
            # ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥
            if adapter_type == 'mock':
                self.error_occurred.emit("âŒ ç”Ÿäº§ç¯å¢ƒä¸å…è®¸ä½¿ç”¨mock LLMé€‚é…å™¨ï¼è¯·åœ¨æ™ºèƒ½è®­ç»ƒè®¾ç½®ä¸­é…ç½®çœŸå®çš„LLMæœåŠ¡ã€‚")
                raise ValueError("ç”Ÿäº§ç¯å¢ƒä¸å…è®¸ä½¿ç”¨mock LLMé€‚é…å™¨")
            
            # åˆå§‹åŒ–LLMæ¡†æ¶
            self.llm_framework = LLMFramework(
                adapter_type=adapter_type,
                adapter_config=llm_config.get('adapter_config', {})
            )
            self.llm_framework.start()
            
            # åˆå§‹åŒ–åˆ†æå¼•æ“
            self.analysis_engine = TrainingAnalysisEngine(self.llm_framework.llm_adapter)
            
            # è·å–æŒ‡æ ‡é‡‡é›†å™¨
            self.metrics_collector = get_global_metrics_collector()
            
            # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
            self.report_generator = ParameterTuningReportGenerator()
            
            print(f"âœ… æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨LLMé€‚é…å™¨: {adapter_type}")
            
        except Exception as e:
            self.error_occurred.emit(f"åˆå§‹åŒ–ç»„ä»¶å¤±è´¥: {str(e)}")
            # ç”Ÿäº§ç¯å¢ƒä¸­ä¸å…è®¸ä½¿ç”¨æ¨¡æ‹Ÿé€‚é…å™¨ä½œä¸ºåå¤‡
            raise
    
    def _load_llm_config(self) -> Dict[str, Any]:
        """åŠ è½½LLMé…ç½®"""
        try:
            print("[DEBUG] å¼€å§‹åŠ è½½LLMé…ç½®...")
            
            # é¦–å…ˆå°è¯•ä»æ™ºèƒ½è®­ç»ƒè®¾ç½®ä¸­åŠ è½½
            intelligent_config_file = "setting/intelligent_training_config.json"
            if os.path.exists(intelligent_config_file):
                print(f"[DEBUG] æ£€æŸ¥æ™ºèƒ½è®­ç»ƒé…ç½®æ–‡ä»¶: {intelligent_config_file}")
                with open(intelligent_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    llm_config = config.get('llm_config', {})
                    if llm_config:
                        print(f"[DEBUG] ä»æ™ºèƒ½è®­ç»ƒé…ç½®ä¸­æ‰¾åˆ°LLMé…ç½®: {llm_config}")
                        return llm_config
                    else:
                        print("[DEBUG] æ™ºèƒ½è®­ç»ƒé…ç½®ä¸­æœªæ‰¾åˆ°llm_config")
            else:
                print(f"[DEBUG] æ™ºèƒ½è®­ç»ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {intelligent_config_file}")
            
            # ç„¶åå°è¯•ä»AIè®¾ç½®ä¸­åŠ è½½
            ai_config_file = "setting/ai_config.json"
            if os.path.exists(ai_config_file):
                print(f"[DEBUG] æ£€æŸ¥AIé…ç½®æ–‡ä»¶: {ai_config_file}")
                with open(ai_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    
                    # è·å–é»˜è®¤é€‚é…å™¨ç±»å‹
                    default_adapter = config.get('general', {}).get('default_adapter', 'openai')
                    print(f"[DEBUG] ä»AIé…ç½®ä¸­è·å–é»˜è®¤é€‚é…å™¨: {default_adapter}")
                    
                    # æ ¹æ®é€‚é…å™¨ç±»å‹è·å–å¯¹åº”é…ç½®
                    if default_adapter == 'deepseek':
                        adapter_config = config.get('deepseek', {})
                        print(f"[DEBUG] åŠ è½½DeepSeeké…ç½®: {adapter_config}")
                        result = {
                            'adapter_type': 'deepseek',
                            'adapter_config': {
                                'api_key': adapter_config.get('api_key', ''),
                                'base_url': adapter_config.get('base_url', ''),
                                'model': adapter_config.get('model', 'deepseek-chat'),
                                'temperature': adapter_config.get('temperature', 0.7),
                                'max_tokens': adapter_config.get('max_tokens', 3000)
                            }
                        }
                        print(f"[DEBUG] è¿”å›DeepSeeké…ç½®: {result}")
                        return result
                    elif default_adapter == 'openai':
                        adapter_config = config.get('openai', {})
                        return {
                            'adapter_type': 'openai',
                            'adapter_config': {
                                'api_key': adapter_config.get('api_key', ''),
                                'base_url': adapter_config.get('base_url', ''),
                                'model': adapter_config.get('model', 'gpt-4'),
                                'temperature': adapter_config.get('temperature', 0.7),
                                'max_tokens': adapter_config.get('max_tokens', 1000)
                            }
                        }
                    elif default_adapter == 'ollama':
                        adapter_config = config.get('ollama', {})
                        return {
                            'adapter_type': 'ollama',
                            'adapter_config': {
                                'base_url': adapter_config.get('base_url', ''),
                                'model': adapter_config.get('model', 'llama2'),
                                'temperature': adapter_config.get('temperature', 0.7),
                                'num_predict': adapter_config.get('num_predict', 1000),
                                'timeout': adapter_config.get('timeout', 120)
                            }
                        }
                    elif default_adapter == 'custom':
                        adapter_config = config.get('custom_api', {})
                        return {
                            'adapter_type': 'custom',
                            'adapter_config': {
                                'api_key': adapter_config.get('api_key', ''),
                                'base_url': adapter_config.get('base_url', ''),
                                'model': adapter_config.get('model', ''),
                                'temperature': adapter_config.get('temperature', 0.7),
                                'max_tokens': adapter_config.get('max_tokens', 1000)
                            }
                        }
            
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›é»˜è®¤é…ç½®
            return {
                'adapter_type': 'openai',
                'adapter_config': {
                    'api_key': '',
                    'model': 'gpt-4',
                    'temperature': 0.7,
                    'max_tokens': 1000
                }
            }
            
        except Exception as e:
            print(f"åŠ è½½LLMé…ç½®å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'adapter_type': 'openai',
                'adapter_config': {
                    'api_key': '',
                    'model': 'gpt-4',
                    'temperature': 0.7,
                    'max_tokens': 1000
                }
            }
    
    def _load_parameter_constraints(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½å‚æ•°çº¦æŸé…ç½®"""
        return {
            'learning_rate': {
                'min': 1e-6,
                'max': 0.1,
                'step': 1e-6,
                'description': 'å­¦ä¹ ç‡'
            },
            'batch_size': {
                'min': 1,
                'max': 128,
                'step': 1,
                'description': 'æ‰¹æ¬¡å¤§å°'
            },
            'num_epochs': {
                'min': 1,
                'max': 1000,
                'step': 1,
                'description': 'è®­ç»ƒè½®æ•°'
            },
            'dropout_rate': {
                'min': 0.0,
                'max': 0.9,
                'step': 0.01,
                'description': 'Dropoutç‡'
            },
            'weight_decay': {
                'min': 0.0,
                'max': 0.01,
                'step': 1e-6,
                'description': 'æƒé‡è¡°å‡'
            },
            'early_stopping_patience': {
                'min': 1,
                'max': 50,
                'step': 1,
                'description': 'æ—©åœè€å¿ƒå€¼'
            }
        }
    
    def clear_analysis_cache(self):
        """æ¸…é™¤åˆ†æç¼“å­˜ï¼Œç¡®ä¿æ–°çš„åˆ†æä¼šè¯å¼€å§‹æ—¶ç¼“å­˜æ˜¯å¹²å‡€çš„"""
        self._cached_analysis_result = None
        self._cached_analysis_key = None
        print("[DEBUG] åˆ†æç¼“å­˜å·²æ¸…é™¤")
    
    def start_training_session(self, initial_config: Dict[str, Any]) -> str:
        """å¼€å§‹æ–°çš„è®­ç»ƒä¼šè¯"""
        try:
            # æ¸…é™¤åˆ†æç¼“å­˜ï¼Œç¡®ä¿æ–°ä¼šè¯å¼€å§‹æ—¶ç¼“å­˜æ˜¯å¹²å‡€çš„
            self.clear_analysis_cache()
            
            session_id = f"session_{int(time.time())}"
            
            self.current_session = TrainingSession(
                session_id=session_id,
                start_time=time.time(),
                original_config=copy.deepcopy(initial_config),
                adjustments=[],
                final_config=None,
                status='running'
            )
            
            self.status_updated.emit(f"å¼€å§‹è®­ç»ƒä¼šè¯: {session_id}")
            return session_id
            
        except Exception as e:
            self.error_occurred.emit(f"å¼€å§‹è®­ç»ƒä¼šè¯å¤±è´¥: {str(e)}")
            return ""
    
    def generate_optimized_config(self, 
                                current_config: Dict[str, Any],
                                training_metrics: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
        try:
            self.status_updated.emit("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–é…ç½®...")
            print(f"[DEBUG] generate_optimized_config called | thread={threading.current_thread().name}")
            
            # è·å–å®æ—¶è®­ç»ƒæ•°æ®
            if training_metrics is None:
                real_data = self.metrics_collector.get_current_training_data_for_ai()
                if 'error' in real_data:
                    self.error_occurred.emit(f"æ— æ³•è·å–è®­ç»ƒæ•°æ®: {real_data['error']}")
                    return current_config
                training_metrics = real_data.get('current_metrics', {})
            print(f"[DEBUG] metrics epoch={training_metrics.get('epoch')} train_loss={training_metrics.get('train_loss')} val_loss={training_metrics.get('val_loss')}")
            
            # ç”Ÿæˆç¼“å­˜é”®ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥ä½¿ç”¨ç›¸åŒåˆ†æç»“æœ
            cache_key = self._generate_analysis_cache_key(current_config, training_metrics)
            print(f"[DEBUG] analysis cache_key={cache_key}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ
            if (self._cached_analysis_key == cache_key and 
                self._cached_analysis_result is not None):
                self.status_updated.emit("ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœç¡®ä¿ä¸€è‡´æ€§...")
                analysis_result = self._cached_analysis_result
                print("[DEBUG] ä½¿ç”¨ç¼“å­˜åˆ†æç»“æœ")
            else:
                # ä½¿ç”¨LLMåˆ†æå½“å‰é…ç½®å’Œè®­ç»ƒæ•°æ®
                analysis_result = self._analyze_config_and_metrics(current_config, training_metrics)
                # ç¼“å­˜åˆ†æç»“æœ
                self._cached_analysis_result = analysis_result
                self._cached_analysis_key = cache_key
                print("[DEBUG] å†™å…¥ç¼“å­˜åˆ†æç»“æœ")
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization_suggestions = self._generate_optimization_suggestions(
                current_config, training_metrics, analysis_result
            )
            print(f"[DEBUG] å»ºè®®æ•°é‡={len(optimization_suggestions)}")
            
            # åº”ç”¨ä¼˜åŒ–å»ºè®®åˆ°é…ç½®
            optimized_config = self._apply_optimization_suggestions(
                current_config, optimization_suggestions
            )
            print("[DEBUG] åº”ç”¨å»ºè®®å®Œæˆ")
            
            # éªŒè¯é…ç½®æœ‰æ•ˆæ€§
            validated_config = self._validate_config(optimized_config)
            
            # è®°å½•é…ç½®è°ƒæ•´
            # ç¡®ä¿analysis_resultæ˜¯å­—å…¸æ ¼å¼
            if isinstance(analysis_result, str):
                # å¦‚æœåˆ†æç»“æœæ˜¯å­—ç¬¦ä¸²ï¼Œæå–å…¶ä¸­çš„å…³é”®ä¿¡æ¯
                analysis_dict = {
                    'reason': 'LLMæ™ºèƒ½åˆ†æå»ºè®®',
                    'analysis': analysis_result[:500] + '...' if len(analysis_result) > 500 else analysis_result
                }
            else:
                analysis_dict = analysis_result
            
            self._record_config_adjustment(
                current_config, validated_config, training_metrics, analysis_dict
            )
            
            self.status_updated.emit("ä¼˜åŒ–é…ç½®ç”Ÿæˆå®Œæˆ")
            return validated_config
            
        except Exception as e:
            self.error_occurred.emit(f"ç”Ÿæˆä¼˜åŒ–é…ç½®å¤±è´¥: {str(e)}")
            return current_config
    
    def _generate_analysis_cache_key(self, config: Dict[str, Any], metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æç¼“å­˜é”®ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥ä½¿ç”¨ç›¸åŒåˆ†æç»“æœ"""
        try:
            import hashlib
            
            # æå–å…³é”®é…ç½®å‚æ•°
            key_config = {
                'model_name': config.get('model_name'),
                'batch_size': config.get('batch_size'),
                'learning_rate': config.get('learning_rate'),
                'optimizer': config.get('optimizer'),
                'num_epochs': config.get('num_epochs'),
                'use_augmentation': config.get('use_augmentation'),
                'use_class_weights': config.get('use_class_weights'),
                'weight_strategy': config.get('weight_strategy'),
                'lr_scheduler': config.get('lr_scheduler'),
                'early_stopping': config.get('early_stopping'),
                'early_stopping_patience': config.get('early_stopping_patience'),
                'mixed_precision': config.get('mixed_precision'),
                'dropout_rate': config.get('dropout_rate'),
                'weight_decay': config.get('weight_decay'),
                'warmup_enabled': config.get('warmup_enabled'),
                'warmup_steps': config.get('warmup_steps'),
                'warmup_ratio': config.get('warmup_ratio'),
                'gradient_accumulation_enabled': config.get('gradient_accumulation_enabled'),
                'gradient_accumulation_steps': config.get('gradient_accumulation_steps'),
                'advanced_augmentation_enabled': config.get('advanced_augmentation_enabled'),
                'cutmix_prob': config.get('cutmix_prob'),
                'mixup_alpha': config.get('mixup_alpha'),
                'label_smoothing_enabled': config.get('label_smoothing_enabled'),
                'label_smoothing': config.get('label_smoothing'),
                'model_ema': config.get('model_ema'),
                'model_ema_decay': config.get('model_ema_decay')
            }
            
            # æå–å…³é”®è®­ç»ƒæŒ‡æ ‡
            key_metrics = {
                'epoch': metrics.get('epoch'),
                'train_loss': round(metrics.get('train_loss', 0), 4),
                'val_loss': round(metrics.get('val_loss', 0), 4),
                'train_accuracy': round(metrics.get('train_accuracy', 0), 4),
                'val_accuracy': round(metrics.get('val_accuracy', 0), 4),
                'learning_rate': round(metrics.get('learning_rate', 0), 6)
            }
            
            # ç”Ÿæˆå“ˆå¸Œé”®
            cache_data = {
                'config': key_config,
                'metrics': key_metrics,
                'timestamp': int(time.time() // 60)  # æŒ‰åˆ†é’Ÿç¼“å­˜ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„ç¼“å­˜å¤±æ•ˆ
            }
            
            cache_string = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
            cache_key = hashlib.md5(cache_string.encode('utf-8')).hexdigest()
            
            return cache_key
            
        except Exception as e:
            print(f"[WARNING] ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {str(e)}")
            return f"fallback_{int(time.time())}"
    
    def _analyze_config_and_metrics(self, 
                                  config: Dict[str, Any], 
                                  metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé…ç½®å’Œè®­ç»ƒæŒ‡æ ‡"""
        try:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            adapter_type = type(self.llm_framework.llm_adapter).__name__
            self.status_updated.emit(f"ğŸ” å¼€å§‹LLMåˆ†æï¼Œä½¿ç”¨é€‚é…å™¨: {adapter_type}")
            print(f"[DEBUG] ä½¿ç”¨LLMé€‚é…å™¨: {adapter_type}")
            print(f"[DEBUG] é€‚é…å™¨ç±»å‹: {getattr(self.llm_framework.llm_adapter, 'adapter_type', 'unknown')}")
            
            # æ„å»ºåˆ†ææç¤ºè¯
            prompt = self._build_config_analysis_prompt(config, metrics)
            print(f"[DEBUG] åˆ†ææç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
            self.status_updated.emit("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè®­ç»ƒåˆ†æ...")
            print(f"[DEBUG] å¼€å§‹è°ƒç”¨LLMåˆ†æ...")
            
            analysis_result = self.llm_framework.llm_adapter.analyze_metrics(metrics, prompt)
            
            print(f"[DEBUG] LLMåˆ†æç»“æœç±»å‹: {type(analysis_result)}")
            print(f"[DEBUG] LLMåˆ†æç»“æœé•¿åº¦: {len(str(analysis_result))} å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡æ‹Ÿç»“æœ
            if isinstance(analysis_result, str) and "æ¨¡æ‹Ÿåˆ†æç»“æœ" in analysis_result:
                self.status_updated.emit("âš ï¸ æ£€æµ‹åˆ°æ¨¡æ‹Ÿåˆ†æç»“æœï¼è¯·é…ç½®çœŸå®çš„LLMæœåŠ¡")
                print("[WARNING] æ£€æµ‹åˆ°æ¨¡æ‹Ÿåˆ†æç»“æœï¼Œè¯·é…ç½®çœŸå®çš„LLMæœåŠ¡")
            else:
                self.status_updated.emit("âœ… LLMåˆ†æå®Œæˆ")
                print("[DEBUG] LLMåˆ†æå®Œæˆï¼Œç»“æœçœ‹èµ·æ¥æ˜¯çœŸå®çš„")
            
            return analysis_result
            
        except Exception as e:
            self.error_occurred.emit(f"åˆ†æé…ç½®å’ŒæŒ‡æ ‡å¤±è´¥: {str(e)}")
            print(f"[ERROR] LLMåˆ†æå¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def _build_config_analysis_prompt(self, 
                                    config: Dict[str, Any], 
                                    metrics: Dict[str, Any]) -> str:
        """æ„å»ºé…ç½®åˆ†ææç¤ºè¯"""
        return f"""
è¯·åŸºäºä»¥ä¸‹è®­ç»ƒé…ç½®å’Œå®æ—¶è®­ç»ƒæ•°æ®ï¼Œæä¾›ä¸“ä¸šçš„é…ç½®ä¼˜åŒ–å»ºè®®ï¼š

## ğŸ“‹ å½“å‰è®­ç»ƒé…ç½®
```json
{json.dumps(config, ensure_ascii=False, indent=2)}
```

## ğŸ“Š å®æ—¶è®­ç»ƒæŒ‡æ ‡
```json
{json.dumps(metrics, ensure_ascii=False, indent=2)}
```

## ğŸ¯ åˆ†æè¦æ±‚

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›ä»¥ä¸‹åˆ†æï¼š

### 1. é…ç½®è¯„ä¼°
- å½“å‰é…ç½®æ˜¯å¦é€‚åˆå½“å‰æ•°æ®é›†å’Œä»»åŠ¡ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„é…ç½®é—®é¢˜æˆ–å†²çªï¼Ÿ
- å“ªäº›å‚æ•°å¯èƒ½éœ€è¦è°ƒæ•´ï¼Ÿ

### 2. è®­ç»ƒçŠ¶æ€åˆ†æ
- å½“å‰è®­ç»ƒçŠ¶æ€ï¼ˆæ”¶æ•›ã€è¿‡æ‹Ÿåˆã€æ¬ æ‹Ÿåˆç­‰ï¼‰
- è®­ç»ƒæŒ‡æ ‡åæ˜ çš„é—®é¢˜
- ä¸é…ç½®å‚æ•°çš„å…³è”æ€§

### 3. ä¼˜åŒ–å»ºè®®
è¯·æä¾›å…·ä½“çš„å‚æ•°è°ƒæ•´å»ºè®®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{{
    "suggestions": [
        {{
            "parameter": "learning_rate",
            "current_value": 0.001,
            "suggested_value": 0.0005,
            "reason": "è®­ç»ƒæŸå¤±ä¸‹é™ç¼“æ…¢ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡",
            "priority": "high"
        }},
        {{
            "parameter": "batch_size",
            "current_value": 32,
            "suggested_value": 16,
            "reason": "GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å‡å°æ‰¹æ¬¡å¤§å°",
            "priority": "medium"
        }}
    ]
}}
```

### 4. æ³¨æ„äº‹é¡¹
- åªå»ºè®®è°ƒæ•´ç°æœ‰é…ç½®ä¸­çš„å‚æ•°
- ç¡®ä¿å»ºè®®å€¼åœ¨åˆç†èŒƒå›´å†…
- ä¼˜å…ˆè€ƒè™‘é«˜ä¼˜å…ˆçº§å»ºè®®
- ä¿æŒé…ç½®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šæ€§å’Œå®ç”¨æ€§ã€‚
"""
    
    def _generate_optimization_suggestions(self, 
                                         config: Dict[str, Any],
                                         metrics: Dict[str, Any],
                                         analysis_result: Any) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        try:
            print(f"[DEBUG] åˆ†æç»“æœç±»å‹: {type(analysis_result)}")
            print(f"[DEBUG] åˆ†æç»“æœå†…å®¹: {str(analysis_result)[:200]}...")
            
            # ä»LLMåˆ†æç»“æœä¸­æå–å»ºè®®
            if isinstance(analysis_result, dict) and 'suggestions' in analysis_result:
                suggestions.extend(analysis_result['suggestions'])
            elif isinstance(analysis_result, str):
                # å¦‚æœåˆ†æç»“æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æå…¶ä¸­çš„å»ºè®®
                parsed_suggestions = self._parse_suggestions_from_text(analysis_result)
                suggestions.extend(parsed_suggestions)
            
            # åŸºäºè§„åˆ™ç”Ÿæˆé¢å¤–å»ºè®®
            rule_suggestions = self._generate_rule_based_suggestions(config, metrics)
            suggestions.extend(rule_suggestions)
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œç¡®ä¿ç›¸åŒä¼˜å…ˆçº§æ—¶æŒ‰å‚æ•°åæ’åºä»¥ä¿è¯ç¨³å®šæ€§
            suggestions.sort(key=lambda x: (x.get('priority', 'low'), x.get('parameter', '')), reverse=True)
            
            return suggestions
            
        except Exception as e:
            self.error_occurred.emit(f"ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: {str(e)}")
            print(f"[ERROR] ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _parse_suggestions_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­è§£æå»ºè®®"""
        suggestions = []
        
        try:
            print(f"[DEBUG] å¼€å§‹è§£æLLMè¿”å›çš„æ–‡æœ¬å»ºè®®")
            
            # é¦–å…ˆå°è¯•æå–JSONæ ¼å¼çš„å»ºè®®
            json_suggestions = self._extract_json_suggestions(text)
            if json_suggestions:
                print(f"[DEBUG] æˆåŠŸæå–åˆ° {len(json_suggestions)} ä¸ªJSONæ ¼å¼å»ºè®®")
                return json_suggestions
            
            # å¦‚æœæ²¡æœ‰JSONæ ¼å¼ï¼Œåˆ™è¿›è¡Œæ–‡æœ¬è§£æ
            print(f"[DEBUG] æœªæ‰¾åˆ°JSONæ ¼å¼å»ºè®®ï¼Œè¿›è¡Œæ–‡æœ¬è§£æ")
            lines = text.split('\n')
            current_suggestion = None
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æµ‹å»ºè®®é¡¹ï¼ˆä»¥æ•°å­—å¼€å¤´æˆ–åŒ…å«å…³é”®è¯ï¼‰
                if (line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.')) or
                    any(keyword in line.lower() for keyword in ['å»ºè®®', 'æ¨è', 'è°ƒæ•´', 'ä¼˜åŒ–', 'å¢åŠ ', 'å‡å°‘', 'é™ä½', 'æé«˜'])):
                    
                    if current_suggestion:
                        suggestions.append(current_suggestion)
                    
                    current_suggestion = {
                        'type': 'parameter_adjustment',
                        'description': line,
                        'priority': 'medium',
                        'confidence': 0.7
                    }
                elif current_suggestion and line.startswith(('  ', '\t', '-', '*')):
                    # è¿™æ˜¯å»ºè®®çš„è¯¦ç»†è¯´æ˜
                    current_suggestion['description'] += f" {line.strip()}"
            
            if current_suggestion:
                suggestions.append(current_suggestion)
                
        except Exception as e:
            print(f"[DEBUG] è§£ææ–‡æœ¬å»ºè®®å¤±è´¥: {str(e)}")
        
        return suggestions
    
    def _extract_json_suggestions(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–JSONæ ¼å¼çš„å»ºè®®"""
        try:
            import re
            import json
            
            # æŸ¥æ‰¾JSONä»£ç å—
            json_pattern = r'```json\s*(\{.*?\})\s*```'
            matches = re.findall(json_pattern, text, re.DOTALL)
            
            if not matches:
                # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾JSONå¯¹è±¡
                json_pattern = r'\{[^{}]*"suggestions"[^{}]*\[.*?\][^{}]*\}'
                matches = re.findall(json_pattern, text, re.DOTALL)
            
            for match in matches:
                try:
                    json_data = json.loads(match)
                    if 'suggestions' in json_data and isinstance(json_data['suggestions'], list):
                        print(f"[DEBUG] æˆåŠŸè§£æJSONå»ºè®®: {len(json_data['suggestions'])} ä¸ªå»ºè®®")
                        return json_data['suggestions']
                except json.JSONDecodeError as e:
                    print(f"[DEBUG] JSONè§£æå¤±è´¥: {str(e)}")
                    continue
            
            return []
            
        except Exception as e:
            print(f"[DEBUG] æå–JSONå»ºè®®å¤±è´¥: {str(e)}")
            return []
    
    def _generate_rule_based_suggestions(self, 
                                       config: Dict[str, Any],
                                       metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºè§„åˆ™ç”Ÿæˆå»ºè®®"""
        suggestions = []
        
        try:
            train_loss = metrics.get('train_loss', 0)
            val_loss = metrics.get('val_loss', 0)
            train_acc = metrics.get('train_accuracy', 0)
            val_acc = metrics.get('val_accuracy', 0)
            epoch = metrics.get('epoch', 0)
            
            # è¿‡æ‹Ÿåˆæ£€æµ‹ï¼ˆä½¿ç”¨å¯é…ç½®é˜ˆå€¼ï¼Œé»˜è®¤1.3ï¼‰
            overfit_ratio = self.current_session.original_config.get('overfitting_threshold', 1.3) if getattr(self, 'current_session', None) else 1.3
            if val_loss > train_loss * overfit_ratio and epoch > 5:
                suggestions.append({
                    'parameter': 'dropout_rate',
                    'current_value': config.get('dropout_rate', 0.0),
                    'suggested_value': min(0.9, config.get('dropout_rate', 0.0) + 0.1),
                    'reason': 'æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ Dropoutç‡',
                    'priority': 'high'
                })
                
                suggestions.append({
                    'parameter': 'weight_decay',
                    'current_value': config.get('weight_decay', 0.0001),
                    'suggested_value': config.get('weight_decay', 0.0001) * 1.5,
                    'reason': 'æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æƒé‡è¡°å‡',
                    'priority': 'high'
                })
            
            # æ¬ æ‹Ÿåˆæ£€æµ‹
            if train_acc < 0.6 and val_acc < 0.6 and epoch > 10:
                suggestions.append({
                    'parameter': 'learning_rate',
                    'current_value': config.get('learning_rate', 0.001),
                    'suggested_value': config.get('learning_rate', 0.001) * 1.5,
                    'reason': 'æ£€æµ‹åˆ°æ¬ æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ å­¦ä¹ ç‡',
                    'priority': 'high'
                })
            
            # æ”¶æ•›ç¼“æ…¢æ£€æµ‹
            if train_loss > 0.5 and epoch > 15:
                suggestions.append({
                    'parameter': 'learning_rate',
                    'current_value': config.get('learning_rate', 0.001),
                    'suggested_value': config.get('learning_rate', 0.001) * 0.5,
                    'reason': 'è®­ç»ƒæ”¶æ•›ç¼“æ…¢ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡',
                    'priority': 'medium'
                })
            
            return suggestions
            
        except Exception as e:
            self.error_occurred.emit(f"ç”Ÿæˆè§„åˆ™å»ºè®®å¤±è´¥: {str(e)}")
            return []
    
    def _apply_optimization_suggestions(self, 
                                      config: Dict[str, Any],
                                      suggestions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åº”ç”¨ä¼˜åŒ–å»ºè®®åˆ°é…ç½®"""
        try:
            optimized_config = copy.deepcopy(config)
            
            for suggestion in suggestions:
                param_name = suggestion.get('parameter')
                suggested_value = suggestion.get('suggested_value')
                
                if param_name in optimized_config and suggested_value is not None:
                    # åº”ç”¨çº¦æŸ
                    constrained_value = self._apply_parameter_constraints(param_name, suggested_value)
                    optimized_config[param_name] = constrained_value
            
            return optimized_config
            
        except Exception as e:
            self.error_occurred.emit(f"åº”ç”¨ä¼˜åŒ–å»ºè®®å¤±è´¥: {str(e)}")
            return config
    
    def _apply_parameter_constraints(self, param_name: str, value: Any) -> Any:
        """åº”ç”¨å‚æ•°çº¦æŸ"""
        if param_name not in self.parameter_constraints:
            return value
        
        constraints = self.parameter_constraints[param_name]
        
        # åº”ç”¨èŒƒå›´çº¦æŸ
        if 'min' in constraints:
            value = max(value, constraints['min'])
        if 'max' in constraints:
            value = min(value, constraints['max'])
        
        # åº”ç”¨æ­¥é•¿çº¦æŸ
        if 'step' in constraints and isinstance(value, (int, float)):
            step = constraints['step']
            value = round(value / step) * step
        
        return value
    
    def _validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        try:
            validated_config = copy.deepcopy(config)
            
            # éªŒè¯å¿…éœ€å‚æ•°
            required_params = ['model_name', 'batch_size', 'learning_rate', 'num_epochs']
            for param in required_params:
                if param not in validated_config:
                    self.error_occurred.emit(f"ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")
                    return config
            
            # éªŒè¯å‚æ•°ç±»å‹å’ŒèŒƒå›´
            for param_name, value in validated_config.items():
                if param_name in self.parameter_constraints:
                    validated_config[param_name] = self._apply_parameter_constraints(param_name, value)
            
            return validated_config
            
        except Exception as e:
            self.error_occurred.emit(f"éªŒè¯é…ç½®å¤±è´¥: {str(e)}")
            return config
    
    def _record_config_adjustment(self, 
                                original_config: Dict[str, Any],
                                adjusted_config: Dict[str, Any],
                                training_metrics: Dict[str, Any],
                                analysis_result: Dict[str, Any]):
        """è®°å½•é…ç½®è°ƒæ•´"""
        try:
            # å¹¶å‘ä¸é‡å¤å†™å…¥ä¿æŠ¤ï¼šåŒä¸€signatureåœ¨2ç§’å†…åªè®°å½•ä¸€æ¬¡
            signature_source = {
                'orig': original_config,
                'adj': adjusted_config,
                'metrics_epoch': training_metrics.get('epoch'),
            }
            signature_str = json.dumps(signature_source, ensure_ascii=False, sort_keys=True)
            import hashlib, time as _t
            signature = hashlib.md5(signature_str.encode('utf-8')).hexdigest()
            now = _t.time()
            with self._adjustment_lock:
                if self._last_adjustment_signature == signature and (now - self._last_adjustment_time) < 2.0:
                    print("[INFO] æ£€æµ‹åˆ°é‡å¤çš„é…ç½®è°ƒæ•´è®°å½•ï¼Œå·²å»é‡è·³è¿‡")
                    return
                self._last_adjustment_signature = signature
                self._last_adjustment_time = now

            # è®¡ç®—å˜æ›´
            changes = {}
            for key, value in adjusted_config.items():
                if key in original_config and original_config[key] != value:
                    changes[key] = {
                        'from': original_config[key],
                        'to': value
                    }
            
            if not changes:
                return  # æ²¡æœ‰å˜æ›´ï¼Œä¸è®°å½•
            print(f"[DEBUG] è®°å½•é…ç½®è°ƒæ•´ | epoch={training_metrics.get('epoch')} | changes_keys={list(changes.keys())}")
            # åˆ›å»ºè°ƒæ•´è®°å½•
            adjustment = ConfigAdjustment(
                adjustment_id=f"adj_{int(time.time())}",
                timestamp=time.time(),
                original_config=copy.deepcopy(original_config),
                adjusted_config=copy.deepcopy(adjusted_config),
                changes=changes,
                reason=analysis_result.get('reason', 'æ™ºèƒ½ä¼˜åŒ–å»ºè®®'),
                training_metrics=copy.deepcopy(training_metrics),
                llm_analysis=copy.deepcopy(analysis_result),
                status='pending'
            )
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.adjustment_history.append(adjustment)
            if self.current_session:
                self.current_session.adjustments.append(adjustment)
            
            # ç”Ÿæˆå‚æ•°å¾®è°ƒæŠ¥å‘Š
            if self.report_generator:
                try:
                    report_path = self.report_generator.generate_report(
                        original_config=adjustment.original_config,
                        adjusted_config=adjustment.adjusted_config,
                        changes=adjustment.changes,
                        llm_analysis=adjustment.llm_analysis,
                        training_metrics=adjustment.training_metrics,
                        reason=adjustment.reason,
                        session_id=self.current_session.session_id if self.current_session else "",
                        adjustment_id=adjustment.adjustment_id
                    )
                    
                    if report_path:
                        print(f"[INFO] å‚æ•°å¾®è°ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
                        self.status_updated.emit(f"ğŸ“„ å‚æ•°å¾®è°ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {os.path.basename(report_path)}")
                    else:
                        print(f"[WARNING] å‚æ•°å¾®è°ƒæŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                        
                except Exception as e:
                    print(f"[ERROR] ç”Ÿæˆå‚æ•°å¾®è°ƒæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    self.error_occurred.emit(f"ç”Ÿæˆå‚æ•°å¾®è°ƒæŠ¥å‘Šå¤±è´¥: {str(e)}")
            
            # å‘å°„ä¿¡å·
            self.adjustment_recorded.emit(asdict(adjustment))
            
        except Exception as e:
            self.error_occurred.emit(f"è®°å½•é…ç½®è°ƒæ•´å¤±è´¥: {str(e)}")
    
    def apply_config_to_training_system(self, 
                                      config: Dict[str, Any],
                                      training_tab) -> bool:
        """å°†é…ç½®åº”ç”¨åˆ°è®­ç»ƒç³»ç»Ÿ"""
        try:
            self.status_updated.emit("æ­£åœ¨åº”ç”¨é…ç½®åˆ°è®­ç»ƒç³»ç»Ÿ...")
            
            # ä½¿ç”¨ç°æœ‰çš„ConfigApplieråº”ç”¨é…ç½®
            from ..ui.components.training.config_applier import ConfigApplier
            success = ConfigApplier.apply_to_training_tab(config, training_tab)
            
            if success:
                self.status_updated.emit("é…ç½®åº”ç”¨æˆåŠŸ")
                self.config_applied.emit({
                    'config': config,
                    'timestamp': time.time(),
                    'success': True
                })
            else:
                self.error_occurred.emit("é…ç½®åº”ç”¨å¤±è´¥")
                self.config_applied.emit({
                    'config': config,
                    'timestamp': time.time(),
                    'success': False
                })
            
            return success
            
        except Exception as e:
            self.error_occurred.emit(f"åº”ç”¨é…ç½®åˆ°è®­ç»ƒç³»ç»Ÿå¤±è´¥: {str(e)}")
            return False
    
    def get_adjustment_history(self) -> List[Dict[str, Any]]:
        """è·å–è°ƒæ•´å†å²"""
        return [asdict(adj) for adj in self.adjustment_history]
    
    def get_current_session_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ä¼šè¯ä¿¡æ¯"""
        if not self.current_session:
            return {}
        
        return {
            'session_id': self.current_session.session_id,
            'start_time': self.current_session.start_time,
            'status': self.current_session.status,
            'adjustment_count': len(self.current_session.adjustments),
            'original_config': self.current_session.original_config,
            'final_config': self.current_session.final_config
        }
    
    def generate_iteration_summary_report(self, iteration: int, metrics: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¿­ä»£æ€»ç»“æŠ¥å‘Š"""
        try:
            import os
            import json
            from datetime import datetime
            
            # åˆ›å»ºæŠ¥å‘Šç›®å½•
            report_dir = "reports/parameter_tuning"
            os.makedirs(report_dir, exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"iteration_summary_{iteration}_{timestamp}.json"
            report_path = os.path.join(report_dir, report_filename)
            
            # æ„å»ºæŠ¥å‘Šå†…å®¹
            report_data = {
                'iteration': iteration,
                'timestamp': timestamp,
                'metrics': metrics,
                'session_info': self.get_current_session_info(),
                'adjustment_history': self.get_adjustment_history(),
                'report_type': 'iteration_summary'
            }
            
            # ä¿å­˜æŠ¥å‘Š
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.status_updated.emit(f"è¿­ä»£{iteration}æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}")
            return report_path
            
        except Exception as e:
            self.error_occurred.emit(f"ç”Ÿæˆè¿­ä»£æ€»ç»“æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return None
    
    def export_adjustment_report(self) -> Dict[str, Any]:
        """å¯¼å‡ºè°ƒæ•´æŠ¥å‘Š"""
        return {
            'export_time': time.time(),
            'current_session': self.get_current_session_info(),
            'adjustment_history': self.get_adjustment_history(),
            'parameter_constraints': self.parameter_constraints
        }
    
    def stop_training_session(self):
        """åœæ­¢è®­ç»ƒä¼šè¯"""
        if self.current_session:
            self.current_session.status = 'completed'
            self.current_session.final_config = self.current_session.adjustments[-1].adjusted_config if self.current_session.adjustments else self.current_session.original_config
            self.status_updated.emit(f"è®­ç»ƒä¼šè¯ {self.current_session.session_id} å·²åœæ­¢")