import os
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import json
import shutil
from PyQt5.QtCore import QObject, pyqtSignal, QThread
from typing import Dict, Tuple, List, Optional
import time
from src.utils.logger import get_logger, log_error, performance_monitor, PerformanceMonitor

class BatchPredictionThread(QThread):
    """æ‰¹é‡é¢„æµ‹ç‹¬ç«‹çº¿ç¨‹"""
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int)
    status_updated = pyqtSignal(str)
    prediction_finished = pyqtSignal(dict)
    prediction_error = pyqtSignal(str)
    
    def __init__(self, predictor, params):
        super().__init__()
        self.predictor = predictor
        self.params = params
        self._stop_processing = False
        self.logger = get_logger(__name__, "batch_prediction")
        
    def run(self):
        """çº¿ç¨‹è¿è¡Œå…¥å£"""
        try:
            with PerformanceMonitor("batch_prediction_full", "batch_prediction"):
                self._batch_predict()
        except Exception as e:
            import traceback
            error_msg = f"æ‰¹é‡é¢„æµ‹çº¿ç¨‹å‡ºé”™: {str(e)}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            log_error(e, {"operation": "batch_prediction_thread"}, "batch_prediction")
            self.prediction_error.emit(error_msg)
    
    def stop_processing(self):
        """åœæ­¢å¤„ç†"""
        self._stop_processing = True
        
    def _batch_predict(self):
        """æ‰§è¡Œæ‰¹é‡é¢„æµ‹"""
        batch_start_time = time.time()
        print("=" * 60)
        print("å¼€å§‹æ‰¹é‡é¢„æµ‹ï¼ˆç‹¬ç«‹çº¿ç¨‹ï¼‰")
        print(f"æ‰¹é‡é¢„æµ‹å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            if self.predictor.model is None:
                print("âŒ é”™è¯¯: æ¨¡å‹æœªåŠ è½½")
                self.prediction_error.emit('è¯·å…ˆåŠ è½½æ¨¡å‹')
                return
            else:
                print(f"âœ… æ¨¡å‹å·²åŠ è½½: {type(self.predictor.model).__name__}")
                print(f"âœ… è®¾å¤‡: {self.predictor.device}")
                print(f"âœ… æ¨¡å‹çŠ¶æ€: {'è®­ç»ƒæ¨¡å¼' if self.predictor.model.training else 'è¯„ä¼°æ¨¡å¼'}")
            
            if self.predictor.class_names is None or len(self.predictor.class_names) == 0:
                print("âŒ é”™è¯¯: ç±»åˆ«ä¿¡æ¯æœªåŠ è½½")
                self.prediction_error.emit('ç±»åˆ«ä¿¡æ¯æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹')
                return
            else:
                print(f"âœ… ç±»åˆ«ä¿¡æ¯å·²åŠ è½½: {len(self.predictor.class_names)} ä¸ªç±»åˆ«")
                print(f"   ç±»åˆ«åˆ—è¡¨: {self.predictor.class_names}")
                
            source_folder = self.params.get('source_folder')
            target_folder = self.params.get('target_folder')
            
            print(f"ğŸ“ æºæ–‡ä»¶å¤¹: {source_folder}")
            print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶å¤¹: {target_folder}")
            
            # éªŒè¯å¿…è¦å‚æ•°
            if not source_folder:
                self.prediction_error.emit('æºæ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º')
                return
            
            if not target_folder:
                self.prediction_error.emit('ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º')
                return
            
            if not os.path.exists(source_folder):
                self.prediction_error.emit(f'æºæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {source_folder}')
                return
            
            if not os.path.isdir(source_folder):
                self.prediction_error.emit(f'æºè·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {source_folder}')
                return
            
            confidence_threshold = self.params.get('confidence_threshold', 50.0)  # é»˜è®¤50%
            copy_mode = self.params.get('copy_mode', 'copy')
            create_subfolders = self.params.get('create_subfolders', True)
            
            print(f"âš™ï¸ ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}%")
            print(f"âš™ï¸ æ–‡ä»¶æ“ä½œæ¨¡å¼: {copy_mode}")
            print(f"âš™ï¸ åˆ›å»ºå­æ–‡ä»¶å¤¹: {create_subfolders}")
            
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']
            image_files = [f for f in os.listdir(source_folder) 
                          if os.path.isfile(os.path.join(source_folder, f)) and 
                          os.path.splitext(f.lower())[1] in valid_extensions]
            
            if not image_files:
                print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                self.status_updated.emit('æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶')
                return
            
            print(f"ğŸ“· æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
            print(f"   å›¾ç‰‡æ ¼å¼ç»Ÿè®¡: {dict((ext, sum(1 for f in image_files if f.lower().endswith(ext))) for ext in valid_extensions if any(f.lower().endswith(ext) for f in image_files))}")
                
            # åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹
            os.makedirs(target_folder, exist_ok=True)
            
            # å¦‚æœéœ€è¦åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºä¸€ä¸ªå­æ–‡ä»¶å¤¹
            if create_subfolders:
                for class_name in self.predictor.class_names:
                    os.makedirs(os.path.join(target_folder, class_name), exist_ok=True)
                print(f"ğŸ“ å·²åˆ›å»º {len(self.predictor.class_names)} ä¸ªç±»åˆ«å­æ–‡ä»¶å¤¹")
            
            # ç»Ÿè®¡ç»“æœ
            results = {
                'total': len(image_files),
                'processed': 0,
                'classified': 0,
                'unclassified': 0,
                'class_counts': {class_name: 0 for class_name in self.predictor.class_names}
            }
            
            prediction_times = []
            
            # æ‰¹é‡å¤„ç†å›¾ç‰‡
            print("\nğŸ”„ å¼€å§‹å¤„ç†å›¾ç‰‡...")
            for i, image_file in enumerate(image_files):
                if self._stop_processing:
                    print("â¹ï¸ æ‰¹é‡å¤„ç†å·²åœæ­¢")
                    self.status_updated.emit('æ‰¹é‡å¤„ç†å·²åœæ­¢')
                    break
                    
                image_path = os.path.join(source_folder, image_file)
                
                # è®°å½•å•å¼ å›¾ç‰‡é¢„æµ‹æ—¶é—´
                single_start = time.time()
                result = self.predictor.predict_image(image_path)
                single_time = time.time() - single_start
                prediction_times.append(single_time)
                
                if result:
                    # è·å–æœ€é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹
                    top_prediction = result['predictions'][0]
                    class_name = top_prediction['class_name']
                    probability = top_prediction['probability']
                    
                    # æ›´æ–°è¿›åº¦
                    progress = int((i + 1) / len(image_files) * 100)
                    self.progress_updated.emit(progress)
                    self.status_updated.emit(f'å¤„ç†å›¾ç‰‡ {i+1}/{len(image_files)}: {image_file}')
                    
                    results['processed'] += 1
                    
                    # å¦‚æœç½®ä¿¡åº¦é«˜äºé˜ˆå€¼ï¼Œåˆ™åˆ†ç±»å›¾ç‰‡
                    print(f"ğŸ” ç½®ä¿¡åº¦æ¯”è¾ƒ: {image_file}")
                    print(f"   é¢„æµ‹ç½®ä¿¡åº¦: {probability:.2f}%")
                    print(f"   è®¾å®šé˜ˆå€¼: {confidence_threshold}%")
                    print(f"   æ¯”è¾ƒç»“æœ: {probability:.2f}% {'â‰¥' if probability >= confidence_threshold else '<'} {confidence_threshold}%")
                    
                    if probability >= confidence_threshold:
                        print(f"   âœ… ç½®ä¿¡åº¦è¾¾æ ‡ï¼Œå°†åˆ†ç±»åˆ°: {class_name}")
                        # ç¡®å®šç›®æ ‡è·¯å¾„
                        if create_subfolders:
                            target_path = os.path.join(target_folder, class_name, image_file)
                        else:
                            # å¦‚æœä¸åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œåˆ™åœ¨æ–‡ä»¶åå‰æ·»åŠ ç±»åˆ«åç§°
                            base_name, ext = os.path.splitext(image_file)
                            target_path = os.path.join(target_folder, f"{class_name}_{base_name}{ext}")
                        
                        # å¤åˆ¶æˆ–ç§»åŠ¨æ–‡ä»¶
                        try:
                            if copy_mode == 'copy':
                                shutil.copy2(image_path, target_path)
                            else:  # move
                                shutil.move(image_path, target_path)
                                
                            results['classified'] += 1
                            results['class_counts'][class_name] += 1
                            print(f"   âœ… æ–‡ä»¶å·²{('å¤åˆ¶' if copy_mode == 'copy' else 'ç§»åŠ¨')}åˆ°: {target_path}")
                        except Exception as e:
                            print(f"âŒ å¤„ç†æ–‡ä»¶ {image_file} æ—¶å‡ºé”™: {str(e)}")
                            self.status_updated.emit(f'å¤„ç†æ–‡ä»¶ {image_file} æ—¶å‡ºé”™: {str(e)}')
                    else:
                        results['unclassified'] += 1
                        print(f"   âŒ ç½®ä¿¡åº¦ä¸è¾¾æ ‡ï¼Œæœªåˆ†ç±»")
                        print(f"âš ï¸ å›¾ç‰‡ {image_file} ç½®ä¿¡åº¦è¿‡ä½ ({probability:.2f}% < {confidence_threshold}%)ï¼Œæœªåˆ†ç±»")
                else:
                    print(f"âŒ å›¾ç‰‡ {image_file} é¢„æµ‹å¤±è´¥")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            batch_total_time = time.time() - batch_start_time
            avg_prediction_time = sum(prediction_times) / len(prediction_times) if prediction_times else 0
            
            print("\n" + "=" * 60)
            print("æ‰¹é‡é¢„æµ‹å®Œæˆç»Ÿè®¡:")
            print(f"â±ï¸ æ€»è€—æ—¶: {batch_total_time:.2f}ç§’")
            print(f"â±ï¸ å¹³å‡æ¯å¼ å›¾ç‰‡é¢„æµ‹æ—¶é—´: {avg_prediction_time:.4f}ç§’")
            print(f"ğŸƒ é¢„æµ‹é€Ÿåº¦: {len(image_files)/batch_total_time:.2f} å¼ /ç§’")
            print(f"ğŸ“Š æ€»å›¾ç‰‡æ•°: {results['total']}")
            print(f"ğŸ“Š å·²å¤„ç†: {results['processed']}")
            print(f"ğŸ“Š å·²åˆ†ç±»: {results['classified']}")
            print(f"ğŸ“Š æœªåˆ†ç±»: {results['unclassified']}")
            print("ğŸ“Š å„ç±»åˆ«ç»Ÿè®¡:")
            for class_name, count in results['class_counts'].items():
                if count > 0:
                    print(f"   {class_name}: {count} å¼ ")
            print("=" * 60)
            
            # å‘é€å®Œæˆä¿¡å·
            self.prediction_finished.emit(results)
            self.status_updated.emit('æ‰¹é‡å¤„ç†å®Œæˆ')
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            self.prediction_error.emit(f'æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}')

class Predictor(QObject):
    # å®šä¹‰ä¿¡å·
    prediction_finished = pyqtSignal(dict)
    prediction_error = pyqtSignal(str)
    batch_prediction_progress = pyqtSignal(int)
    batch_prediction_status = pyqtSignal(str)
    batch_prediction_finished = pyqtSignal(dict)

    def __init__(self):
        super().__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.class_names = None
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        self._stop_batch_processing = False
        self.batch_prediction_thread = None  # æ·»åŠ çº¿ç¨‹å¼•ç”¨
        self.logger = get_logger(__name__, "predictor")
        
        self.logger.info(f"é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    @performance_monitor("load_model", "predictor")
    def load_model(self, model_path: str, class_info_path: str) -> None:
        """
        åŠ è½½æ¨¡å‹å’Œç±»åˆ«ä¿¡æ¯
        """
        try:
            self.logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_path}")
            self.logger.info(f"ç±»åˆ«ä¿¡æ¯æ–‡ä»¶: {class_info_path}")
            
            # åŠ è½½ç±»åˆ«ä¿¡æ¯
            with open(class_info_path, 'r', encoding='utf-8') as f:
                class_info = json.load(f)
            self.class_names = class_info['class_names']
            self.logger.info(f"ç±»åˆ«ä¿¡æ¯åŠ è½½æˆåŠŸï¼Œç±»åˆ«æ•°é‡: {len(self.class_names)}")
            self.logger.debug(f"ç±»åˆ«åˆ—è¡¨: {self.class_names}")

            # 1. å°è¯•ç›´æ¥åˆ›å»ºResNetæ¨¡å‹å¹¶åŠ è½½æƒé‡
            try:
                num_classes = len(self.class_names)
                # å°è¯•å¤šç§å¸¸è§çš„æ¨¡å‹æ¶æ„
                models_to_try = ['resnet18', 'resnet34', 'resnet50', 'resnet101']
                
                for model_arch in models_to_try:
                    print(f"å°è¯•åŠ è½½æ¨¡å‹æ¶æ„: {model_arch}")
                    # å¯¼å…¥å¯¹åº”çš„æ¨¡å‹æ„å»ºå‡½æ•°
                    if model_arch == 'resnet18':
                        from torchvision.models import resnet18
                        self.model = resnet18(pretrained=False)
                    elif model_arch == 'resnet34':
                        from torchvision.models import resnet34
                        self.model = resnet34(pretrained=False)
                    elif model_arch == 'resnet50':
                        from torchvision.models import resnet50
                        self.model = resnet50(pretrained=False)
                    elif model_arch == 'resnet101':
                        from torchvision.models import resnet101
                        self.model = resnet101(pretrained=False)
                    
                    # ä¿®æ”¹æœ€åä¸€å±‚ä»¥åŒ¹é…ç±»åˆ«æ•°
                    in_features = self.model.fc.in_features
                    self.model.fc = nn.Linear(in_features, num_classes)
                    
                    # åŠ è½½æƒé‡
                    state_dict = torch.load(model_path, map_location=self.device)
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„æƒé‡æ ¼å¼
                    if isinstance(state_dict, dict):
                        if 'model_state_dict' in state_dict:
                            state_dict = state_dict['model_state_dict']
                        elif 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                    
                    # å°è¯•åŠ è½½æƒé‡
                    try:
                        self.model.load_state_dict(state_dict, strict=False)
                        print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_arch}")
                        break  # å¦‚æœåŠ è½½æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
                    except Exception as e:
                        print(f"åŠ è½½æ¨¡å‹ {model_arch} å¤±è´¥: {str(e)}")
                        continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹æ¶æ„
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
                if self.model is None:
                    raise Exception("æ‰€æœ‰æ¨¡å‹æ¶æ„å°è¯•å‡å¤±è´¥")
                
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                self.model.to(self.device)
                self.model.eval()
                print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç±»åˆ«æ•°é‡: {len(self.class_names)}")
                
            except Exception as arch_error:
                print(f"å°è¯•åŠ è½½æ¨¡å‹æ¶æ„å¤±è´¥: {str(arch_error)}")
                raise arch_error

        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            error_msg = f'åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}\n{traceback_str}'
            print(error_msg)
            self.prediction_error.emit(error_msg)

    def predict(self, image_path: str, top_k: int = 3) -> None:
        """
        é¢„æµ‹å•å¼ å›¾ç‰‡ç±»åˆ«
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            top_k: è¿”å›å‰kä¸ªé¢„æµ‹ç»“æœ
        """
        try:
            if self.model is None:
                self.prediction_error.emit("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹")
                return
                
            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            try:
                image = Image.open(image_path).convert('RGB')
            except Exception as img_err:
                self.prediction_error.emit(f"æ— æ³•åŠ è½½å›¾åƒ: {str(img_err)}")
                return
                
            image_tensor = self.transform(image).unsqueeze(0).to(self.device)

            # é¢„æµ‹
            try:
                with torch.no_grad():
                    outputs = self.model(image_tensor)
                    # æ£€æŸ¥è¾“å‡ºæ ¼å¼
                    if isinstance(outputs, tuple):
                        # æœ‰äº›æ¨¡å‹è¿”å›å¤šä¸ªè¾“å‡ºï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ª
                        outputs = outputs[0]
                    
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    top_k = min(top_k, len(self.class_names), probabilities.size(1))
                    top_prob, top_class = torch.topk(probabilities, top_k)
            except Exception as pred_err:
                self.prediction_error.emit(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {str(pred_err)}")
                return

            # è·å–é¢„æµ‹ç»“æœ
            predictions = []
            for i in range(top_k):
                try:
                    class_idx = top_class[0][i].item()
                    if 0 <= class_idx < len(self.class_names):
                        prob = top_prob[0][i].item()
                        predictions.append({
                            'class_name': self.class_names[class_idx],
                            'probability': prob * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        })
                except Exception as idx_err:
                    print(f"å¤„ç†é¢„æµ‹ç»“æœ {i} æ—¶å‡ºé”™: {str(idx_err)}")
                    continue

            if not predictions:
                self.prediction_error.emit("æ— æ³•è·å–æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
                return
                
            # å‘é€é¢„æµ‹ç»“æœ
            result = {
                'predictions': predictions,
                'image_path': image_path
            }
            self.prediction_finished.emit(result)

        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            self.prediction_error.emit(f'é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}\n{traceback_str}')

    def predict_image(self, image_path: str, top_k: int = 3) -> Optional[Dict]:
        """
        é¢„æµ‹å•å¼ å›¾ç‰‡ç±»åˆ«å¹¶è¿”å›ç»“æœï¼ˆä¸å‘é€ä¿¡å·ï¼‰
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            top_k: è¿”å›å‰kä¸ªé¢„æµ‹ç»“æœ
        """
        start_time = time.time()
        
        try:
            if self.model is None:
                print("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹")
                return None
                
            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            try:
                load_start = time.time()
                image = Image.open(image_path).convert('RGB')
                load_time = time.time() - load_start
                print(f"å›¾ç‰‡åŠ è½½æ—¶é—´: {load_time:.4f}ç§’")
            except Exception as img_err:
                print(f"æ— æ³•åŠ è½½å›¾åƒ: {str(img_err)}")
                return None
                
            preprocess_start = time.time()
            image_tensor = self.transform(image).unsqueeze(0).to(self.device)
            preprocess_time = time.time() - preprocess_start
            print(f"å›¾ç‰‡é¢„å¤„ç†æ—¶é—´: {preprocess_time:.4f}ç§’")

            # é¢„æµ‹
            try:
                predict_start = time.time()
                with torch.no_grad():
                    outputs = self.model(image_tensor)
                    # æ£€æŸ¥è¾“å‡ºæ ¼å¼
                    if isinstance(outputs, tuple):
                        # æœ‰äº›æ¨¡å‹è¿”å›å¤šä¸ªè¾“å‡ºï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ª
                        outputs = outputs[0]
                    
                    probabilities = torch.nn.functional.softmax(outputs, dim=1)
                    top_k = min(top_k, len(self.class_names), probabilities.size(1))
                    top_prob, top_class = torch.topk(probabilities, top_k)
                predict_time = time.time() - predict_start
                print(f"æ¨¡å‹æ¨ç†æ—¶é—´: {predict_time:.4f}ç§’")
                print(f"æ¨¡å‹è¾“å‡ºshape: {outputs.shape}")
                print(f"æ¦‚ç‡åˆ†å¸ƒå‰3ä¸ªå€¼: {probabilities[0][:3].tolist()}")
            except Exception as pred_err:
                print(f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {str(pred_err)}")
                return None

            # è·å–é¢„æµ‹ç»“æœ
            predictions = []
            for i in range(top_k):
                try:
                    class_idx = top_class[0][i].item()
                    if 0 <= class_idx < len(self.class_names):
                        prob = top_prob[0][i].item()
                        predictions.append({
                            'class_name': self.class_names[class_idx],
                            'probability': prob * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        })
                        print(f"é¢„æµ‹ç»“æœ {i+1}: {self.class_names[class_idx]} - {prob*100:.2f}%")
                except Exception as idx_err:
                    print(f"å¤„ç†é¢„æµ‹ç»“æœ {i} æ—¶å‡ºé”™: {str(idx_err)}")
                    continue

            if not predictions:
                print("æ— æ³•è·å–æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
                return None
                
            total_time = time.time() - start_time
            print(f"å•å¼ å›¾ç‰‡é¢„æµ‹æ€»æ—¶é—´: {total_time:.4f}ç§’")
            print(f"å›¾ç‰‡: {os.path.basename(image_path)}, æœ€é«˜ç½®ä¿¡åº¦: {predictions[0]['class_name']} ({predictions[0]['probability']:.2f}%)")
                
            return {
                'predictions': predictions,
                'image_path': image_path
            }

        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            print(f'é¢„æµ‹å›¾ç‰‡ {image_path} æ—¶å‡ºé”™: {str(e)}\n{traceback_str}')
            return None

    def batch_predict(self, params: Dict) -> None:
        """
        å¯åŠ¨æ‰¹é‡é¢„æµ‹çº¿ç¨‹
        
        å‚æ•°:
            params: åŒ…å«æ‰¹é‡é¢„æµ‹å‚æ•°çš„å­—å…¸
                - source_folder: æºå›¾ç‰‡æ–‡ä»¶å¤¹
                - target_folder: ç›®æ ‡æ–‡ä»¶å¤¹ï¼ˆåˆ†ç±»åçš„å›¾ç‰‡å°†ä¿å­˜åœ¨è¿™é‡Œï¼‰
                - confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œåªæœ‰é«˜äºæ­¤é˜ˆå€¼çš„é¢„æµ‹æ‰ä¼šè¢«æ¥å—
                - copy_mode: 'copy'ï¼ˆå¤åˆ¶ï¼‰æˆ– 'move'ï¼ˆç§»åŠ¨ï¼‰
                - create_subfolders: æ˜¯å¦ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºå­æ–‡ä»¶å¤¹
        """
        # å¦‚æœå·²æœ‰çº¿ç¨‹åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å®ƒ
        if self.batch_prediction_thread and self.batch_prediction_thread.isRunning():
            print("âš ï¸ å·²æœ‰æ‰¹é‡é¢„æµ‹çº¿ç¨‹åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å®ƒ")
            self.stop_batch_processing()
            self.batch_prediction_thread.wait()  # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        
        # åˆ›å»ºæ–°çš„æ‰¹é‡é¢„æµ‹çº¿ç¨‹
        self.batch_prediction_thread = BatchPredictionThread(self, params)
            
        # è¿æ¥çº¿ç¨‹ä¿¡å·åˆ°Predictorçš„ä¿¡å·
        self.batch_prediction_thread.progress_updated.connect(self.batch_prediction_progress.emit)
        self.batch_prediction_thread.status_updated.connect(self.batch_prediction_status.emit)
        self.batch_prediction_thread.prediction_finished.connect(self.batch_prediction_finished.emit)
        self.batch_prediction_thread.prediction_error.connect(self.prediction_error.emit)
            
        # å¯åŠ¨çº¿ç¨‹
        print("ğŸš€ å¯åŠ¨æ‰¹é‡é¢„æµ‹ç‹¬ç«‹çº¿ç¨‹")
        self.batch_prediction_thread.start()

    def stop_batch_processing(self) -> None:
        """åœæ­¢æ‰¹é‡å¤„ç†"""
        self._stop_batch_processing = True
        
        # å¦‚æœæœ‰è¿è¡Œä¸­çš„æ‰¹é‡é¢„æµ‹çº¿ç¨‹ï¼Œåœæ­¢å®ƒ
        if self.batch_prediction_thread and self.batch_prediction_thread.isRunning():
            print("ğŸ›‘ æ­£åœ¨åœæ­¢æ‰¹é‡é¢„æµ‹çº¿ç¨‹...")
            self.batch_prediction_thread.stop_processing()
            # ä¸åœ¨è¿™é‡Œwait()ï¼Œè®©è°ƒç”¨è€…å†³å®šæ˜¯å¦ç­‰å¾…
    
    def is_batch_prediction_running(self) -> bool:
        """æ£€æŸ¥æ‰¹é‡é¢„æµ‹çº¿ç¨‹æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return self.batch_prediction_thread is not None and self.batch_prediction_thread.isRunning()
    
    def wait_for_batch_prediction_to_finish(self, timeout_ms: int = 5000) -> bool:
        """ç­‰å¾…æ‰¹é‡é¢„æµ‹çº¿ç¨‹å®Œæˆ
        
        Args:
            timeout_ms: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            bool: Trueè¡¨ç¤ºçº¿ç¨‹æ­£å¸¸ç»“æŸï¼ŒFalseè¡¨ç¤ºè¶…æ—¶
        """
        if self.batch_prediction_thread and self.batch_prediction_thread.isRunning():
            return self.batch_prediction_thread.wait(timeout_ms)
        return True
    
    def cleanup_batch_prediction_thread(self):
        """æ¸…ç†æ‰¹é‡é¢„æµ‹çº¿ç¨‹"""
        if self.batch_prediction_thread:
            if self.batch_prediction_thread.isRunning():
                self.batch_prediction_thread.stop_processing()
                self.batch_prediction_thread.wait(3000)  # ç­‰å¾…3ç§’
            self.batch_prediction_thread.deleteLater()
            self.batch_prediction_thread = None

    def _create_model(self, model_name: str, num_classes: int) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        if model_name == 'ResNet50':
            from torchvision.models import resnet50
            model = resnet50(pretrained=False)  # ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            model.fc = nn.Linear(model.fc.in_features, num_classes)
        else:
            raise ValueError(f'ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}')
        
        return model 

    def load_model_with_info(self, model_info: Dict) -> None:
        """
        æ ¹æ®æ¨¡å‹ä¿¡æ¯åŠ è½½æ¨¡å‹å’Œç±»åˆ«ä¿¡æ¯
        
        å‚æ•°:
            model_info: åŒ…å«æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
                - model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
                - class_info_path: ç±»åˆ«ä¿¡æ¯æ–‡ä»¶è·¯å¾„
                - model_type: æ¨¡å‹ç±»å‹ï¼ˆåˆ†ç±»æ¨¡å‹æˆ–æ£€æµ‹æ¨¡å‹ï¼‰
                - model_arch: æ¨¡å‹æ¶æ„ï¼ˆResNet18, ResNet34ç­‰ï¼‰
        """
        try:
            model_path = model_info.get('model_path')
            class_info_path = model_info.get('class_info_path')
            model_type = model_info.get('model_type')
            model_arch = model_info.get('model_arch')
            
            print(f"åŠ è½½æ¨¡å‹ä¿¡æ¯: {model_type} - {model_arch}")
            print(f"æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"ç±»åˆ«ä¿¡æ¯è·¯å¾„: {class_info_path}")
            
            # åŠ è½½ç±»åˆ«ä¿¡æ¯
            with open(class_info_path, 'r', encoding='utf-8') as f:
                class_info = json.load(f)
            self.class_names = class_info['class_names']
            print(f"ç±»åˆ«ä¿¡æ¯åŠ è½½æˆåŠŸï¼Œç±»åˆ«: {self.class_names}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹å’Œæ¶æ„åŠ è½½ä¸åŒçš„æ¨¡å‹
            if model_type == "åˆ†ç±»æ¨¡å‹":
                self._load_classification_model(model_path, model_arch)
            elif model_type == "æ£€æµ‹æ¨¡å‹":
                self._load_detection_model(model_path, model_arch)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_type} - {model_arch}")
            
        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            error_msg = f'åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}\n{traceback_str}'
            print(error_msg)
            self.prediction_error.emit(error_msg)
            
    def _load_classification_model(self, model_path: str, model_arch: str) -> None:
        """åŠ è½½åˆ†ç±»æ¨¡å‹"""
        try:
            num_classes = len(self.class_names)
            
            # æ ¹æ®æ¶æ„åˆ›å»ºå¯¹åº”çš„æ¨¡å‹
            if model_arch in ["ResNet18", "ResNet34", "ResNet50", "ResNet101", "ResNet152"]:
                # ResNetç³»åˆ—æ¨¡å‹
                if model_arch == "ResNet18":
                    from torchvision.models import resnet18
                    self.model = resnet18(pretrained=False)
                    in_features = self.model.fc.in_features
                elif model_arch == "ResNet34":
                    from torchvision.models import resnet34
                    self.model = resnet34(pretrained=False)
                    in_features = self.model.fc.in_features
                elif model_arch == "ResNet50":
                    from torchvision.models import resnet50
                    self.model = resnet50(pretrained=False)
                    in_features = self.model.fc.in_features
                elif model_arch == "ResNet101":
                    from torchvision.models import resnet101
                    self.model = resnet101(pretrained=False)
                    in_features = self.model.fc.in_features
                elif model_arch == "ResNet152":
                    from torchvision.models import resnet152
                    self.model = resnet152(pretrained=False)
                    in_features = self.model.fc.in_features
                
                # ä¿®æ”¹åˆ†ç±»å¤´
                self.model.fc = nn.Linear(in_features, num_classes)
            
            elif model_arch in ["MobileNetV2", "MobileNetV3", "MobileNetV3Small", "MobileNetV3Large"]:
                # MobileNetç³»åˆ—
                if model_arch == "MobileNetV2":
                    from torchvision.models import mobilenet_v2
                    self.model = mobilenet_v2(pretrained=False)
                    in_features = self.model.classifier[1].in_features
                    self.model.classifier[1] = nn.Linear(in_features, num_classes)
                elif model_arch == "MobileNetV3" or model_arch == "MobileNetV3Large":
                    from torchvision.models import mobilenet_v3_large
                    self.model = mobilenet_v3_large(pretrained=False)
                    in_features = self.model.classifier[3].in_features
                    self.model.classifier[3] = nn.Linear(in_features, num_classes)
                elif model_arch == "MobileNetV3Small":
                    from torchvision.models import mobilenet_v3_small
                    self.model = mobilenet_v3_small(pretrained=False)
                    in_features = self.model.classifier[3].in_features
                    self.model.classifier[3] = nn.Linear(in_features, num_classes)
            
            elif model_arch.startswith("EfficientNet"):
                # EfficientNetç³»åˆ—
                if model_arch == "EfficientNetB0":
                    from torchvision.models import efficientnet_b0
                    self.model = efficientnet_b0(pretrained=False)
                elif model_arch == "EfficientNetB1":
                    from torchvision.models import efficientnet_b1
                    self.model = efficientnet_b1(pretrained=False)
                elif model_arch == "EfficientNetB2":
                    from torchvision.models import efficientnet_b2
                    self.model = efficientnet_b2(pretrained=False)
                elif model_arch == "EfficientNetB3":
                    from torchvision.models import efficientnet_b3
                    self.model = efficientnet_b3(pretrained=False)
                elif model_arch == "EfficientNetB4":
                    from torchvision.models import efficientnet_b4
                    self.model = efficientnet_b4(pretrained=False)
                
                # ä¿®æ”¹åˆ†ç±»å¤´
                in_features = self.model.classifier[1].in_features
                self.model.classifier[1] = nn.Linear(in_features, num_classes)
            
            elif model_arch in ["VGG16", "VGG19"]:
                # VGGç³»åˆ—
                if model_arch == "VGG16":
                    from torchvision.models import vgg16
                    self.model = vgg16(pretrained=False)
                elif model_arch == "VGG19":
                    from torchvision.models import vgg19
                    self.model = vgg19(pretrained=False)
                
                # ä¿®æ”¹åˆ†ç±»å¤´
                in_features = self.model.classifier[6].in_features
                self.model.classifier[6] = nn.Linear(in_features, num_classes)
            
            elif model_arch.startswith("DenseNet"):
                # DenseNetç³»åˆ—
                if model_arch == "DenseNet121":
                    from torchvision.models import densenet121
                    self.model = densenet121(pretrained=False)
                elif model_arch == "DenseNet169":
                    from torchvision.models import densenet169
                    self.model = densenet169(pretrained=False)
                elif model_arch == "DenseNet201":
                    from torchvision.models import densenet201
                    self.model = densenet201(pretrained=False)
                    
                # ä¿®æ”¹åˆ†ç±»å¤´
                in_features = self.model.classifier.in_features
                self.model.classifier = nn.Linear(in_features, num_classes)
            
            elif model_arch in ["InceptionV3", "Xception"]:
                # å…¶ä»–å¤æ‚æ¨¡å‹
                print(f"æ³¨æ„: {model_arch}æ¨¡å‹éœ€è¦ç‰¹æ®Šçš„é¢„å¤„ç†ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´transform")
                if model_arch == "InceptionV3":
                    from torchvision.models import inception_v3
                    self.model = inception_v3(pretrained=False)
                    in_features = self.model.fc.in_features
                    self.model.fc = nn.Linear(in_features, num_classes)
                elif model_arch == "Xception":
                    # Xceptionéœ€è¦å¤–éƒ¨åº“æ”¯æŒ
                    try:
                        from pretrainedmodels import xception
                        self.model = xception(num_classes=1000, pretrained=None)
                        self.model.last_linear = nn.Linear(self.model.last_linear.in_features, num_classes)
                    except ImportError:
                        raise ImportError("åŠ è½½Xceptionæ¨¡å‹éœ€è¦å®‰è£…pretrainedmodelsåº“: pip install pretrainedmodels")
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»æ¨¡å‹æ¶æ„: {model_arch}")
                
            print(f"æˆåŠŸåˆ›å»ºæ¨¡å‹æ¶æ„: {model_arch}")
                
            # åŠ è½½æ¨¡å‹æƒé‡
            state_dict = torch.load(model_path, map_location=self.device)
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
            if isinstance(state_dict, dict):
                if 'model_state_dict' in state_dict:
                    state_dict = state_dict['model_state_dict']
                elif 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
            
            # åŠ è½½æƒé‡ï¼ˆä½¿ç”¨éä¸¥æ ¼æ¨¡å¼ï¼Œå…è®¸éƒ¨åˆ†æƒé‡ä¸åŒ¹é…ï¼‰
            self.model.load_state_dict(state_dict, strict=False)
            self.model.to(self.device)
            self.model.eval()
            
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            
        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            error_msg = f"åŠ è½½åˆ†ç±»æ¨¡å‹å¤±è´¥: {str(e)}\n{traceback_str}"
            print(error_msg)
            raise Exception(error_msg)
            
    def _load_detection_model(self, model_path: str, model_arch: str) -> None:
        """åŠ è½½æ£€æµ‹æ¨¡å‹"""
        try:
            # YOLOç³»åˆ—æ¨¡å‹
            if model_arch.startswith("YOLO"):
                if model_arch == "YOLOv5":
                    # å°è¯•å¯¼å…¥YOLOv5
                    try:
                        import yolov5
                        self.model = yolov5.load(model_path)
                        print(f"ä½¿ç”¨yolov5åº“åŠ è½½æ¨¡å‹: {model_path}")
                    except ImportError:
                        raise ImportError("éœ€è¦å®‰è£…YOLOv5åº“: pip install yolov5")
                elif model_arch == "YOLOv8":
                    # å°è¯•å¯¼å…¥Ultralytics YOLOv8
                    try:
                        from ultralytics import YOLO
                        self.model = YOLO(model_path)
                        print(f"ä½¿ç”¨ultralyticsåº“åŠ è½½æ¨¡å‹: {model_path}")
                    except ImportError:
                        raise ImportError("éœ€è¦å®‰è£…Ultralyticsåº“: pip install ultralytics")
                elif model_arch in ["YOLOv7", "YOLOv6", "YOLOv4", "YOLOv3"]:
                    # è¿™äº›YOLOç‰ˆæœ¬éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå¯èƒ½éœ€è¦å¯¼å…¥ä¸“é—¨çš„ä»£ç åº“
                    try:
                        if model_arch == "YOLOv7":
                            # YOLOv7é€šå¸¸éœ€è¦ç‰¹å®šçš„ä»£ç åº“å’Œæƒé‡æ ¼å¼
                            from models.experimental import attempt_load
                            self.model = attempt_load(model_path, map_location=self.device)
                            print(f"ä½¿ç”¨YOLOv7ä¸“ç”¨åŠ è½½å™¨åŠ è½½æ¨¡å‹: {model_path}")
                        elif model_arch == "YOLOv6":
                            # YOLOv6å¯èƒ½éœ€è¦å®‰è£…ç‰¹å®šç‰ˆæœ¬
                            from yolov6.core.inferer import Inferer
                            self.model = Inferer(model_path, device=self.device)
                            print(f"ä½¿ç”¨YOLOv6åº“åŠ è½½æ¨¡å‹: {model_path}")
                        elif model_arch in ["YOLOv4", "YOLOv3"]:
                            # YOLOv3/v4å¯èƒ½ä½¿ç”¨Darknetæ ¼å¼
                            import cv2
                            self.model = cv2.dnn.readNetFromDarknet(model_path, model_path.replace(".weights", ".cfg"))
                            self.model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
                            self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
                            print(f"ä½¿ç”¨OpenCV DNNåŠ è½½{model_arch}æ¨¡å‹: {model_path}")
                    except ImportError as e:
                        raise ImportError(f"åŠ è½½{model_arch}éœ€è¦å®‰è£…ç‰¹å®šåº“: {str(e)}")
                    except Exception as e:
                        raise Exception(f"åŠ è½½{model_arch}æ¨¡å‹å¤±è´¥: {str(e)}")
            
            # SSDç³»åˆ—æ¨¡å‹
            elif model_arch.startswith("SSD"):
                try:
                    import torch
                    # ä¸åŒSSDå˜ä½“çš„å¤„ç†
                    if model_arch == "SSD":
                        from torchvision.models.detection import ssd300_vgg16
                        self.model = ssd300_vgg16(pretrained=False, num_classes=len(self.class_names) + 1)
                    elif model_arch == "SSD300":
                        from torchvision.models.detection import ssd300_vgg16
                        self.model = ssd300_vgg16(pretrained=False, num_classes=len(self.class_names) + 1)
                    elif model_arch == "SSD512":
                        # éœ€è¦è‡ªå®šä¹‰å®ç°æˆ–ç¬¬ä¸‰æ–¹åº“
                        raise NotImplementedError("SSD512éœ€è¦ç‰¹å®šçš„å®ç°ï¼Œç›®å‰å°šæœªæ”¯æŒ")
                    
                    # åŠ è½½æ¨¡å‹æƒé‡
                    state_dict = torch.load(model_path, map_location=self.device)
                    
                    # å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
                    if isinstance(state_dict, dict):
                        if 'model_state_dict' in state_dict:
                            state_dict = state_dict['model_state_dict']
                        elif 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                    
                    self.model.load_state_dict(state_dict, strict=False)
                    self.model.to(self.device)
                    self.model.eval()
                    print(f"æˆåŠŸåŠ è½½SSDæ¨¡å‹: {model_path}")
                    
                except ImportError:
                    raise ImportError("åŠ è½½SSDæ¨¡å‹éœ€è¦å®‰è£…PyTorchå’Œtorchvision")
                
            # Faster R-CNNå’ŒMask R-CNNç³»åˆ—
            elif model_arch in ["Faster R-CNN", "Mask R-CNN"]:
                try:
                    import torch
                    if model_arch == "Faster R-CNN":
                        from torchvision.models.detection import fasterrcnn_resnet50_fpn
                        self.model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=len(self.class_names) + 1)
                    elif model_arch == "Mask R-CNN":
                        from torchvision.models.detection import maskrcnn_resnet50_fpn
                        self.model = maskrcnn_resnet50_fpn(pretrained=False, num_classes=len(self.class_names) + 1)
                        
                    # åŠ è½½æ¨¡å‹æƒé‡
                    state_dict = torch.load(model_path, map_location=self.device)
                    
                    # å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
                    if isinstance(state_dict, dict):
                        if 'model_state_dict' in state_dict:
                            state_dict = state_dict['model_state_dict']
                        elif 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                    
                    self.model.load_state_dict(state_dict, strict=False)
                    self.model.to(self.device)
                    self.model.eval()
                    print(f"æˆåŠŸåŠ è½½{model_arch}æ¨¡å‹: {model_path}")
                    
                except ImportError:
                    raise ImportError(f"åŠ è½½{model_arch}æ¨¡å‹éœ€è¦å®‰è£…PyTorchå’Œtorchvision")
            
            # RetinaNetæ¨¡å‹
            elif model_arch == "RetinaNet":
                try:
                    import torch
                    from torchvision.models.detection import retinanet_resnet50_fpn
                    self.model = retinanet_resnet50_fpn(pretrained=False, num_classes=len(self.class_names) + 1)
                    
                    # åŠ è½½æ¨¡å‹æƒé‡
                    state_dict = torch.load(model_path, map_location=self.device)
                    
                    # å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
                    if isinstance(state_dict, dict):
                        if 'model_state_dict' in state_dict:
                            state_dict = state_dict['model_state_dict']
                        elif 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                    
                    self.model.load_state_dict(state_dict, strict=False)
                    self.model.to(self.device)
                    self.model.eval()
                    print(f"æˆåŠŸåŠ è½½RetinaNetæ¨¡å‹: {model_path}")
                    
                except ImportError:
                    raise ImportError("åŠ è½½RetinaNetæ¨¡å‹éœ€è¦å®‰è£…PyTorchå’Œtorchvision")
            
            # DETRæ¨¡å‹
            elif model_arch == "DETR":
                try:
                    import torch
                    # å°è¯•ä½¿ç”¨DETR
                    try:
                        from transformers import DetrForObjectDetection
                        self.model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
                        # ä¿®æ”¹åˆ†ç±»å¤´ä»¥é€‚åº”è‡ªå®šä¹‰ç±»åˆ«æ•°é‡
                        self.model.config.num_labels = len(self.class_names)
                        self.model.class_labels_classifier = nn.Linear(
                            in_features=self.model.class_labels_classifier.in_features,
                            out_features=len(self.class_names)
                        )
                    except ImportError:
                        # å¤‡é€‰ï¼šä½¿ç”¨torchvisionçš„DETRå®ç°
                        from torchvision.models.detection import detr_resnet50
                        self.model = detr_resnet50(pretrained=False, num_classes=len(self.class_names) + 1)
                    
                    # åŠ è½½æ¨¡å‹æƒé‡
                    state_dict = torch.load(model_path, map_location=self.device)
                    
                    # å¤„ç†ä¸åŒæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
                    if isinstance(state_dict, dict):
                        if 'model_state_dict' in state_dict:
                            state_dict = state_dict['model_state_dict']
                        elif 'state_dict' in state_dict:
                            state_dict = state_dict['state_dict']
                    
                    self.model.load_state_dict(state_dict, strict=False)
                    self.model.to(self.device)
                    self.model.eval()
                    print(f"æˆåŠŸåŠ è½½DETRæ¨¡å‹: {model_path}")
                    
                except ImportError:
                    raise ImportError("åŠ è½½DETRæ¨¡å‹éœ€è¦å®‰è£…transformersåº“æˆ–æœ€æ–°ç‰ˆæœ¬çš„torchvision")
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹æ¨¡å‹æ¶æ„: {model_arch}")
                
        except Exception as e:
            import traceback
            traceback_str = traceback.format_exc()
            error_msg = f"åŠ è½½æ£€æµ‹æ¨¡å‹å¤±è´¥: {str(e)}\n{traceback_str}"
            print(error_msg)
            raise Exception(error_msg) 