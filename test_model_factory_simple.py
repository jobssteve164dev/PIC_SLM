#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæ¨¡å‹å·¥å‚Tabæµ‹è¯•è„šæœ¬
"""

import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

def test_llm_framework():
    """æµ‹è¯•LLMæ¡†æ¶"""
    print("ğŸ§  æµ‹è¯•LLMæ¡†æ¶...")
    
    try:
        from src.llm.llm_framework import LLMFramework
        from src.llm.model_adapters import create_llm_adapter
        
        # åˆ›å»ºLLMæ¡†æ¶ä½¿ç”¨æ¨¡æ‹Ÿé€‚é…å™¨
        framework = LLMFramework('mock')
        
        # å¯åŠ¨æ¡†æ¶
        framework.start()
        
        print("âœ… LLMæ¡†æ¶å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_metrics = {
            'epoch': 10,
            'train_loss': 0.234,
            'val_loss': 0.287,
            'train_accuracy': 0.894,
            'val_accuracy': 0.856
        }
        
        print("ğŸ“Š æµ‹è¯•è®­ç»ƒæŒ‡æ ‡åˆ†æ...")
        analysis = framework.analyze_training_metrics(test_metrics)
        print(f"   åˆ†æç»“æœ: {analysis.get('combined_insights', 'N/A')[:100]}...")
        
        print("ğŸ’¡ æµ‹è¯•è¶…å‚æ•°å»ºè®®...")
        suggestions = framework.get_hyperparameter_suggestions(test_metrics)
        if isinstance(suggestions, dict):
            suggestions_text = suggestions.get('llm_suggestions', str(suggestions))
        else:
            suggestions_text = str(suggestions)
        print(f"   å»ºè®®ç»“æœ: {suggestions_text[:100]}...")
        
        print("ğŸ”§ æµ‹è¯•é—®é¢˜è¯Šæ–­...")
        diagnosis = framework.diagnose_training_problems(test_metrics)
        if isinstance(diagnosis, dict):
            diagnosis_text = diagnosis.get('llm_diagnosis', str(diagnosis))
        else:
            diagnosis_text = str(diagnosis)
        print(f"   è¯Šæ–­ç»“æœ: {diagnosis_text[:100]}...")
        
        print("ğŸ“ˆ æµ‹è¯•æ¨¡å‹å¯¹æ¯”...")
        model_results = [
            {'model_name': 'ResNet50', 'accuracy': 0.89, 'val_loss': 0.23},
            {'model_name': 'EfficientNet', 'accuracy': 0.92, 'val_loss': 0.19}
        ]
        comparison = framework.compare_model_results(model_results)
        if isinstance(comparison, dict):
            comparison_text = comparison.get('analysis', str(comparison))
        else:
            comparison_text = str(comparison)
        print(f"   å¯¹æ¯”ç»“æœ: {comparison_text[:100]}...")
        
        print("ğŸ’¬ æµ‹è¯•å¯¹è¯åŠŸèƒ½...")
        response = framework.chat_with_training_context("è®­ç»ƒçŠ¶æ€å¦‚ä½•ï¼Ÿ")
        if isinstance(response, dict):
            response_text = response.get('response', str(response))
        else:
            response_text = str(response)
        print(f"   å¯¹è¯å“åº”: {response_text[:100]}...")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = framework.get_framework_stats()
        print(f"ğŸ“Š æ¡†æ¶ç»Ÿè®¡: æ€»è¯·æ±‚ {stats.get('total_requests', 0)}, æˆåŠŸç‡ {stats.get('success_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMæ¡†æ¶æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_ui_components():
    """æµ‹è¯•UIç»„ä»¶ï¼ˆæ— GUIï¼‰"""
    print("\nğŸ¨ æµ‹è¯•UIç»„ä»¶...")
    
    try:
        # æµ‹è¯•æ¨¡å‹å·¥å‚Tabç±»çš„å¯¼å…¥
        from src.ui.model_factory_tab import ModelFactoryTab, LLMChatWidget, AnalysisPanelWidget
        print("âœ… æ¨¡å‹å·¥å‚Tabç»„ä»¶å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åŸºç±»å¯¼å…¥
        from src.ui.base_tab import BaseTab
        print("âœ… BaseTabåŸºç±»å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ UIç»„ä»¶æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_integration():
    """æµ‹è¯•é›†æˆåŠŸèƒ½"""
    print("\nğŸ”— æµ‹è¯•é›†æˆåŠŸèƒ½...")
    
    try:
        # æµ‹è¯•ä¸»çª—å£å¯¼å…¥
        from src.ui.main_window import MainWindow
        print("âœ… ä¸»çª—å£å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹å·¥å‚Tabæ˜¯å¦å·²é›†æˆ
        # è¿™é‡Œæˆ‘ä»¬åªèƒ½æ£€æŸ¥å¯¼å…¥ï¼Œä¸èƒ½å®é™…åˆ›å»ºGUI
        print("âœ… é›†æˆæµ‹è¯•é€šè¿‡ï¼ˆé™æ€æ£€æŸ¥ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸ­ ç¬¬ä¸‰é˜¶æ®µï¼šAIæ¨¡å‹å·¥å‚ - åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # æµ‹è¯•LLMæ¡†æ¶
    results.append(test_llm_framework())
    
    # æµ‹è¯•UIç»„ä»¶
    results.append(test_ui_components())
    
    # æµ‹è¯•é›†æˆåŠŸèƒ½
    results.append(test_integration())
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    test_names = ["LLMæ¡†æ¶", "UIç»„ä»¶", "é›†æˆåŠŸèƒ½"]
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{i+1}. {name}: {status}")
    
    success_count = sum(results)
    total_count = len(results)
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {success_count}/{total_count} é¡¹æµ‹è¯•é€šè¿‡")
    
    if success_count == total_count:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¬¬ä¸‰é˜¶æ®µå¼€å‘æˆåŠŸï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        return False

if __name__ == "__main__":
    main() 