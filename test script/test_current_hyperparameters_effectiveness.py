#!/usr/bin/env python3
"""
æµ‹è¯•å½“å‰ç‰ˆæœ¬è¶…å‚æ•°ç”Ÿæ•ˆæ€§éªŒè¯è„šæœ¬

éªŒè¯æ™ºèƒ½æ¨æ–­é€»è¾‘æ˜¯å¦æ­£ç¡®è¯†åˆ«è¶…å‚æ•°å¯ç”¨çŠ¶æ€ï¼Œ
ä»¥åŠè¶…å‚æ•°æ˜¯å¦çœŸæ­£åº”ç”¨åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚
"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir / "src"))

# ç›´æ¥å¯¼å…¥éœ€è¦çš„æ¨¡å—
try:
    from training_components.optimizer_factory import OptimizerFactory
    import torch
    import torch.nn as nn
    print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def enhance_config_with_enable_states(config):
    """
    æ™ºèƒ½æ¨æ–­å¹¶å¢å¼ºé…ç½®ä¸­çš„å¯ç”¨çŠ¶æ€å­—æ®µ
    
    ç›´æ¥å®ç°ConfigApplierçš„é€»è¾‘ï¼Œé¿å…UIä¾èµ–
    """
    enhanced_config = config.copy()
    
    # æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'min_lr_enabled' not in enhanced_config:
        min_lr = enhanced_config.get('min_lr', 0.0)
        enhanced_config['min_lr_enabled'] = min_lr > 0.0
    
    # å­¦ä¹ ç‡é¢„çƒ­å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'warmup_enabled' not in enhanced_config:
        warmup_steps = enhanced_config.get('warmup_steps', 0)
        warmup_ratio = enhanced_config.get('warmup_ratio', 0.0)
        enhanced_config['warmup_enabled'] = warmup_steps > 0 or warmup_ratio > 0.0
    
    # æ ‡ç­¾å¹³æ»‘å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'label_smoothing_enabled' not in enhanced_config:
        label_smoothing = enhanced_config.get('label_smoothing', 0.0)
        enhanced_config['label_smoothing_enabled'] = label_smoothing > 0.0
    
    return enhanced_config


def test_critical_min_lr_issue():
    """æµ‹è¯•å…³é”®çš„æœ€å°å­¦ä¹ ç‡é—®é¢˜"""
    print("\nğŸ” æµ‹è¯•: å…³é”®çš„æœ€å°å­¦ä¹ ç‡é—®é¢˜")
    
    # è¿™æ˜¯å¯¼è‡´å­¦ä¹ ç‡æ›²çº¿å·®å¼‚çš„å…³é”®é—®é¢˜
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æ¨¡æ‹Ÿ7æœˆ4æ—¥ä¹‹å‰çš„é…ç½®ï¼ˆæ²¡æœ‰å¯ç”¨çŠ¶æ€å­—æ®µï¼‰
    old_config = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'min_lr': 1e-6  # æ—§ç‰ˆæœ¬ä¼šç›´æ¥ä½¿ç”¨è¿™ä¸ªå€¼
    }
    
    print(f"åŸå§‹é…ç½®: {old_config}")
    
    # åº”ç”¨æ™ºèƒ½æ¨æ–­
    enhanced_config = enhance_config_with_enable_states(old_config)
    
    # æ£€æŸ¥æ¨æ–­ç»“æœ
    min_lr_enabled = enhanced_config.get('min_lr_enabled', False)
    print(f"æ™ºèƒ½æ¨æ–­ç»“æœ: min_lr_enabled = {min_lr_enabled}")
    print(f"å¢å¼ºåé…ç½®: {enhanced_config}")
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = OptimizerFactory.create_scheduler(optimizer, enhanced_config)
    
    if scheduler and hasattr(scheduler, 'eta_min'):
        print(f"å®é™… eta_min = {scheduler.eta_min}")
        
        # å…³é”®é—®é¢˜ï¼šå¦‚æœmin_lr_enabledè¢«æ¨æ–­ä¸ºFalseï¼Œeta_minä¼šæ˜¯0è€Œä¸æ˜¯1e-6
        if min_lr_enabled:
            expected_eta_min = 1e-6
            print("âœ… æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ­£ç¡®æ¨æ–­")
        else:
            expected_eta_min = 0
            print("âŒ æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ¨æ–­é”™è¯¯ï¼")
            print("   è¿™ä¼šå¯¼è‡´å­¦ä¹ ç‡å¯ä»¥é™åˆ°æ¥è¿‘0ï¼Œä¸7æœˆ4æ—¥ç‰ˆæœ¬è¡Œä¸ºä¸åŒ")
            return False
        
        if abs(scheduler.eta_min - expected_eta_min) < 1e-10:
            print("âœ… eta_minå€¼æ­£ç¡®")
        else:
            print(f"âŒ eta_minå€¼é”™è¯¯: æœŸæœ› {expected_eta_min}, å®é™… {scheduler.eta_min}")
            return False
    else:
        print("âŒ è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    return True


def test_scheduler_comparison():
    """æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„è°ƒåº¦å™¨è¡Œä¸º"""
    print("\nğŸ” æµ‹è¯•: è°ƒåº¦å™¨è¡Œä¸ºå¯¹æ¯”")
    
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æµ‹è¯•åœºæ™¯
    test_cases = [
        {
            'name': '7æœˆ4æ—¥ç‰ˆæœ¬æ¨¡æ‹Ÿï¼ˆç›´æ¥ä½¿ç”¨min_lrï¼‰',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr': 1e-6},
            'use_enhancement': False,  # ä¸ä½¿ç”¨æ™ºèƒ½æ¨æ–­
            'expected_eta_min': 1e-6
        },
        {
            'name': 'å½“å‰ç‰ˆæœ¬ï¼ˆæ™ºèƒ½æ¨æ–­ï¼‰',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr': 1e-6},
            'use_enhancement': True,  # ä½¿ç”¨æ™ºèƒ½æ¨æ–­
            'expected_eta_min': 1e-6  # æœŸæœ›æ¨æ–­ä¸ºå¯ç”¨
        },
        {
            'name': 'æ˜¾å¼å¯ç”¨æœ€å°å­¦ä¹ ç‡',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr_enabled': True, 'min_lr': 1e-6},
            'use_enhancement': False,
            'expected_eta_min': 1e-6
        },
        {
            'name': 'æ˜¾å¼ç¦ç”¨æœ€å°å­¦ä¹ ç‡',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr_enabled': False, 'min_lr': 1e-6},
            'use_enhancement': False,
            'expected_eta_min': 0
        }
    ]
    
    all_passed = True
    
    for case in test_cases:
        print(f"\n  åœºæ™¯: {case['name']}")
        
        config = case['config'].copy()
        if case['use_enhancement']:
            config = enhance_config_with_enable_states(config)
        
        # æ¨¡æ‹Ÿ7æœˆ4æ—¥ç‰ˆæœ¬çš„è¡Œä¸ºï¼ˆç›´æ¥ä½¿ç”¨min_lrï¼‰
        if case['name'] == '7æœˆ4æ—¥ç‰ˆæœ¬æ¨¡æ‹Ÿï¼ˆç›´æ¥ä½¿ç”¨min_lrï¼‰':
            # ç›´æ¥åˆ›å»ºè°ƒåº¦å™¨ï¼Œä¸ç»è¿‡å½“å‰ç‰ˆæœ¬çš„é€»è¾‘
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=config['T_max'], eta_min=config['min_lr']
            )
        else:
            scheduler = OptimizerFactory.create_scheduler(optimizer, config)
        
        if scheduler and hasattr(scheduler, 'eta_min'):
            print(f"    é…ç½®: {config}")
            print(f"    eta_min: {scheduler.eta_min}")
            
            if abs(scheduler.eta_min - case['expected_eta_min']) < 1e-10:
                print("    âœ… ç»“æœæ­£ç¡®")
            else:
                print(f"    âŒ ç»“æœé”™è¯¯: æœŸæœ› {case['expected_eta_min']}, å®é™… {scheduler.eta_min}")
                all_passed = False
        else:
            print("    âŒ è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
            all_passed = False
    
    return all_passed


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å½“å‰ç‰ˆæœ¬è¶…å‚æ•°ç”Ÿæ•ˆæ€§...\n")
    
    # æ‰§è¡Œå…³é”®æµ‹è¯•
    test1_result = test_critical_min_lr_issue()
    test2_result = test_scheduler_comparison()
    
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("="*60)
    
    print(f"å…³é”®æœ€å°å­¦ä¹ ç‡é—®é¢˜: {'âœ… é€šè¿‡' if test1_result else 'âŒ å¤±è´¥'}")
    print(f"è°ƒåº¦å™¨è¡Œä¸ºå¯¹æ¯”: {'âœ… é€šè¿‡' if test2_result else 'âŒ å¤±è´¥'}")
    
    if test1_result and test2_result:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼")
        print("å½“å‰ç‰ˆæœ¬çš„æ™ºèƒ½æ¨æ–­é€»è¾‘å·¥ä½œæ­£å¸¸ï¼Œè¶…å‚æ•°åº”è¯¥èƒ½æ­£ç¡®ç”Ÿæ•ˆã€‚")
    else:
        print("\nâš ï¸  æµ‹è¯•å¤±è´¥ï¼Œå‘ç°è¶…å‚æ•°ç”Ÿæ•ˆæ€§é—®é¢˜ï¼")
        print("\nğŸ”§ é—®é¢˜åˆ†æ:")
        if not test1_result:
            print("- æ™ºèƒ½æ¨æ–­é€»è¾‘å¯èƒ½å­˜åœ¨é—®é¢˜")
            print("- æ—§é…ç½®æ–‡ä»¶çš„min_lrå¯èƒ½ä¸ä¼šè¢«æ­£ç¡®è¯†åˆ«")
        if not test2_result:
            print("- ä¸åŒé…ç½®åœºæ™¯ä¸‹çš„è¡Œä¸ºä¸ä¸€è‡´")
        
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥ConfigApplierä¸­çš„æ™ºèƒ½æ¨æ–­é€»è¾‘")
        print("2. ç¡®ä¿å‘åå…¼å®¹æ€§å¤„ç†æ­£ç¡®")
        print("3. åœ¨è®­ç»ƒé…ç½®ä¸­æ˜¾å¼è®¾ç½®å¯ç”¨çŠ¶æ€å­—æ®µ")
    
    return test1_result and test2_result


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 