#!/usr/bin/env python3
"""
æµ‹è¯•å½“å‰ç‰ˆæœ¬è¶…å‚æ•°ç”Ÿæ•ˆæ€§éªŒè¯è„šæœ¬

éªŒè¯æ™ºèƒ½æ¨æ–­é€»è¾‘æ˜¯å¦æ­£ç¡®è¯†åˆ«è¶…å‚æ•°å¯ç”¨çŠ¶æ€ï¼Œ
ä»¥åŠè¶…å‚æ•°æ˜¯å¦çœŸæ­£åº”ç”¨åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚
"""

import sys
import os
import json
import tempfile
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# ç›´æ¥å¯¼å…¥éœ€è¦çš„æ¨¡å—ï¼Œé¿å…UIç»„ä»¶çš„å¤æ‚ä¾èµ–
try:
    from training_components.optimizer_factory import OptimizerFactory
    from training_components.training_validator import TrainingValidator
    import torch
    import torch.nn as nn
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def enhance_config_with_enable_states(config):
    """
    æ™ºèƒ½æ¨æ–­å¹¶å¢å¼ºé…ç½®ä¸­çš„å¯ç”¨çŠ¶æ€å­—æ®µ
    
    ç›´æ¥å®ç°ConfigApplierçš„é€»è¾‘ï¼Œé¿å…UIä¾èµ–
    """
    enhanced_config = config.copy()
    
    # å­¦ä¹ ç‡é¢„çƒ­å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'warmup_enabled' not in enhanced_config:
        warmup_steps = enhanced_config.get('warmup_steps', 0)
        warmup_ratio = enhanced_config.get('warmup_ratio', 0.0)
        enhanced_config['warmup_enabled'] = warmup_steps > 0 or warmup_ratio > 0.0
    
    # æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'min_lr_enabled' not in enhanced_config:
        min_lr = enhanced_config.get('min_lr', 0.0)
        enhanced_config['min_lr_enabled'] = min_lr > 0.0
    
    # æ ‡ç­¾å¹³æ»‘å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'label_smoothing_enabled' not in enhanced_config:
        label_smoothing = enhanced_config.get('label_smoothing', 0.0)
        enhanced_config['label_smoothing_enabled'] = label_smoothing > 0.0
    
    # æ¢¯åº¦ç´¯ç§¯å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'gradient_accumulation_enabled' not in enhanced_config:
        gradient_accumulation_steps = enhanced_config.get('gradient_accumulation_steps', 1)
        enhanced_config['gradient_accumulation_enabled'] = gradient_accumulation_steps > 1
    
    # é«˜çº§æ•°æ®å¢å¼ºå¯ç”¨çŠ¶æ€æ¨æ–­
    if 'advanced_augmentation_enabled' not in enhanced_config:
        cutmix_prob = enhanced_config.get('cutmix_prob', 0.0)
        mixup_alpha = enhanced_config.get('mixup_alpha', 0.0)
        enhanced_config['advanced_augmentation_enabled'] = cutmix_prob > 0.0 or mixup_alpha > 0.0
    
    # æŸå¤±ç¼©æ”¾å¯ç”¨çŠ¶æ€æ¨æ–­
    if 'loss_scaling_enabled' not in enhanced_config:
        loss_scale = enhanced_config.get('loss_scale', 'none')
        enhanced_config['loss_scaling_enabled'] = loss_scale != 'none'
    
    return enhanced_config


def test_config_enhancement():
    """æµ‹è¯•é…ç½®å¢å¼ºåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•1: é…ç½®å¢å¼ºåŠŸèƒ½")
    
    # æ¨¡æ‹Ÿæ—§ç‰ˆæœ¬é…ç½®æ–‡ä»¶ï¼ˆæ²¡æœ‰å¯ç”¨çŠ¶æ€å­—æ®µï¼‰
    old_config = {
        'warmup_steps': 100,
        'warmup_ratio': 0.0,
        'min_lr': 1e-6,
        'label_smoothing': 0.1,
        'gradient_accumulation_steps': 4,
        'cutmix_prob': 0.5,
        'mixup_alpha': 0.2,
        'loss_scale': 'dynamic'
    }
    
    # åº”ç”¨æ™ºèƒ½æ¨æ–­
    enhanced_config = enhance_config_with_enable_states(old_config)
    
    # éªŒè¯æ¨æ–­ç»“æœ
    expected_states = {
        'warmup_enabled': True,  # warmup_steps > 0
        'min_lr_enabled': True,  # min_lr > 0
        'label_smoothing_enabled': True,  # label_smoothing > 0
        'gradient_accumulation_enabled': True,  # gradient_accumulation_steps > 1
        'advanced_augmentation_enabled': True,  # cutmix_prob > 0 or mixup_alpha > 0
        'loss_scaling_enabled': True  # loss_scale != 'none'
    }
    
    success = True
    for key, expected in expected_states.items():
        actual = enhanced_config.get(key, False)
        if actual != expected:
            print(f"âŒ {key}: æœŸæœ› {expected}, å®é™… {actual}")
            success = False
        else:
            print(f"âœ… {key}: {actual}")
    
    return success


def test_scheduler_creation():
    """æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»º"""
    print("\nğŸ” æµ‹è¯•2: å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»º")
    
    # åˆ›å»ºç®€å•æ¨¡å‹ç”¨äºæµ‹è¯•
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æµ‹è¯•åœºæ™¯1: å¯ç”¨æœ€å°å­¦ä¹ ç‡
    config1 = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'min_lr_enabled': True,
        'min_lr': 1e-6
    }
    
    scheduler1 = OptimizerFactory.create_scheduler(optimizer, config1)
    if scheduler1 and hasattr(scheduler1, 'eta_min'):
        print(f"âœ… å¯ç”¨æœ€å°å­¦ä¹ ç‡: eta_min = {scheduler1.eta_min}")
    else:
        print("âŒ å¯ç”¨æœ€å°å­¦ä¹ ç‡: è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    # æµ‹è¯•åœºæ™¯2: ç¦ç”¨æœ€å°å­¦ä¹ ç‡
    config2 = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'min_lr_enabled': False,
        'min_lr': 1e-6
    }
    
    scheduler2 = OptimizerFactory.create_scheduler(optimizer, config2)
    if scheduler2 and hasattr(scheduler2, 'eta_min'):
        print(f"âœ… ç¦ç”¨æœ€å°å­¦ä¹ ç‡: eta_min = {scheduler2.eta_min}")
        if scheduler2.eta_min != 0:
            print("âŒ ç¦ç”¨æœ€å°å­¦ä¹ ç‡æ—¶ï¼Œeta_minåº”è¯¥ä¸º0")
            return False
    else:
        print("âŒ ç¦ç”¨æœ€å°å­¦ä¹ ç‡: è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    # æµ‹è¯•åœºæ™¯3: æ—§é…ç½®æ–‡ä»¶ï¼ˆæ™ºèƒ½æ¨æ–­ï¼‰
    old_config = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'min_lr': 1e-6  # æ²¡æœ‰min_lr_enabledå­—æ®µ
    }
    
    enhanced_config = enhance_config_with_enable_states(old_config)
    scheduler3 = OptimizerFactory.create_scheduler(optimizer, enhanced_config)
    
    if scheduler3 and hasattr(scheduler3, 'eta_min'):
        min_lr_enabled = enhanced_config.get('min_lr_enabled', False)
        expected_eta_min = 1e-6 if min_lr_enabled else 0
        print(f"âœ… æ™ºèƒ½æ¨æ–­: min_lr_enabled = {min_lr_enabled}, eta_min = {scheduler3.eta_min}")
        
        if scheduler3.eta_min != expected_eta_min:
            print(f"âŒ æ™ºèƒ½æ¨æ–­ç»“æœä¸æ­£ç¡®: æœŸæœ› eta_min = {expected_eta_min}, å®é™… = {scheduler3.eta_min}")
            return False
    else:
        print("âŒ æ™ºèƒ½æ¨æ–­: è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    return True


def test_warmup_functionality():
    """æµ‹è¯•é¢„çƒ­åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•3: é¢„çƒ­åŠŸèƒ½")
    
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æµ‹è¯•åœºæ™¯1: å¯ç”¨é¢„çƒ­
    config1 = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'warmup_enabled': True,
        'warmup_steps': 10,
        'warmup_method': 'linear'
    }
    
    scheduler1 = OptimizerFactory.create_scheduler(optimizer, config1, total_steps=100)
    if scheduler1:
        # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„çƒ­è°ƒåº¦å™¨
        if hasattr(scheduler1, 'warmup_steps'):
            print(f"âœ… å¯ç”¨é¢„çƒ­: warmup_steps = {scheduler1.warmup_steps}")
        else:
            print("âŒ å¯ç”¨é¢„çƒ­: æœªåˆ›å»ºé¢„çƒ­è°ƒåº¦å™¨")
            return False
    else:
        print("âŒ å¯ç”¨é¢„çƒ­: è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    # æµ‹è¯•åœºæ™¯2: ç¦ç”¨é¢„çƒ­
    config2 = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'warmup_enabled': False,
        'warmup_steps': 10  # å³ä½¿è®¾ç½®äº†æ­¥æ•°ï¼Œä¹Ÿä¸åº”è¯¥å¯ç”¨
    }
    
    scheduler2 = OptimizerFactory.create_scheduler(optimizer, config2, total_steps=100)
    if scheduler2:
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸºç¡€è°ƒåº¦å™¨ï¼ˆä¸æ˜¯é¢„çƒ­è°ƒåº¦å™¨ï¼‰
        if not hasattr(scheduler2, 'warmup_steps'):
            print("âœ… ç¦ç”¨é¢„çƒ­: ä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨")
        else:
            print("âŒ ç¦ç”¨é¢„çƒ­: é”™è¯¯åœ°åˆ›å»ºäº†é¢„çƒ­è°ƒåº¦å™¨")
            return False
    else:
        print("âŒ ç¦ç”¨é¢„çƒ­: è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    return True


def test_loss_function_creation():
    """æµ‹è¯•æŸå¤±å‡½æ•°åˆ›å»º"""
    print("\nğŸ” æµ‹è¯•4: æŸå¤±å‡½æ•°åˆ›å»º")
    
    # æµ‹è¯•åœºæ™¯1: å¯ç”¨æ ‡ç­¾å¹³æ»‘
    config1 = {
        'label_smoothing_enabled': True,
        'label_smoothing': 0.1
    }
    
    criterion1 = OptimizerFactory.create_criterion(config1)
    if hasattr(criterion1, 'smoothing'):
        print(f"âœ… å¯ç”¨æ ‡ç­¾å¹³æ»‘: smoothing = {criterion1.smoothing}")
    else:
        print("âŒ å¯ç”¨æ ‡ç­¾å¹³æ»‘: æœªåˆ›å»ºæ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°")
        return False
    
    # æµ‹è¯•åœºæ™¯2: ç¦ç”¨æ ‡ç­¾å¹³æ»‘
    config2 = {
        'label_smoothing_enabled': False,
        'label_smoothing': 0.1  # å³ä½¿è®¾ç½®äº†å€¼ï¼Œä¹Ÿä¸åº”è¯¥å¯ç”¨
    }
    
    criterion2 = OptimizerFactory.create_criterion(config2)
    if isinstance(criterion2, nn.CrossEntropyLoss):
        print("âœ… ç¦ç”¨æ ‡ç­¾å¹³æ»‘: ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µæŸå¤±")
    else:
        print("âŒ ç¦ç”¨æ ‡ç­¾å¹³æ»‘: é”™è¯¯åœ°åˆ›å»ºäº†æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°")
        return False
    
    return True


def test_critical_min_lr_issue():
    """æµ‹è¯•å…³é”®çš„æœ€å°å­¦ä¹ ç‡é—®é¢˜"""
    print("\nğŸ” æµ‹è¯•5: å…³é”®çš„æœ€å°å­¦ä¹ ç‡é—®é¢˜")
    
    # è¿™æ˜¯å¯¼è‡´å­¦ä¹ ç‡æ›²çº¿å·®å¼‚çš„å…³é”®é—®é¢˜
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # æ¨¡æ‹Ÿ7æœˆ4æ—¥ä¹‹å‰çš„é…ç½®ï¼ˆæ²¡æœ‰å¯ç”¨çŠ¶æ€å­—æ®µï¼‰
    old_config = {
        'lr_scheduler': 'CosineAnnealingLR',
        'T_max': 50,
        'min_lr': 1e-6  # æ—§ç‰ˆæœ¬ä¼šç›´æ¥ä½¿ç”¨è¿™ä¸ªå€¼
    }
    
    # åº”ç”¨æ™ºèƒ½æ¨æ–­
    enhanced_config = enhance_config_with_enable_states(old_config)
    
    # æ£€æŸ¥æ¨æ–­ç»“æœ
    min_lr_enabled = enhanced_config.get('min_lr_enabled', False)
    print(f"æ™ºèƒ½æ¨æ–­ç»“æœ: min_lr_enabled = {min_lr_enabled}")
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = OptimizerFactory.create_scheduler(optimizer, enhanced_config)
    
    if scheduler and hasattr(scheduler, 'eta_min'):
        print(f"å®é™… eta_min = {scheduler.eta_min}")
        
        # å…³é”®é—®é¢˜ï¼šå¦‚æœmin_lr_enabledè¢«æ¨æ–­ä¸ºFalseï¼Œeta_minä¼šæ˜¯0è€Œä¸æ˜¯1e-6
        if min_lr_enabled:
            expected_eta_min = 1e-6
            print("âœ… æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ­£ç¡®æ¨æ–­")
        else:
            expected_eta_min = 0
            print("âŒ æœ€å°å­¦ä¹ ç‡å¯ç”¨çŠ¶æ€æ¨æ–­é”™è¯¯ï¼")
            print("   è¿™ä¼šå¯¼è‡´å­¦ä¹ ç‡å¯ä»¥é™åˆ°æ¥è¿‘0ï¼Œä¸7æœˆ4æ—¥ç‰ˆæœ¬è¡Œä¸ºä¸åŒ")
            return False
        
        if abs(scheduler.eta_min - expected_eta_min) < 1e-10:
            print("âœ… eta_minå€¼æ­£ç¡®")
        else:
            print(f"âŒ eta_minå€¼é”™è¯¯: æœŸæœ› {expected_eta_min}, å®é™… {scheduler.eta_min}")
            return False
    else:
        print("âŒ è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
        return False
    
    return True


def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("\nğŸ” æµ‹è¯•6: å‘åå…¼å®¹æ€§")
    
    # æ¨¡æ‹Ÿå„ç§æ—§é…ç½®æ–‡ä»¶åœºæ™¯
    test_cases = [
        {
            'name': 'é»˜è®¤min_lré…ç½®',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr': 1e-6},
            'expected_min_lr_enabled': True,  # åº”è¯¥æ¨æ–­ä¸ºå¯ç”¨
            'expected_eta_min': 1e-6
        },
        {
            'name': 'é›¶min_lré…ç½®',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr': 0.0},
            'expected_min_lr_enabled': False,  # åº”è¯¥æ¨æ–­ä¸ºç¦ç”¨
            'expected_eta_min': 0
        },
        {
            'name': 'æ— min_lré…ç½®',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50},
            'expected_min_lr_enabled': False,  # åº”è¯¥æ¨æ–­ä¸ºç¦ç”¨
            'expected_eta_min': 0
        },
        {
            'name': 'è‡ªå®šä¹‰min_lré…ç½®',
            'config': {'lr_scheduler': 'CosineAnnealingLR', 'T_max': 50, 'min_lr': 1e-5},
            'expected_min_lr_enabled': True,  # åº”è¯¥æ¨æ–­ä¸ºå¯ç”¨
            'expected_eta_min': 1e-5
        }
    ]
    
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    all_passed = True
    
    for case in test_cases:
        print(f"\n  æµ‹è¯•åœºæ™¯: {case['name']}")
        
        # åº”ç”¨æ™ºèƒ½æ¨æ–­
        enhanced_config = enhance_config_with_enable_states(case['config'])
        
        # æ£€æŸ¥æ¨æ–­ç»“æœ
        min_lr_enabled = enhanced_config.get('min_lr_enabled', False)
        if min_lr_enabled != case['expected_min_lr_enabled']:
            print(f"    âŒ æ¨æ–­é”™è¯¯: æœŸæœ› {case['expected_min_lr_enabled']}, å®é™… {min_lr_enabled}")
            all_passed = False
            continue
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = OptimizerFactory.create_scheduler(optimizer, enhanced_config)
        
        if scheduler and hasattr(scheduler, 'eta_min'):
            if abs(scheduler.eta_min - case['expected_eta_min']) < 1e-10:
                print(f"    âœ… æ­£ç¡®: eta_min = {scheduler.eta_min}")
            else:
                print(f"    âŒ eta_miné”™è¯¯: æœŸæœ› {case['expected_eta_min']}, å®é™… {scheduler.eta_min}")
                all_passed = False
        else:
            print("    âŒ è°ƒåº¦å™¨åˆ›å»ºå¤±è´¥")
            all_passed = False
    
    return all_passed


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å½“å‰ç‰ˆæœ¬è¶…å‚æ•°ç”Ÿæ•ˆæ€§...\n")
    
    test_results = []
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(("é…ç½®å¢å¼ºåŠŸèƒ½", test_config_enhancement()))
    test_results.append(("å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»º", test_scheduler_creation()))
    test_results.append(("é¢„çƒ­åŠŸèƒ½", test_warmup_functionality()))
    test_results.append(("æŸå¤±å‡½æ•°åˆ›å»º", test_loss_function_creation()))
    test_results.append(("å…³é”®æœ€å°å­¦ä¹ ç‡é—®é¢˜", test_critical_min_lr_issue()))
    test_results.append(("å‘åå…¼å®¹æ€§", test_backward_compatibility()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("="*60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼å½“å‰ç‰ˆæœ¬è¶…å‚æ•°åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œå­˜åœ¨è¶…å‚æ•°ç”Ÿæ•ˆæ€§é—®é¢˜")
        
        # æä¾›ä¿®å¤å»ºè®®
        print("\nğŸ”§ ä¿®å¤å»ºè®®:")
        print("1. æ£€æŸ¥æ™ºèƒ½æ¨æ–­é€»è¾‘æ˜¯å¦æ­£ç¡®å¤„ç†é»˜è®¤å€¼")
        print("2. ç¡®è®¤å‘åå…¼å®¹æ€§æ˜¯å¦ç¬¦åˆé¢„æœŸ")
        print("3. éªŒè¯å…³é”®çš„æœ€å°å­¦ä¹ ç‡æ¨æ–­é€»è¾‘")
        
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 