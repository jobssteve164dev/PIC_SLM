#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºAIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•è®­ç»ƒé…ç½®æ–‡ä»¶é›†æˆåŠŸèƒ½
"""

import sys
import os
import json
import time

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, '..', 'src')
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

def test_enhanced_analysis_engine():
    """æµ‹è¯•å¢å¼ºçš„åˆ†æå¼•æ“"""
    print("ğŸ§  æµ‹è¯•å¢å¼ºçš„åˆ†æå¼•æ“...")
    
    try:
        from llm.analysis_engine import TrainingAnalysisEngine
        from llm.model_adapters import MockLLMAdapter
        
        # åˆ›å»ºåˆ†æå¼•æ“
        mock_adapter = MockLLMAdapter()
        engine = TrainingAnalysisEngine(mock_adapter)
        
        print("âœ… åˆ†æå¼•æ“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒé…ç½®æŸ¥æ‰¾åŠŸèƒ½
        print("\nğŸ“‹ æµ‹è¯•è®­ç»ƒé…ç½®æŸ¥æ‰¾åŠŸèƒ½...")
        latest_config = engine._find_latest_training_config()
        if latest_config:
            print(f"   æ‰¾åˆ°é…ç½®æ–‡ä»¶: {latest_config['file_path']}")
            print(f"   é…ç½®æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(latest_config['timestamp']))}")
            print(f"   æ¨¡å‹åç§°: {latest_config['config'].get('model_name', 'N/A')}")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒé…ç½®æ–‡ä»¶")
        
        # æµ‹è¯•è®­ç»ƒé…ç½®ä¸Šä¸‹æ–‡ç”Ÿæˆ
        print("\nğŸ“ æµ‹è¯•è®­ç»ƒé…ç½®ä¸Šä¸‹æ–‡ç”Ÿæˆ...")
        config_context = engine._get_training_config_context()
        print(f"   é…ç½®ä¸Šä¸‹æ–‡é•¿åº¦: {len(config_context)} å­—ç¬¦")
        print(f"   é…ç½®ä¸Šä¸‹æ–‡é¢„è§ˆ: {config_context[:200]}...")
        
        # æµ‹è¯•å¢å¼ºçš„åˆ†ææç¤ºè¯æ„å»º
        print("\nğŸ”§ æµ‹è¯•å¢å¼ºçš„åˆ†ææç¤ºè¯æ„å»º...")
        test_metrics = {
            'epoch': 10,
            'train_loss': 0.234,
            'val_loss': 0.287,
            'train_accuracy': 0.894,
            'val_accuracy': 0.856
        }
        test_trends = {
            'train_losses': [0.5, 0.4, 0.3, 0.25, 0.234],
            'val_losses': [0.6, 0.5, 0.4, 0.3, 0.287],
            'train_accuracies': [0.7, 0.8, 0.85, 0.88, 0.894],
            'val_accuracies': [0.65, 0.75, 0.8, 0.83, 0.856]
        }
        test_real_data = {
            'session_id': 'test_session_001',
            'collection_duration': 3600.0,
            'total_data_points': 50,
            'training_status': 'training'
        }
        
        enhanced_prompt = engine._build_enhanced_analysis_prompt(test_metrics, test_trends, test_real_data)
        print(f"   å¢å¼ºæç¤ºè¯é•¿åº¦: {len(enhanced_prompt)} å­—ç¬¦")
        print(f"   æç¤ºè¯åŒ…å«é…ç½®ä¿¡æ¯: {'æ˜¯' if 'è®­ç»ƒé…ç½®ä¿¡æ¯' in enhanced_prompt else 'å¦'}")
        print(f"   æç¤ºè¯åŒ…å«å®æ—¶æ•°æ®: {'æ˜¯' if 'å®æ—¶è®­ç»ƒæ•°æ®' in enhanced_prompt else 'å¦'}")
        
        # æµ‹è¯•å®Œæ•´çš„çœŸå®æ•°æ®åˆ†æ
        print("\nğŸ“Š æµ‹è¯•å®Œæ•´çš„çœŸå®æ•°æ®åˆ†æ...")
        analysis_result = engine.analyze_real_training_progress()
        
        if 'error' in analysis_result:
            print(f"   âš ï¸ åˆ†æå¤±è´¥: {analysis_result['error']}")
        else:
            print(f"   âœ… åˆ†ææˆåŠŸ")
            print(f"   æ•°æ®æ¥æº: {analysis_result.get('data_source', 'N/A')}")
            print(f"   ä¼šè¯ID: {analysis_result.get('session_id', 'N/A')}")
            print(f"   ç»¼åˆåˆ†æé•¿åº¦: {len(analysis_result.get('combined_insights', ''))} å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é…ç½®ä¿¡æ¯
            combined_insights = analysis_result.get('combined_insights', '')
            has_config_info = 'è®­ç»ƒé…ç½®ä¿¡æ¯' in combined_insights or 'æ¨¡å‹æ¶æ„' in combined_insights
            print(f"   åŒ…å«é…ç½®ä¿¡æ¯: {'æ˜¯' if has_config_info else 'å¦'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_adapter_custom_prompt():
    """æµ‹è¯•LLMé€‚é…å™¨çš„è‡ªå®šä¹‰æç¤ºè¯åŠŸèƒ½"""
    print("\nğŸ¤– æµ‹è¯•LLMé€‚é…å™¨è‡ªå®šä¹‰æç¤ºè¯åŠŸèƒ½...")
    
    try:
        from llm.model_adapters import MockLLMAdapter
        
        # åˆ›å»ºæ¨¡æ‹Ÿé€‚é…å™¨
        adapter = MockLLMAdapter()
        
        # æµ‹è¯•æ ‡å‡†åˆ†æ
        print("   æµ‹è¯•æ ‡å‡†åˆ†æ...")
        test_metrics = {'epoch': 10, 'train_loss': 0.234}
        standard_result = adapter.analyze_metrics(test_metrics)
        print(f"   æ ‡å‡†åˆ†æç»“æœé•¿åº¦: {len(standard_result)} å­—ç¬¦")
        
        # æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯
        print("   æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯...")
        custom_prompt = """
è¯·åŸºäºä»¥ä¸‹è®­ç»ƒé…ç½®è¿›è¡Œä¸“ä¸šåˆ†æï¼š

## è®­ç»ƒé…ç½®
- æ¨¡å‹: ResNet50
- å­¦ä¹ ç‡: 0.001
- æ‰¹æ¬¡å¤§å°: 32
- ä¼˜åŒ–å™¨: Adam

## è®­ç»ƒæŒ‡æ ‡
- Epoch: 10
- è®­ç»ƒæŸå¤±: 0.234

è¯·æä¾›é’ˆå¯¹æ€§çš„ä¼˜åŒ–å»ºè®®ã€‚
"""
        custom_result = adapter.analyze_metrics(test_metrics, custom_prompt)
        print(f"   è‡ªå®šä¹‰åˆ†æç»“æœé•¿åº¦: {len(custom_result)} å­—ç¬¦")
        print(f"   è‡ªå®šä¹‰åˆ†æåŒ…å«é…ç½®ä¿¡æ¯: {'æ˜¯' if 'ResNet50' in custom_result else 'å¦'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_enhanced_chat_functionality():
    """æµ‹è¯•å¢å¼ºçš„èŠå¤©åŠŸèƒ½"""
    print("\nğŸ’¬ æµ‹è¯•å¢å¼ºçš„èŠå¤©åŠŸèƒ½...")
    
    try:
        from llm.llm_framework import LLMFramework
        
        # åˆ›å»ºLLMæ¡†æ¶
        framework = LLMFramework('mock')
        framework.start()
        
        print("âœ… LLMæ¡†æ¶åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•èŠå¤©åŠŸèƒ½
        test_question = "å½“å‰è®­ç»ƒæ•ˆæœå¦‚ä½•ï¼Ÿæœ‰ä»€ä¹ˆä¼˜åŒ–å»ºè®®å—ï¼Ÿ"
        print(f"   æµ‹è¯•é—®é¢˜: {test_question}")
        
        chat_result = framework.chat_with_training_context(test_question)
        
        if isinstance(chat_result, dict) and 'error' in chat_result:
            print(f"   âš ï¸ èŠå¤©å¤±è´¥: {chat_result['error']}")
        else:
            response = chat_result.get('response', str(chat_result))
            print(f"   âœ… èŠå¤©æˆåŠŸ")
            print(f"   å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            print(f"   å“åº”åŒ…å«é…ç½®ä¿¡æ¯: {'æ˜¯' if 'è®­ç»ƒé…ç½®' in response or 'æ¨¡å‹æ¶æ„' in response else 'å¦'}")
        
        framework.stop()
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def create_test_config_file():
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\nğŸ“ åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶...")
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        test_config_dir = "models/params/classification"
        os.makedirs(test_config_dir, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
        test_config = {
            "data_dir": "test_dataset",
            "model_name": "ResNet50",
            "num_epochs": 50,
            "batch_size": 32,
            "learning_rate": 0.001,
            "model_save_dir": "models/saved_models",
            "task_type": "classification",
            "optimizer": "Adam",
            "use_pretrained": True,
            "metrics": ["accuracy"],
            "use_tensorboard": True,
            "weight_decay": 0.0001,
            "lr_scheduler": "StepLR",
            "use_augmentation": True,
            "early_stopping": True,
            "early_stopping_patience": 10,
            "gradient_clipping": False,
            "mixed_precision": True,
            "dropout_rate": 0.2,
            "activation_function": "ReLU",
            "use_class_weights": True,
            "weight_strategy": "balanced",
            "class_weights": {
                "class1": 1.0,
                "class2": 2.0,
                "class3": 1.5
            },
            "warmup_enabled": True,
            "warmup_steps": 1000,
            "min_lr_enabled": True,
            "min_lr": 1e-6,
            "label_smoothing_enabled": True,
            "label_smoothing": 0.1,
            "model_ema": True,
            "model_ema_decay": 0.9999,
            "model_filename": "ResNet50_test_config",
            "timestamp": time.strftime("%Y%m%d-%H%M%S")
        }
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_file_path = os.path.join(test_config_dir, f"ResNet50_{time.strftime('%Y%m%d-%H%M%S')}_test_config.json")
        with open(config_file_path, 'w', encoding='utf-8') as f:
            json.dump(test_config, f, ensure_ascii=False, indent=4)
        
        print(f"   âœ… æµ‹è¯•é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file_path}")
        return config_file_path
        
    except Exception as e:
        print(f"   âŒ åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¢å¼ºAIåŠ©æ‰‹åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
    test_config_file = create_test_config_file()
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        ("å¢å¼ºåˆ†æå¼•æ“", test_enhanced_analysis_engine),
        ("LLMé€‚é…å™¨è‡ªå®šä¹‰æç¤ºè¯", test_llm_adapter_custom_prompt),
        ("å¢å¼ºèŠå¤©åŠŸèƒ½", test_enhanced_chat_functionality)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        if test_func():
            passed_tests += 1
            print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
        else:
            print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºAIåŠ©æ‰‹åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if test_config_file and os.path.exists(test_config_file):
        try:
            os.remove(test_config_file)
            print(f"ğŸ§¹ å·²æ¸…ç†æµ‹è¯•é…ç½®æ–‡ä»¶: {test_config_file}")
        except:
            pass

if __name__ == "__main__":
    main() 