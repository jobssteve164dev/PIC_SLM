#!/usr/bin/env python3
"""
ç¬¬äºŒé˜¶æ®µLLMæ¡†æ¶å®Œæ•´æ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†LLMæ¡†æ¶çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. è®­ç»ƒæŒ‡æ ‡æ™ºèƒ½åˆ†æ
2. è¶…å‚æ•°ä¼˜åŒ–å»ºè®®
3. è®­ç»ƒé—®é¢˜è¯Šæ–­
4. æ¨¡å‹å¯¹æ¯”åˆ†æ
5. è‡ªç„¶è¯­è¨€å¯¹è¯

è¿è¡Œæ–¹å¼: python phase2_llm_framework_demo.py
"""

import sys
import os
import json
import time
import random

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from llm.llm_framework import LLMFramework, create_llm_framework
from llm.model_adapters import MockLLMAdapter


def generate_realistic_metrics(epoch: int, total_epochs: int = 50) -> dict:
    """ç”ŸæˆçœŸå®çš„è®­ç»ƒæŒ‡æ ‡æ•°æ®"""
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡å˜åŒ–
    progress = epoch / total_epochs
    
    # è®­ç»ƒæŸå¤±ï¼šä»2.5é€æ¸é™ä½åˆ°0.1ï¼Œå¸¦æœ‰éšæœºæ³¢åŠ¨
    base_train_loss = 2.5 * (1 - progress) ** 1.5 + 0.1
    train_loss = base_train_loss + random.uniform(-0.05, 0.05)
    
    # éªŒè¯æŸå¤±ï¼šé€šå¸¸æ¯”è®­ç»ƒæŸå¤±é«˜ä¸€äº›ï¼Œå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆ
    overfitting_factor = 1.0 + 0.3 * progress if progress > 0.7 else 1.0
    val_loss = train_loss * overfitting_factor + random.uniform(-0.02, 0.08)
    
    # è®­ç»ƒå‡†ç¡®ç‡ï¼šä»20%æå‡åˆ°95%
    train_accuracy = 0.2 + 0.75 * (1 - (1 - progress) ** 2) + random.uniform(-0.02, 0.02)
    
    # éªŒè¯å‡†ç¡®ç‡ï¼šé€šå¸¸æ¯”è®­ç»ƒå‡†ç¡®ç‡ä½ä¸€äº›
    val_accuracy = train_accuracy - 0.05 + random.uniform(-0.03, 0.01)
    
    # å­¦ä¹ ç‡ï¼šä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦
    initial_lr = 0.01
    learning_rate = initial_lr * 0.5 * (1 + math.cos(3.14159 * progress))
    
    # GPUå†…å­˜ä½¿ç”¨ï¼šéšè®­ç»ƒè¿›åº¦ç•¥æœ‰å˜åŒ–
    gpu_memory_used = 6.0 + random.uniform(-0.5, 1.0)
    
    # è®­ç»ƒé€Ÿåº¦ï¼šéšç€æ¨¡å‹å¤æ‚åº¦å¢åŠ è€Œç•¥æœ‰ä¸‹é™
    training_speed = 1.5 - 0.3 * progress + random.uniform(-0.1, 0.1)
    
    return {
        "epoch": epoch,
        "total_epochs": total_epochs,
        "train_loss": max(0.01, train_loss),
        "val_loss": max(0.01, val_loss),
        "train_accuracy": max(0.0, min(1.0, train_accuracy)),
        "val_accuracy": max(0.0, min(1.0, val_accuracy)),
        "learning_rate": max(0.0001, learning_rate),
        "gpu_memory_used": max(4.0, min(8.0, gpu_memory_used)),
        "gpu_memory_total": 8.0,
        "training_speed": max(0.5, training_speed),
        "gradient_norm": random.uniform(0.01, 0.5),
        "weight_norm": random.uniform(10, 50)
    }


def demo_metrics_analysis(framework: LLMFramework):
    """æ¼”ç¤ºè®­ç»ƒæŒ‡æ ‡åˆ†æåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ” æ¼”ç¤ºåŠŸèƒ½ï¼šè®­ç»ƒæŒ‡æ ‡æ™ºèƒ½åˆ†æ")
    print("="*60)
    
    # ç”Ÿæˆå‡ ä¸ªä¸åŒé˜¶æ®µçš„è®­ç»ƒæŒ‡æ ‡
    test_cases = [
        ("æ—©æœŸè®­ç»ƒé˜¶æ®µ", generate_realistic_metrics(5, 50)),
        ("ä¸­æœŸè®­ç»ƒé˜¶æ®µ", generate_realistic_metrics(25, 50)),
        ("åæœŸè®­ç»ƒé˜¶æ®µ", generate_realistic_metrics(45, 50))
    ]
    
    for stage_name, metrics in test_cases:
        print(f"\nğŸ“Š {stage_name} - Epoch {metrics['epoch']}")
        print(f"   è®­ç»ƒæŸå¤±: {metrics['train_loss']:.4f} | éªŒè¯æŸå¤±: {metrics['val_loss']:.4f}")
        print(f"   è®­ç»ƒå‡†ç¡®ç‡: {metrics['train_accuracy']:.1%} | éªŒè¯å‡†ç¡®ç‡: {metrics['val_accuracy']:.1%}")
        
        # è¿›è¡Œåˆ†æ
        analysis_result = framework.analyze_training_metrics(metrics)
        
        if 'error' not in analysis_result:
            rule_analysis = analysis_result.get('rule_analysis', {})
            print(f"   ğŸ¤– AIåˆ†æ: è®­ç»ƒçŠ¶æ€={rule_analysis.get('training_state', 'æœªçŸ¥')}")
            print(f"           æ”¶æ•›çŠ¶æ€={rule_analysis.get('convergence_status', 'æœªçŸ¥')}")
            print(f"           è¿‡æ‹Ÿåˆé£é™©={rule_analysis.get('overfitting_risk', 'æœªçŸ¥')}")
            
            alerts = analysis_result.get('alerts', [])
            if alerts:
                print(f"   âš ï¸  è­¦æŠ¥: {len(alerts)}ä¸ª")
                for alert in alerts:
                    print(f"       - {alert.get('message', '')}")
        else:
            print(f"   âŒ åˆ†æå¤±è´¥: {analysis_result['error']}")
        
        time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´


def demo_hyperparameter_suggestions(framework: LLMFramework):
    """æ¼”ç¤ºè¶…å‚æ•°ä¼˜åŒ–å»ºè®®åŠŸèƒ½"""
    print("\n" + "="*60)
    print("âš™ï¸  æ¼”ç¤ºåŠŸèƒ½ï¼šè¶…å‚æ•°ä¼˜åŒ–å»ºè®®")
    print("="*60)
    
    # æ¨¡æ‹Ÿä¸åŒçš„è®­ç»ƒåœºæ™¯
    scenarios = [
        {
            "name": "è¿‡æ‹Ÿåˆåœºæ™¯",
            "metrics": {
                "epoch": 30,
                "train_loss": 0.15,
                "val_loss": 0.45,  # éªŒè¯æŸå¤±æ˜æ˜¾é«˜äºè®­ç»ƒæŸå¤±
                "train_accuracy": 0.95,
                "val_accuracy": 0.78,
                "learning_rate": 0.001,
                "gpu_memory_used": 5.5
            },
            "params": {"batch_size": 32, "learning_rate": 0.001, "dropout": 0.1}
        },
        {
            "name": "å­¦ä¹ ç‡è¿‡é«˜åœºæ™¯",
            "metrics": {
                "epoch": 10,
                "train_loss": 2.8,  # æŸå¤±å±…é«˜ä¸ä¸‹
                "val_loss": 3.1,
                "train_accuracy": 0.25,
                "val_accuracy": 0.22,
                "learning_rate": 0.1,  # å­¦ä¹ ç‡è¿‡é«˜
                "gpu_memory_used": 3.2
            },
            "params": {"batch_size": 16, "learning_rate": 0.1, "dropout": 0.2}
        }
    ]
    
    for scenario in scenarios:
        print(f"\nğŸ“ˆ {scenario['name']}")
        metrics = scenario['metrics']
        params = scenario['params']
        
        print(f"   å½“å‰å‚æ•°: æ‰¹é‡å¤§å°={params['batch_size']}, å­¦ä¹ ç‡={params['learning_rate']}")
        print(f"   è®­ç»ƒçŠ¶æ€: æŸå¤±={metrics['train_loss']:.3f}/{metrics['val_loss']:.3f}, å‡†ç¡®ç‡={metrics['train_accuracy']:.1%}/{metrics['val_accuracy']:.1%}")
        
        # è·å–ä¼˜åŒ–å»ºè®®
        suggestion_result = framework.get_hyperparameter_suggestions(metrics, params)
        
        if 'error' not in suggestion_result:
            rule_suggestions = suggestion_result.get('rule_suggestions', [])
            print(f"   ğŸ¯ AIå»ºè®®: {len(rule_suggestions)}æ¡ä¼˜åŒ–å»ºè®®")
            
            for i, suggestion in enumerate(rule_suggestions[:3], 1):  # æ˜¾ç¤ºå‰3æ¡
                param = suggestion.get('parameter', '')
                current = suggestion.get('current_value', '')
                suggested = suggestion.get('suggested_value', '')
                reason = suggestion.get('reason', '')
                priority = suggestion.get('priority', '')
                
                print(f"      {i}. [{priority.upper()}] {param}: {current} â†’ {suggested}")
                print(f"         ç†ç”±: {reason}")
            
            # æ˜¾ç¤ºé¢„æœŸæ”¹è¿›
            improvements = suggestion_result.get('expected_improvements', {})
            if improvements:
                print(f"   ğŸ“Š é¢„æœŸæ”¹è¿›:")
                for param, improvement in list(improvements.items())[:2]:
                    print(f"      - {param}: {improvement}")
        else:
            print(f"   âŒ å»ºè®®ç”Ÿæˆå¤±è´¥: {suggestion_result['error']}")
        
        time.sleep(1)


def demo_problem_diagnosis(framework: LLMFramework):
    """æ¼”ç¤ºè®­ç»ƒé—®é¢˜è¯Šæ–­åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ”§ æ¼”ç¤ºåŠŸèƒ½ï¼šè®­ç»ƒé—®é¢˜è¯Šæ–­")
    print("="*60)
    
    # æ¨¡æ‹Ÿé—®é¢˜åœºæ™¯
    problem_scenarios = [
        {
            "name": "æ¢¯åº¦çˆ†ç‚¸é—®é¢˜",
            "metrics": {
                "epoch": 8,
                "train_loss": 150.0,  # æŸå¤±çˆ†ç‚¸
                "val_loss": 200.0,
                "train_accuracy": 0.1,
                "val_accuracy": 0.1,
                "learning_rate": 0.01,
                "gradient_norm": 50.0  # æ¢¯åº¦èŒƒæ•°è¿‡å¤§
            },
            "error_info": "RuntimeError: Loss became NaN during training"
        },
        {
            "name": "GPUå†…å­˜ä¸è¶³",
            "metrics": {
                "epoch": 15,
                "train_loss": 0.8,
                "val_loss": 0.9,
                "train_accuracy": 0.65,
                "val_accuracy": 0.62,
                "learning_rate": 0.002,
                "gpu_memory_used": 7.8,
                "gpu_memory_total": 8.0  # å†…å­˜ä½¿ç”¨ç‡97.5%
            },
            "error_info": "CUDA out of memory"
        }
    ]
    
    for scenario in problem_scenarios:
        print(f"\nğŸš¨ {scenario['name']}")
        metrics = scenario['metrics']
        error_info = scenario.get('error_info')
        
        if error_info:
            print(f"   é”™è¯¯ä¿¡æ¯: {error_info}")
        
        print(f"   è®­ç»ƒçŠ¶æ€: Epoch {metrics['epoch']}, æŸå¤±={metrics['train_loss']:.2f}")
        
        # è¿›è¡Œé—®é¢˜è¯Šæ–­
        diagnosis_result = framework.diagnose_training_problems(metrics, error_info)
        
        if 'error' not in diagnosis_result:
            anomalies = diagnosis_result.get('detected_anomalies', [])
            severity = diagnosis_result.get('severity_assessment', 'unknown')
            actions = diagnosis_result.get('recommended_actions', [])
            
            print(f"   ğŸ” æ£€æµ‹å¼‚å¸¸: {len(anomalies)}ä¸ª (ä¸¥é‡ç¨‹åº¦: {severity})")
            for anomaly in anomalies[:2]:  # æ˜¾ç¤ºå‰2ä¸ªå¼‚å¸¸
                print(f"      - {anomaly.get('description', '')}")
            
            print(f"   ğŸ’¡ æ¨èè¡ŒåŠ¨: {len(actions)}é¡¹")
            for action in actions[:2]:  # æ˜¾ç¤ºå‰2ä¸ªè¡ŒåŠ¨
                desc = action.get('description', '')
                priority = action.get('priority', '')
                time_est = action.get('expected_time', '')
                print(f"      - [{priority.upper()}] {desc} (é¢„è®¡ç”¨æ—¶: {time_est})")
            
            # æ˜¾ç¤ºLLMè¯Šæ–­
            llm_diagnosis = diagnosis_result.get('llm_diagnosis', '')
            if llm_diagnosis and len(llm_diagnosis) > 100:
                print(f"   ğŸ¤– AIè¯Šæ–­: {llm_diagnosis[:100]}...")
        else:
            print(f"   âŒ è¯Šæ–­å¤±è´¥: {diagnosis_result['error']}")
        
        time.sleep(1)


def demo_model_comparison(framework: LLMFramework):
    """æ¼”ç¤ºæ¨¡å‹å¯¹æ¯”åˆ†æåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ† æ¼”ç¤ºåŠŸèƒ½ï¼šæ¨¡å‹å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    # æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è®­ç»ƒç»“æœ
    model_results = [
        {
            "model_name": "ResNet50",
            "architecture": "CNN",
            "parameters": 25557032,
            "train_loss": 0.15,
            "val_loss": 0.22,
            "train_accuracy": 0.94,
            "val_accuracy": 0.89,
            "training_time": 3600,  # ç§’
            "gpu_memory": 6.2
        },
        {
            "model_name": "EfficientNet-B0",
            "architecture": "CNN",
            "parameters": 5288548,
            "train_loss": 0.18,
            "val_loss": 0.21,
            "train_accuracy": 0.92,
            "val_accuracy": 0.90,
            "training_time": 2800,
            "gpu_memory": 4.8
        },
        {
            "model_name": "MobileNetV2",
            "architecture": "CNN",
            "parameters": 3504872,
            "train_loss": 0.22,
            "val_loss": 0.26,
            "train_accuracy": 0.89,
            "val_accuracy": 0.86,
            "training_time": 1800,
            "gpu_memory": 3.5
        }
    ]
    
    print(f"ğŸ“‹ å¯¹æ¯” {len(model_results)} ä¸ªæ¨¡å‹:")
    for model in model_results:
        print(f"   â€¢ {model['model_name']}: å‚æ•°é‡={model['parameters']:,}, éªŒè¯å‡†ç¡®ç‡={model['val_accuracy']:.1%}")
    
    # è¿›è¡Œæ¨¡å‹å¯¹æ¯”
    comparison_result = framework.compare_model_results(model_results)
    
    if 'error' not in comparison_result:
        rule_comparison = comparison_result.get('rule_comparison', {})
        best_model = comparison_result.get('best_model', {})
        ranking = comparison_result.get('performance_ranking', [])
        
        print(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model.get('model_name', 'æœªçŸ¥')}")
        print(f"   éªŒè¯å‡†ç¡®ç‡: {best_model.get('val_accuracy', 0):.1%}")
        print(f"   éªŒè¯æŸå¤±: {best_model.get('val_loss', 0):.3f}")
        
        print(f"\nğŸ“Š æ€§èƒ½æ’å:")
        for i, model in enumerate(ranking[:3], 1):
            name = model.get('model_name', 'æœªçŸ¥')
            score = model.get('score', 0)
            val_acc = model.get('val_accuracy', 0)
            print(f"   {i}. {name} (ç»¼åˆè¯„åˆ†: {score:.3f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.1%})")
        
        # æ˜¾ç¤ºå¯¹æ¯”æ‘˜è¦
        summary = rule_comparison.get('summary', {})
        if summary:
            acc_range = summary.get('accuracy_range', [0, 0])
            print(f"\nğŸ“ˆ å¯¹æ¯”æ‘˜è¦:")
            print(f"   å‡†ç¡®ç‡èŒƒå›´: {acc_range[0]:.1%} - {acc_range[1]:.1%}")
            print(f"   æ¨¡å‹æ€»æ•°: {summary.get('total_models', 0)}")
    else:
        print(f"   âŒ å¯¹æ¯”å¤±è´¥: {comparison_result['error']}")
    
    time.sleep(1)


def demo_chat_functionality(framework: LLMFramework):
    """æ¼”ç¤ºè‡ªç„¶è¯­è¨€å¯¹è¯åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ’¬ æ¼”ç¤ºåŠŸèƒ½ï¼šè‡ªç„¶è¯­è¨€å¯¹è¯")
    print("="*60)
    
    # é¢„è®¾ä¸€äº›è®­ç»ƒä¸Šä¸‹æ–‡
    context_metrics = generate_realistic_metrics(20, 50)
    framework.analyze_training_metrics(context_metrics)  # å»ºç«‹ä¸Šä¸‹æ–‡
    
    # æ¨¡æ‹Ÿç”¨æˆ·é—®é¢˜
    questions = [
        "å½“å‰è®­ç»ƒæ•ˆæœå¦‚ä½•ï¼Ÿ",
        "æˆ‘çš„æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ï¼Ÿ",
        "ä¸ºä»€ä¹ˆéªŒè¯æŸå¤±æ¯”è®­ç»ƒæŸå¤±é«˜ï¼Ÿ",
        "å¦‚ä½•æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿ",
        "GPUå†…å­˜ä½¿ç”¨ç‡æ˜¯å¦æ­£å¸¸ï¼Ÿ"
    ]
    
    print("ğŸ¤– AIè®­ç»ƒåŠ©æ‰‹å·²å‡†å¤‡å°±ç»ªï¼ŒåŸºäºå½“å‰è®­ç»ƒä¸Šä¸‹æ–‡å›ç­”é—®é¢˜\n")
    
    for i, question in enumerate(questions, 1):
        print(f"ğŸ‘¤ ç”¨æˆ·é—®é¢˜ {i}: {question}")
        
        # è·å–AIå›ç­”
        chat_result = framework.chat_with_training_context(question)
        
        if 'error' not in chat_result:
            response = chat_result.get('response', '')
            processing_time = chat_result.get('framework_info', {}).get('processing_time', 0)
            
            # æˆªå–å›ç­”çš„å‰200ä¸ªå­—ç¬¦ç”¨äºå±•ç¤º
            display_response = response[:200] + "..." if len(response) > 200 else response
            print(f"ğŸ¤– AIå›ç­”: {display_response}")
            print(f"   (å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’)")
        else:
            print(f"âŒ å›ç­”å¤±è´¥: {chat_result['error']}")
        
        print()
        time.sleep(0.5)


def demo_framework_stats(framework: LLMFramework):
    """æ¼”ç¤ºæ¡†æ¶ç»Ÿè®¡å’Œå¥åº·ç›‘æ§"""
    print("\n" + "="*60)
    print("ğŸ“Š æ¼”ç¤ºåŠŸèƒ½ï¼šæ¡†æ¶ç»Ÿè®¡å’Œå¥åº·ç›‘æ§")
    print("="*60)
    
    # è·å–æ¡†æ¶ç»Ÿè®¡
    stats = framework.get_framework_stats()
    
    print("ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
    perf_stats = stats.get('performance_stats', {})
    print(f"   æ€»è¯·æ±‚æ•°: {perf_stats.get('total_requests', 0)}")
    print(f"   æˆåŠŸè¯·æ±‚: {perf_stats.get('successful_requests', 0)}")
    print(f"   å¤±è´¥è¯·æ±‚: {perf_stats.get('failed_requests', 0)}")
    print(f"   å¹³å‡å“åº”æ—¶é—´: {perf_stats.get('average_response_time', 0):.2f}ç§’")
    
    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    health = framework.get_system_health()
    
    print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·çŠ¶æ€: {health.get('overall_status', 'æœªçŸ¥')}")
    
    components = health.get('components', {})
    for comp_name, comp_info in components.items():
        status = comp_info.get('status', 'æœªçŸ¥')
        comp_type = comp_info.get('type', '')
        print(f"   â€¢ {comp_name}: {status} ({comp_type})")
    
    issues = health.get('issues', [])
    if issues:
        print(f"\nâš ï¸  å‘ç°é—®é¢˜ ({len(issues)}ä¸ª):")
        for issue in issues:
            print(f"   - {issue}")
    
    recommendations = health.get('recommendations', [])
    if recommendations:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for rec in recommendations:
            print(f"   - {rec}")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ ç¬¬äºŒé˜¶æ®µLLMæ¡†æ¶å®Œæ•´æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºLLMæ¡†æ¶å®ä¾‹
    print("ğŸ”§ åˆå§‹åŒ–LLMæ¡†æ¶...")
    framework_config = {
        'adapter_type': 'mock',  # ä½¿ç”¨æ¨¡æ‹Ÿé€‚é…å™¨è¿›è¡Œæ¼”ç¤º
        'adapter_config': {},
        'enable_streaming': True,
        'auto_start': True
    }
    
    framework = create_llm_framework(framework_config)
    
    try:
        # æ¼”ç¤ºå„é¡¹åŠŸèƒ½
        demo_metrics_analysis(framework)
        demo_hyperparameter_suggestions(framework)
        demo_problem_diagnosis(framework)
        demo_model_comparison(framework)
        demo_chat_functionality(framework)
        demo_framework_stats(framework)
        
        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“‹ æ¼”ç¤ºå®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
        print("="*60)
        
        final_stats = framework.get_framework_stats()
        perf_stats = final_stats.get('performance_stats', {})
        
        print(f"âœ… æ€»å…±å¤„ç†è¯·æ±‚: {perf_stats.get('total_requests', 0)} ä¸ª")
        print(f"âœ… æˆåŠŸç‡: {perf_stats.get('successful_requests', 0) / max(1, perf_stats.get('total_requests', 1)) * 100:.1f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {perf_stats.get('average_response_time', 0):.2f} ç§’")
        
        # ç¡®ä¿llm_trainç›®å½•å­˜åœ¨
        os.makedirs('llm_train', exist_ok=True)
        
        # å¯¼å‡ºåˆ†ææŠ¥å‘Šåˆ°llm_trainç›®å½•
        report = framework.export_analysis_report(include_history=False)
        report_file = f"llm_train/llm_framework_demo_report_{int(time.time())}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²å¯¼å‡º: {report_file}")
        
        print("\nğŸ‰ ç¬¬äºŒé˜¶æ®µLLMæ¡†æ¶æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
        print("   æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å‡æ­£å¸¸å·¥ä½œï¼Œç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›å…¥ç¬¬ä¸‰é˜¶æ®µçš„ç”¨æˆ·ç•Œé¢é›†æˆã€‚")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        framework.stop()
        print("\nğŸ”§ LLMæ¡†æ¶å·²åœæ­¢")


if __name__ == "__main__":
    # å¯¼å…¥æ•°å­¦åº“ç”¨äºç”ŸæˆçœŸå®æŒ‡æ ‡
    import math
    main() 